{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the computational training website for Arcadia Science. The purpose of this website is to catalogue and host training materials at Arcadia Science. We strive to make our training material open and accessible so that it can be useful at time of delivery and for asynchronous learning. To discover training materials, navigate to the pages on the left navigation bar. The materials on this site include trainings as part of Arcadia Users Group (AUG) and workshops. AUG is a forum for Arcadians to bring data, conceptual problems, or low-level coding questions to discuss in a group setting. We also periodically prepare organized materials to teach in these sessions, which are available on the Arcadia Users Group page. Additionally, we lead longer Workshops for Arcadians on specific topics and skills.","title":"Computational Training Home"},{"location":"CONTRIBUTING/","text":"Contribution guide Contribution philosophy The lessons on the Arcadia Computational Training website are written to empower learners to pick up skills both during formal teaching encounters and as asynchronous materials they can be used by anyone, anytime to pick up or refresh those skills. Best practices for computational lesson development Clear learning objectives : Begin each lesson with explicit learning objectives that outline what the learners should expect to achieve by the end of the session. This helps learners and instructors gauge progress and focus on the key takeaways. Domain-relevant examples : Use examples that relate to the domain you're teaching to help learners understand the practical application of the concepts being taught. This aids in retention and engagement. Incremental skill development : Organize the lessons in a logical sequence that builds upon previous skills and concepts. This allows learners to progressively gain expertise as they move through the material. Interactive hands-on exercises : Include hands-on exercises throughout the lessons to give learners the opportunity to practice their skills in real-time. This helps to reinforce learning and allows instructors to provide immediate feedback. Accessible language and clear explanations : Use clear, concise language and avoid jargon to ensure that learners can easily understand the material. Provide explanations and examples for all key terms and concepts. Visual aids and multimedia resources : Incorporate visual aids, such as diagrams and flowcharts, to help learners visualize complex concepts. Provide links to supplementary resources, such as videos and articles, to support diverse learning styles. Self-assessment and challenges : Include periodic self-assessment questions and challenges to help learners evaluate their progress and identify areas for improvement. This also enables instructors to adapt their teaching approach based on learner performance. Comprehensive documentation : Provide thorough documentation for all aspects of the lesson, including step-by-step guides, code examples, and troubleshooting tips. Open access and reusable materials : Make the materials freely available online under an open license, enabling learners to access them at their convenience and allowing other educators to reuse and adapt the content for their own purposes. Crash course in teaching computation Establish a welcoming environment : Create a positive atmosphere by introducing yourself, setting expectations, and encouraging learners to share their backgrounds and goals. This helps to build rapport and foster a supportive learning community. Use active learning techniques : Engage learners by incorporating activities, such as group discussions, pair programming, or think-pair-share exercises, which facilitate participation and promote deeper understanding. Encourage questions : Ask open-ended questions like \"What questions do you have?\" instead of \"Are there any questions?\" This creates a more inclusive environment and prompts learners to reflect on their understanding. Be mindful of language : Avoid using diminishing language, such as \"just\" or \"simply,\" which can make learners feel inadequate if they are struggling with a concept. Use inclusive and accessible language to ensure everyone feels valued and included. Use live programming and narration : Type out code or commands as you go, narrating any shortcut keystrokes you use to help learners follow along. This interactive approach allows you to pace yourself, making it easier for learners to keep up with the lesson. Break down complex concepts : Present complex ideas in smaller, manageable chunks, and provide clear explanations and examples for each. This helps learners to digest the material more easily and promotes better comprehension. Be adaptable : Be prepared to adjust your teaching style or lesson plan to accommodate learners' needs and backgrounds. If learners are struggling with a particular concept, take the time to provide additional support or revisit the topic. Embrace errors as learning opportunities : Recognize that errors made while programming can be good examples of how to debug issues in real life and can often be instructional. Use these moments to demonstrate problem-solving techniques and encourage learners to develop their own debugging skills. Reflect and iterate : After delivering the lesson, reflect on what went well and what could be improved. Seek feedback from learners and fellow instructors, and use this information to refine your teaching methods and lesson materials for future sessions. Below we include more resources on how to teach programming. Tips for instructors \"Helpers\" checklist with many generalizable tips. \"Helpers\" are expert individuals that help during a workshop or lesson when learners have a specific problem that doesn't need to be addressed by the entire group. Carpentries instructor training curriculum How to contribute via GitHub This website is built using MkDocs from the GitHub respository Arcadia-Science/arcadia-computational-training . Contributions should be made on a branch or fork, integrated via pull request, and reviewed by at least one Arcadian. Contributing to lesson materials in arcadia-users-group Each AUG lesson should be placed in its own folder. Folder names should follow the naming convention DATE-short-description-of-tutorial.md where dates are formatted as yearmonthdata with noseparaters, short descriptions are concise tags for the content of the lesson and where each word is separated by a hyphen. Lesson materials should be in markdown format. Any companion materials to the lesson should be stored in the folder when possible. If the additional files are too large, download provenance should be recorded clearly in the lesson. Images rendered inline in the lesson text should also be stored in this folder. The relative path (e.g. just the file name) can be used to render the image in the lesson. Lessons are linked from the overview.md document, making them discoverable and rendered on arcadia-science.github.io/arcadia-computational-training. Linking to external training materials Sometimes lesson material belongs better in a different repository or we'll cover open lessons developed by other organizations such as The Carpentries . When this is the case, link to the lesson material in the overview.md document. The lesson will not be rendered on the computational training website, but it will be linked and discoverable from there. Attribution This document was inspired by The Carpentries documentation and our experience participating in The Carpentries community.","title":"Contribute"},{"location":"CONTRIBUTING/#contribution-guide","text":"","title":"Contribution guide"},{"location":"CONTRIBUTING/#contribution-philosophy","text":"The lessons on the Arcadia Computational Training website are written to empower learners to pick up skills both during formal teaching encounters and as asynchronous materials they can be used by anyone, anytime to pick up or refresh those skills.","title":"Contribution philosophy"},{"location":"CONTRIBUTING/#best-practices-for-computational-lesson-development","text":"Clear learning objectives : Begin each lesson with explicit learning objectives that outline what the learners should expect to achieve by the end of the session. This helps learners and instructors gauge progress and focus on the key takeaways. Domain-relevant examples : Use examples that relate to the domain you're teaching to help learners understand the practical application of the concepts being taught. This aids in retention and engagement. Incremental skill development : Organize the lessons in a logical sequence that builds upon previous skills and concepts. This allows learners to progressively gain expertise as they move through the material. Interactive hands-on exercises : Include hands-on exercises throughout the lessons to give learners the opportunity to practice their skills in real-time. This helps to reinforce learning and allows instructors to provide immediate feedback. Accessible language and clear explanations : Use clear, concise language and avoid jargon to ensure that learners can easily understand the material. Provide explanations and examples for all key terms and concepts. Visual aids and multimedia resources : Incorporate visual aids, such as diagrams and flowcharts, to help learners visualize complex concepts. Provide links to supplementary resources, such as videos and articles, to support diverse learning styles. Self-assessment and challenges : Include periodic self-assessment questions and challenges to help learners evaluate their progress and identify areas for improvement. This also enables instructors to adapt their teaching approach based on learner performance. Comprehensive documentation : Provide thorough documentation for all aspects of the lesson, including step-by-step guides, code examples, and troubleshooting tips. Open access and reusable materials : Make the materials freely available online under an open license, enabling learners to access them at their convenience and allowing other educators to reuse and adapt the content for their own purposes.","title":"Best practices for computational lesson development"},{"location":"CONTRIBUTING/#crash-course-in-teaching-computation","text":"Establish a welcoming environment : Create a positive atmosphere by introducing yourself, setting expectations, and encouraging learners to share their backgrounds and goals. This helps to build rapport and foster a supportive learning community. Use active learning techniques : Engage learners by incorporating activities, such as group discussions, pair programming, or think-pair-share exercises, which facilitate participation and promote deeper understanding. Encourage questions : Ask open-ended questions like \"What questions do you have?\" instead of \"Are there any questions?\" This creates a more inclusive environment and prompts learners to reflect on their understanding. Be mindful of language : Avoid using diminishing language, such as \"just\" or \"simply,\" which can make learners feel inadequate if they are struggling with a concept. Use inclusive and accessible language to ensure everyone feels valued and included. Use live programming and narration : Type out code or commands as you go, narrating any shortcut keystrokes you use to help learners follow along. This interactive approach allows you to pace yourself, making it easier for learners to keep up with the lesson. Break down complex concepts : Present complex ideas in smaller, manageable chunks, and provide clear explanations and examples for each. This helps learners to digest the material more easily and promotes better comprehension. Be adaptable : Be prepared to adjust your teaching style or lesson plan to accommodate learners' needs and backgrounds. If learners are struggling with a particular concept, take the time to provide additional support or revisit the topic. Embrace errors as learning opportunities : Recognize that errors made while programming can be good examples of how to debug issues in real life and can often be instructional. Use these moments to demonstrate problem-solving techniques and encourage learners to develop their own debugging skills. Reflect and iterate : After delivering the lesson, reflect on what went well and what could be improved. Seek feedback from learners and fellow instructors, and use this information to refine your teaching methods and lesson materials for future sessions. Below we include more resources on how to teach programming. Tips for instructors \"Helpers\" checklist with many generalizable tips. \"Helpers\" are expert individuals that help during a workshop or lesson when learners have a specific problem that doesn't need to be addressed by the entire group. Carpentries instructor training curriculum","title":"Crash course in teaching computation"},{"location":"CONTRIBUTING/#how-to-contribute-via-github","text":"This website is built using MkDocs from the GitHub respository Arcadia-Science/arcadia-computational-training . Contributions should be made on a branch or fork, integrated via pull request, and reviewed by at least one Arcadian.","title":"How to contribute via GitHub"},{"location":"CONTRIBUTING/#contributing-to-lesson-materials-in-arcadia-users-group","text":"Each AUG lesson should be placed in its own folder. Folder names should follow the naming convention DATE-short-description-of-tutorial.md where dates are formatted as yearmonthdata with noseparaters, short descriptions are concise tags for the content of the lesson and where each word is separated by a hyphen. Lesson materials should be in markdown format. Any companion materials to the lesson should be stored in the folder when possible. If the additional files are too large, download provenance should be recorded clearly in the lesson. Images rendered inline in the lesson text should also be stored in this folder. The relative path (e.g. just the file name) can be used to render the image in the lesson. Lessons are linked from the overview.md document, making them discoverable and rendered on arcadia-science.github.io/arcadia-computational-training.","title":"Contributing to lesson materials in arcadia-users-group"},{"location":"CONTRIBUTING/#linking-to-external-training-materials","text":"Sometimes lesson material belongs better in a different repository or we'll cover open lessons developed by other organizations such as The Carpentries . When this is the case, link to the lesson material in the overview.md document. The lesson will not be rendered on the computational training website, but it will be linked and discoverable from there.","title":"Linking to external training materials"},{"location":"CONTRIBUTING/#attribution","text":"This document was inspired by The Carpentries documentation and our experience participating in The Carpentries community.","title":"Attribution"},{"location":"_static/","text":"Static files This folder is for static files associated with the mkdocs site that don't have other natural homes (e.g. are not pngs associated with lesson materials).","title":"Static files"},{"location":"_static/#static-files","text":"This folder is for static files associated with the mkdocs site that don't have other natural homes (e.g. are not pngs associated with lesson materials).","title":"Static files"},{"location":"arcadia-users-group/","text":"Arcadia Users' Group training materials This is the location for training materials associated with tutorials given during Arcadia Users' Group (AUG). Contributing If you have lesson contributions to AUG, see the section Contributing to lesson materials in arcadia-users-group in CONTRIBUTING.md .","title":"Arcadia Users' Group training materials"},{"location":"arcadia-users-group/#arcadia-users-group-training-materials","text":"This is the location for training materials associated with tutorials given during Arcadia Users' Group (AUG).","title":"Arcadia Users' Group training materials"},{"location":"arcadia-users-group/#contributing","text":"If you have lesson contributions to AUG, see the section Contributing to lesson materials in arcadia-users-group in CONTRIBUTING.md .","title":"Contributing"},{"location":"arcadia-users-group/overview/","text":"Arcadia Users' Group Arcadia Users\u2019 Group (AUG) is a forum for Arcadians to bring their biological data and discuss high level conceptual problems (what is genome assembly?), low level coding problems (what does this error message even mean?!), and everything in between. Like the start codon, AUG aims to get Arcadians started on projects and help them get re-started after they run into problems. AUG is a weekly hybrid meetup. On training weeks, we offer 20 minute - 1 hour training or code review sessions. See the tables below for upcoming and past lesson materials. On non-training weeks, we host a 2 hour hybrid remote/in-person office-hours style working group to ask and answer questions and to co-work on data-relevant problems. What is a Users\u2019 group? A users\u2019 group is a club focused on a computer-related technology. Unlike python users\u2019 groups (PUGs) and R users\u2019 groups (RUGs), AUG is language- and technology-agnostic. We focus on anything Arcadians need to be successful with their data-related biology. This is the landing page for tutorials delivered during AUG sessions. Upcoming trainings The schedule is subject to change. Date Tutorial Description Previous trainings Note recordings are only available to people with @arcadiascience.com email addresses. Date Tutorial Description Link to recording Aug 13th, 2024 Illustrator Basics & Templates This lesson provides an overview of how to use the custom libraries and templates in Illustrator that adhere to Arcadia's style guidelines. recording Jul 30, 2024 Making figures in python with arcadia-pycolor and in R with arcadiathemeR This lesson covers how to make figures in python and R that adhere to Arcadia's style guidelines using the arcadia-pycolor and arcadiathemeR packages, respectively. recording Jul 23, 2024 Introduction to Design Principles This lesson gives an introduction to design principles and different components to consider when creating figures and illustrations. recording Mar 5, 2024 GitHub Templates: new repos with set structures This lesson provides an overview of how to create a new repository from a GitHub template. Using a Python analysis repository as an example, it walks through the function of each file in the template and the benefits that come from using templates. Feb 6, 2024 Code formatting, linting, and style guides This lesson dives into the essentials of Python code formatting, linting, and style guides. It emphasizes why consistent code style is crucial for readability and maintenance and introduces tools like black for auto-formatting and ruff for linting. The session also explains Python-specific conventions, such as PEP8 and the Google Python Style Guide, with practical examples to illustrate how these practices improve code quality. recording Nov 28, 2023 Building test data sets Tests should run quickly, ensure that code produces the correct output, and ensure that the output makes sense. Designing test data sets that fulfill all of these criteria is an art. This lesson will cover some guiding principles and strategies for making new test data sets. recording Nov 14, 2023 Testing concepts & terminology Unit tests & assertions & continuous integration, oh my! This introduction to testing will cover the concepts and goals of unit testing and provide a hands-on primer for writing & running tests. recording 1 recording 2 Oct 31, 2023 Intro to R & tidyverse This lesson will provide a quick introduction to R and its objects and then cover how to manipulate, analyze, and export data using tidyverse packages. This lesson is derived from the Data Carpentry Data Analysis and Visualisation in R for Ecologists lessons. recording Oct 17, 2023 Intro to R & data visualization with ggplot2 This lesson will cover how to visualize data using the ggplot2 package while building a mental model for how R works. This lesson is derived from the Data Carpentry Data Analysis and Visualisation in R for Ecologists lessons. recording Jul 11, 2023 Intro to 3D Printing Introduces learners to the basics of 3D printing, specifically focusing on designing stamps to create microchambers for cells using Autodesk Fusion 360 software and utilizing the Form2 SLA printer from Formlabs for actual 3D printing. recording Apr 11, 2023 Automation and programming Opentrons This lesson provides an introduction to Opentrons liquid handling robot via the Python API and other programs. recording Mar 28, 2023 Introduction to Python part 3 This is the third and final lesson in the intro to Python series. It covers how to interact with packages, how to read and write files,plotting with matplotlib, and dataframe operations with pandas. recording Mar 14, 2023 Introduction to Python part 2 This is the second lesson in an intro to Python series. It covers data structures (lists, dictionaries), loops, conditionals, and methods. Scroll to the bottom of the lesson for another practice problem set. recording 1 recording 2 Feb 28, 2023 Introduction to Python part 1 This is the first lesson in an intro to Python series. It covers variables, data types, functions, and running scripts from the command line. Scroll to the bottom of the lesson for a practice problem set. recording Feb 14, 2023 Code review during the pub process We'll provide an overview of what to expect from the code review component of the pub process, including common mistakes and a refresher to help the process go more smoothly. Dec 5, 2022 Keyboard shortcuts for Terminal This lesson covers keyboard shortcuts that can be used in Terminal to quickly navigate around commands that you write. recording Oct 31, 2022 Turning a GitHub repo into a collection of interactive notebooks with Binder This week we\u2019ll introduce Binder, an executable environment that makes your code immediately reproducible by anyone, anywhere. recording Oct 24, 2022 Introduction to Jupyter notebooks Jupyter notebooks are web-based interactive computing environments that combine code, equations, narrative text, and visualizations in a single document. This lesson introduces jupyter notebooks and how to install and run them locally. recording Oct 17, 2022 Conda for software installation & environment management In bioinformatics, we often use mulitiple software tools to answer a research question. In this lesson, we will learn how to use conda to install software. Conda is a software installation and management system that helps us conduct reproducible analyses while avoiding software conflicts. Then, we'll learn how to organize our software installations into environments, and how to use environments to ensure our analyses are repeatable. recording Oct 11, 2022 Increasing developer productivity In the last couple of weeks, we learned many shell and git commands. This week, we'll take a step back to learn about using an integrated development environment (IDE) to develop our scripts/pipelines and customizing our terminals with shortcuts and git visualizations. recording Oct 3, 2022 Code review using GitHub As the last piece of our Git and GitHub workshop we'll go over the key element of code review and how to execute a review on GitHub. Sept 26, 2022 Undoing changes in a Git repository This lesson is a continuation of our Git and GitHub workshop . We will cover how to undo changes in a Git repository using git restore , git revert , or git checkout . recording Sept 19, 2022 How to use S3 with the command line interface (CLI) Arcadia uses Amazon Web Services S3 to store some files remotely. This lesson introduces 2 tools (the official AWS CLI and s5cmd ) that enable command line access to S3 recording Sept 12, 2022 Introduction to the command line part 2 This next lesson on the command line covers how to create and (re)move directories and files ( mkdir , rmdir , cp , mv , rm ) and how to investigate the contents of a file thoroughly ( less , wc , grep ) recording Sept 6, 2022 Introduction to the command line part 1 The command line is an essential interface to control remote computers or to execute automated analysis. This lesson introduces the concept of the command line, shows how to look at files ( ls , head , tail ), and how to navigate around a computer ( cd , pwd ) recording Aug 29, 2022 Project organization and file & resource management It\u2019s not uncommon to generate thousands of files during a bioinformatics analysis. In this lesson, we will cover project organization strategies for keeping track of the files we generate and how we generate them. Be kind to future you: a little organization effort now can save hours of headache later! recording Aug 22, 2022 Introduction to markdown syntax Markdown is a lightweight markup language (think HTML or LaTeX but\u2026lighter) that\u2019s used by lots of websites and software (Slack and Notion and GitHub, oh my!). This quick introduction will show you how to easily format plain text that is readable and that renders into beautifully formatted documents. recording","title":"Arcadia Users Group"},{"location":"arcadia-users-group/overview/#arcadia-users-group","text":"Arcadia Users\u2019 Group (AUG) is a forum for Arcadians to bring their biological data and discuss high level conceptual problems (what is genome assembly?), low level coding problems (what does this error message even mean?!), and everything in between. Like the start codon, AUG aims to get Arcadians started on projects and help them get re-started after they run into problems. AUG is a weekly hybrid meetup. On training weeks, we offer 20 minute - 1 hour training or code review sessions. See the tables below for upcoming and past lesson materials. On non-training weeks, we host a 2 hour hybrid remote/in-person office-hours style working group to ask and answer questions and to co-work on data-relevant problems. What is a Users\u2019 group? A users\u2019 group is a club focused on a computer-related technology. Unlike python users\u2019 groups (PUGs) and R users\u2019 groups (RUGs), AUG is language- and technology-agnostic. We focus on anything Arcadians need to be successful with their data-related biology. This is the landing page for tutorials delivered during AUG sessions.","title":"Arcadia Users' Group"},{"location":"arcadia-users-group/overview/#upcoming-trainings","text":"The schedule is subject to change. Date Tutorial Description","title":"Upcoming trainings"},{"location":"arcadia-users-group/overview/#previous-trainings","text":"Note recordings are only available to people with @arcadiascience.com email addresses. Date Tutorial Description Link to recording Aug 13th, 2024 Illustrator Basics & Templates This lesson provides an overview of how to use the custom libraries and templates in Illustrator that adhere to Arcadia's style guidelines. recording Jul 30, 2024 Making figures in python with arcadia-pycolor and in R with arcadiathemeR This lesson covers how to make figures in python and R that adhere to Arcadia's style guidelines using the arcadia-pycolor and arcadiathemeR packages, respectively. recording Jul 23, 2024 Introduction to Design Principles This lesson gives an introduction to design principles and different components to consider when creating figures and illustrations. recording Mar 5, 2024 GitHub Templates: new repos with set structures This lesson provides an overview of how to create a new repository from a GitHub template. Using a Python analysis repository as an example, it walks through the function of each file in the template and the benefits that come from using templates. Feb 6, 2024 Code formatting, linting, and style guides This lesson dives into the essentials of Python code formatting, linting, and style guides. It emphasizes why consistent code style is crucial for readability and maintenance and introduces tools like black for auto-formatting and ruff for linting. The session also explains Python-specific conventions, such as PEP8 and the Google Python Style Guide, with practical examples to illustrate how these practices improve code quality. recording Nov 28, 2023 Building test data sets Tests should run quickly, ensure that code produces the correct output, and ensure that the output makes sense. Designing test data sets that fulfill all of these criteria is an art. This lesson will cover some guiding principles and strategies for making new test data sets. recording Nov 14, 2023 Testing concepts & terminology Unit tests & assertions & continuous integration, oh my! This introduction to testing will cover the concepts and goals of unit testing and provide a hands-on primer for writing & running tests. recording 1 recording 2 Oct 31, 2023 Intro to R & tidyverse This lesson will provide a quick introduction to R and its objects and then cover how to manipulate, analyze, and export data using tidyverse packages. This lesson is derived from the Data Carpentry Data Analysis and Visualisation in R for Ecologists lessons. recording Oct 17, 2023 Intro to R & data visualization with ggplot2 This lesson will cover how to visualize data using the ggplot2 package while building a mental model for how R works. This lesson is derived from the Data Carpentry Data Analysis and Visualisation in R for Ecologists lessons. recording Jul 11, 2023 Intro to 3D Printing Introduces learners to the basics of 3D printing, specifically focusing on designing stamps to create microchambers for cells using Autodesk Fusion 360 software and utilizing the Form2 SLA printer from Formlabs for actual 3D printing. recording Apr 11, 2023 Automation and programming Opentrons This lesson provides an introduction to Opentrons liquid handling robot via the Python API and other programs. recording Mar 28, 2023 Introduction to Python part 3 This is the third and final lesson in the intro to Python series. It covers how to interact with packages, how to read and write files,plotting with matplotlib, and dataframe operations with pandas. recording Mar 14, 2023 Introduction to Python part 2 This is the second lesson in an intro to Python series. It covers data structures (lists, dictionaries), loops, conditionals, and methods. Scroll to the bottom of the lesson for another practice problem set. recording 1 recording 2 Feb 28, 2023 Introduction to Python part 1 This is the first lesson in an intro to Python series. It covers variables, data types, functions, and running scripts from the command line. Scroll to the bottom of the lesson for a practice problem set. recording Feb 14, 2023 Code review during the pub process We'll provide an overview of what to expect from the code review component of the pub process, including common mistakes and a refresher to help the process go more smoothly. Dec 5, 2022 Keyboard shortcuts for Terminal This lesson covers keyboard shortcuts that can be used in Terminal to quickly navigate around commands that you write. recording Oct 31, 2022 Turning a GitHub repo into a collection of interactive notebooks with Binder This week we\u2019ll introduce Binder, an executable environment that makes your code immediately reproducible by anyone, anywhere. recording Oct 24, 2022 Introduction to Jupyter notebooks Jupyter notebooks are web-based interactive computing environments that combine code, equations, narrative text, and visualizations in a single document. This lesson introduces jupyter notebooks and how to install and run them locally. recording Oct 17, 2022 Conda for software installation & environment management In bioinformatics, we often use mulitiple software tools to answer a research question. In this lesson, we will learn how to use conda to install software. Conda is a software installation and management system that helps us conduct reproducible analyses while avoiding software conflicts. Then, we'll learn how to organize our software installations into environments, and how to use environments to ensure our analyses are repeatable. recording Oct 11, 2022 Increasing developer productivity In the last couple of weeks, we learned many shell and git commands. This week, we'll take a step back to learn about using an integrated development environment (IDE) to develop our scripts/pipelines and customizing our terminals with shortcuts and git visualizations. recording Oct 3, 2022 Code review using GitHub As the last piece of our Git and GitHub workshop we'll go over the key element of code review and how to execute a review on GitHub. Sept 26, 2022 Undoing changes in a Git repository This lesson is a continuation of our Git and GitHub workshop . We will cover how to undo changes in a Git repository using git restore , git revert , or git checkout . recording Sept 19, 2022 How to use S3 with the command line interface (CLI) Arcadia uses Amazon Web Services S3 to store some files remotely. This lesson introduces 2 tools (the official AWS CLI and s5cmd ) that enable command line access to S3 recording Sept 12, 2022 Introduction to the command line part 2 This next lesson on the command line covers how to create and (re)move directories and files ( mkdir , rmdir , cp , mv , rm ) and how to investigate the contents of a file thoroughly ( less , wc , grep ) recording Sept 6, 2022 Introduction to the command line part 1 The command line is an essential interface to control remote computers or to execute automated analysis. This lesson introduces the concept of the command line, shows how to look at files ( ls , head , tail ), and how to navigate around a computer ( cd , pwd ) recording Aug 29, 2022 Project organization and file & resource management It\u2019s not uncommon to generate thousands of files during a bioinformatics analysis. In this lesson, we will cover project organization strategies for keeping track of the files we generate and how we generate them. Be kind to future you: a little organization effort now can save hours of headache later! recording Aug 22, 2022 Introduction to markdown syntax Markdown is a lightweight markup language (think HTML or LaTeX but\u2026lighter) that\u2019s used by lots of websites and software (Slack and Notion and GitHub, oh my!). This quick introduction will show you how to easily format plain text that is readable and that renders into beautifully formatted documents. recording","title":"Previous trainings"},{"location":"arcadia-users-group/20220822-intro-to-markdown-syntax/lesson/","text":"Introduction to markdown syntax Markdown is a lightweight markup language. A markup language is a \"text-encoding system consisting of a set of symbols inserted in a text document to control its structure, formatting, or the relationship between its parts\" ( source ). Other common markup languages include LaTeX and HTML. Markdown was created to enable people to easily read and write plain text format. Markdown (and other markup languages) can be automatically rendered to control the display format of a document. It's often used in formatting for things like websites & blogs and in computational notebooks (jupyter & RMarkdown). This tutorial provides a brief introduction to the markdown syntax and where you may encounter it. Markdown demonstration with side-by-side rendering in HackMD HackMD is a collaborative markdown editor. It's similar to Google Docs but all text is written in markdown. We'll use HackMD to practice writing markdown syntax and to watch it render instantaneously. Open a new HackMD document and make sure \"Both\" mode is selected at the top of the document viewer. Copy and paste the following text into the left hand panel. The text will auto-render on the right hand panel and demonstrate how the markdown language is interpreted and displayed. This content is modified from this gist . # Heading 1 ## Heading 2 ### Heading 3 #### Heading 4 ##### Heading 5 ###### Heading 6 --- Paragraph ~~Mistaken text.~~ *Italics* **Bold** --- Tasks - [ ] a task list item - [ ] incomplete - [x] completed --- Code Blocks text `Inline Code` text 4 space indention makes full-width standard code blocks \\``` # (remove the slashes above and below the back ticks to render properly # three backticks also makes a standard full-width code block library(dplyr) library(readr) \\``` --- * List item one * List item two * A nested item --- 1. Number list item one 1.1. A nested item 2. Number list item two 3. Number list item three --- > Quote > > Second line Quote --- Standard link = http://ghost.org [Custom Text Link](http://ghost.org) --- Image ![](https://i.imgur.com/k3H6cEi.jpg) --- Table | Left-Aligned | Center Aligned | Right Aligned | | :------------ |:---------------:| -----:| | col 3 is | some wordy text | $1600 | | col 2 is | centered | $12 | | zebra stripes | are neat | $1 | Markdown and Notion & Slack Both Slack and Notion auto-render text written in markdown as the text is written. You don't have to write in markdown (you can use the GUI buttons in each app to format your text), but it's a nice option to more quickly format text. Markdown and GitHub GitHub is a code hosting platform for version control and collaboration. GitHub renders markdown in a few key places. The first is in a repository's README . When a README is included in a repository, GitHub renders it below the code in that folder. Most repositories contain a README in the main folder that describes the content of that repository. See the image below for an example of a README that was auto-rendered by GitHub. Sometimes, folders contained within the repository will also contain a README describing the contents of that specific folder. GitHub issues and pull requests also support markdown syntax. GitHub issues track ideas, feedback, tasks, or bugs for a repository. GitHub pull requests contain suggested code or document changes along with a narrative left in a comment section. Using markdown in issues and in pull request comments makes ideas and feedback easier to quickly read and digest. Lastly, jupyter notebooks that are uploaded to GitHub are rendered, including the markdown syntax used inline in the document. Since a rendered notebook nicely presents the code that was run, its output, and any documentation added to the analysis, jupyter notebooks are a good way to share a data analysis. Flavors of markdown By and large, the core syntax of markdown is consistent across platforms. However, different platforms support different features. Each platform is briefly introduced below, followed by some platform-specific markdown features. GitHub: GitHub supports a strict super set of the markdown language. See this webpage for a description of GitHub-flavored markdown. HackMD: as mentioned above, HackMD is a collaborative markdown editor. It supports many additional markdown goodies that may not be available on or rendered by other platforms. drag-and-drop png -> URL -> rendered image: HackMD allows you to drag and drop an image into the text space, where it will generate a URL for the image and format that link as in markdown syntax for the image to render. Once this URL exists, you can drop it in any markdown document and it will render. I don't know how long these image links will be supported for. So far, my oldest image link is 2 years old and is still going strong. :::idea : generates a blue-hued call out box that is separated from the rest of the text. Table of contents: including [toc] at the top of a document generates a table of contents from headings in the document. user name tags: You can generate a name tag (e.g. to label a specific addition to the document) using > [name=taylorreiter] . RMarkdown: Rmarkdown weaves together code, code outputs, and text into a single document. Documents begin with a yaml header to specify how the document should be rendered. Code blocks begin with three back ticks, but code blocks that should be executed start with {r} after the three back ticks. The content inside the curly brackets controls the run and rendering behavior of a specific code block and its outputs. More on markdown See below for links to additional materials on markdown. GitHub markdown cheatsheet RMarkdown cheatsheet The Carpentries alpha introduction to markdown lesson","title":"Introduction to markdown syntax"},{"location":"arcadia-users-group/20220822-intro-to-markdown-syntax/lesson/#introduction-to-markdown-syntax","text":"Markdown is a lightweight markup language. A markup language is a \"text-encoding system consisting of a set of symbols inserted in a text document to control its structure, formatting, or the relationship between its parts\" ( source ). Other common markup languages include LaTeX and HTML. Markdown was created to enable people to easily read and write plain text format. Markdown (and other markup languages) can be automatically rendered to control the display format of a document. It's often used in formatting for things like websites & blogs and in computational notebooks (jupyter & RMarkdown). This tutorial provides a brief introduction to the markdown syntax and where you may encounter it.","title":"Introduction to markdown syntax"},{"location":"arcadia-users-group/20220822-intro-to-markdown-syntax/lesson/#markdown-demonstration-with-side-by-side-rendering-in-hackmd","text":"HackMD is a collaborative markdown editor. It's similar to Google Docs but all text is written in markdown. We'll use HackMD to practice writing markdown syntax and to watch it render instantaneously. Open a new HackMD document and make sure \"Both\" mode is selected at the top of the document viewer. Copy and paste the following text into the left hand panel. The text will auto-render on the right hand panel and demonstrate how the markdown language is interpreted and displayed. This content is modified from this gist . # Heading 1 ## Heading 2 ### Heading 3 #### Heading 4 ##### Heading 5 ###### Heading 6 --- Paragraph ~~Mistaken text.~~ *Italics* **Bold** --- Tasks - [ ] a task list item - [ ] incomplete - [x] completed --- Code Blocks text `Inline Code` text 4 space indention makes full-width standard code blocks \\``` # (remove the slashes above and below the back ticks to render properly # three backticks also makes a standard full-width code block library(dplyr) library(readr) \\``` --- * List item one * List item two * A nested item --- 1. Number list item one 1.1. A nested item 2. Number list item two 3. Number list item three --- > Quote > > Second line Quote --- Standard link = http://ghost.org [Custom Text Link](http://ghost.org) --- Image ![](https://i.imgur.com/k3H6cEi.jpg) --- Table | Left-Aligned | Center Aligned | Right Aligned | | :------------ |:---------------:| -----:| | col 3 is | some wordy text | $1600 | | col 2 is | centered | $12 | | zebra stripes | are neat | $1 |","title":"Markdown demonstration with side-by-side rendering in HackMD"},{"location":"arcadia-users-group/20220822-intro-to-markdown-syntax/lesson/#markdown-and-notion-slack","text":"Both Slack and Notion auto-render text written in markdown as the text is written. You don't have to write in markdown (you can use the GUI buttons in each app to format your text), but it's a nice option to more quickly format text.","title":"Markdown and Notion &amp; Slack"},{"location":"arcadia-users-group/20220822-intro-to-markdown-syntax/lesson/#markdown-and-github","text":"GitHub is a code hosting platform for version control and collaboration. GitHub renders markdown in a few key places. The first is in a repository's README . When a README is included in a repository, GitHub renders it below the code in that folder. Most repositories contain a README in the main folder that describes the content of that repository. See the image below for an example of a README that was auto-rendered by GitHub. Sometimes, folders contained within the repository will also contain a README describing the contents of that specific folder. GitHub issues and pull requests also support markdown syntax. GitHub issues track ideas, feedback, tasks, or bugs for a repository. GitHub pull requests contain suggested code or document changes along with a narrative left in a comment section. Using markdown in issues and in pull request comments makes ideas and feedback easier to quickly read and digest. Lastly, jupyter notebooks that are uploaded to GitHub are rendered, including the markdown syntax used inline in the document. Since a rendered notebook nicely presents the code that was run, its output, and any documentation added to the analysis, jupyter notebooks are a good way to share a data analysis.","title":"Markdown and GitHub"},{"location":"arcadia-users-group/20220822-intro-to-markdown-syntax/lesson/#flavors-of-markdown","text":"By and large, the core syntax of markdown is consistent across platforms. However, different platforms support different features. Each platform is briefly introduced below, followed by some platform-specific markdown features. GitHub: GitHub supports a strict super set of the markdown language. See this webpage for a description of GitHub-flavored markdown. HackMD: as mentioned above, HackMD is a collaborative markdown editor. It supports many additional markdown goodies that may not be available on or rendered by other platforms. drag-and-drop png -> URL -> rendered image: HackMD allows you to drag and drop an image into the text space, where it will generate a URL for the image and format that link as in markdown syntax for the image to render. Once this URL exists, you can drop it in any markdown document and it will render. I don't know how long these image links will be supported for. So far, my oldest image link is 2 years old and is still going strong. :::idea : generates a blue-hued call out box that is separated from the rest of the text. Table of contents: including [toc] at the top of a document generates a table of contents from headings in the document. user name tags: You can generate a name tag (e.g. to label a specific addition to the document) using > [name=taylorreiter] . RMarkdown: Rmarkdown weaves together code, code outputs, and text into a single document. Documents begin with a yaml header to specify how the document should be rendered. Code blocks begin with three back ticks, but code blocks that should be executed start with {r} after the three back ticks. The content inside the curly brackets controls the run and rendering behavior of a specific code block and its outputs.","title":"Flavors of markdown"},{"location":"arcadia-users-group/20220822-intro-to-markdown-syntax/lesson/#more-on-markdown","text":"See below for links to additional materials on markdown. GitHub markdown cheatsheet RMarkdown cheatsheet The Carpentries alpha introduction to markdown lesson","title":"More on markdown"},{"location":"arcadia-users-group/20220829-project-organization/lesson/","text":"Project organization and file & resource management Bioinformatics data analyses often have many different input and output files with hundreds to thousands of intermediate files. Keeping all of these files organized along with the code you used to obtain or make them and any other notes you may have is hard . This lessons covers some ideas for how to stay organized. There is no one-size-fits all, either for data analysts or for projects. You'll have to experiment to see what strategy works best for you and inevitably you'll get better with practice and feedback. Some not bad ideas to consider Use a good but flexible directory structure The foundation to any computational project is a directory structure you use to store your data, notes, and analysis results. Directory structures are a place where you can choose your own adventure. Below we show two different strategies. Both are from real data analysis project but have been simplified to fit better on the screen (see repository for left project here and the right project here ). The images were generated using the linux program tree with parameter -L 2 to reduce the depth of folders and files shown to two levels. On the left side, the project created four top level directories for each of the four analyses they they undertook. Inside of each directory is a set of folder with data, models, and logs, and then a bunch of ordered jupyter notebooks that record the analyses themselves. There are two other folders as well, one to explore the data ( explor_simulation_approach ) and one with scripts that were used across all of the analyses ( generic_expression_patterns_modules ). On the right side, the project created a directory for inputs , outputs , software envs , notebooks , scripts , and a sandbox . The inputs folder holds all of the inputs to the data analysis workflow which includes data files produced in the lab or any file downloaded from the internet. The outputs folder holds all of the analysis outputs, named by input data type and the tool that worked on the data. The notebooks folder holds jupyter notebooks that analyze files in the outputs directory. The files are named by date and the type of analysis that takes place in them to help identify interesting notebooks at much later times. The scripts folder contains the scripts used over and over to produce the output files from the input files. The sandbox folder documents random analyses that haven't been fully baked yet but have been experimented with and so are documented. Lastly, the envs folder documents conda software environments for all of the software installed and used for this analysis. There are many other great organization strategies, and often they may vary by project. The important part is to think about the directory structure as you're starting a project to set yourself up for organizational success. Use consistent, self-documenting names Using consistent and descriptive identifiers for your files, scripts, variables, workflows, projects, and even manuscripts helps keep your projects organized and interpretable for you and collaborators. For workflow systems, this strategy can be implemented by tagging output files with a descriptive identifier for each analysis step, either in the file name or by placing output files within a descriptive output folder. For example, the file shown in the figure below has been pre-processed with a quality control trimming step. For data analysis project with many steps, placing results from each step of your analysis in isolated, descriptive folders is one strategy for keeping your project work space clean and organized. Document data and analysis exploration using computational notebooks Computational notebooks allow users to combine narrative, code, and code output (e.g., visualizations) in a single location, enabling the user to conduct analysis and visually assess the results in a single file. These notebooks allow for fully documented iterative analysis development and are particularly useful for data exploration and developing visualizations prior to integration into a workflow or as a report generated by a workflow that can be shared with collaborators. Version control your project As your project develops, version control allows you to keep track of changes over time. Several methods can be used to track changes even without version control software, including frequent hard drive backups or manually saving different versions of the same file -- e.g., by appending the date to a script name or appending \"version_1\" or \"version_FINAL\" to a manuscript draft. However, version control systems such as Git or Mercurial can both simplify and standardize this process, particularly as workflow length and complexity increase. These systems can keep track of all changes over time, even across multiple systems, scripting languages, and project contributors. They can help you keep your data analysis project clean -- you can safely delete code you no longer are using, knowing that previous versions are recorded. Furthermore, backing up your version-controlled analysis in an online repository such as GitHub can provide insurance against computer crashes or other accidents. Use relative paths A path is how you refer to a file on your computer so that the computer can do something with it. A relative path is a path that's specified from a specific location on your computer, in this case relative to where your analysis directory lives. In the figure below, if our analysis was taking place in the thing directory, we would look into the backup folder using the path backup . This is as opposed to it's absolute path, /Users/things/backup . Using relative paths ensures that if someone else picks up your analysis folder and tries to re-do what you have done, the paths will still be accurate on their system. Systematically document your project Pervasive documentation provides indispensable context for biological insights derived from an analysis, facilitates transparency in research, and increases re-usability of the analysis code. Good documentation covers all aspects of a project, including organization of files and results, clear and commented code, and accompanying explanatory documents for design decisions and metadata. Some good anchors to reach for: * Every project should have a descriptive README file that describes the purpose the project. * Each script should have at minimum a comment at the top describing what the script is used for. * File and folder names should be descriptive. * If possible, try and use a workflow manager. These tools document the connection between different analysis steps and are in and of themselves a minimal form of documentation. Credits The ideas presented in this document are from the following sources, sometimes verbatim: Streamlining data-intensive biology with workflow systems, GigaScience, Volume 10, Issue 1, January 2021, giaa140, https://doi.org/10.1093/gigascience/giaa140 The Data Carpentry Shell Genomics lesson","title":"Project organization and file & resource management"},{"location":"arcadia-users-group/20220829-project-organization/lesson/#project-organization-and-file-resource-management","text":"Bioinformatics data analyses often have many different input and output files with hundreds to thousands of intermediate files. Keeping all of these files organized along with the code you used to obtain or make them and any other notes you may have is hard . This lessons covers some ideas for how to stay organized. There is no one-size-fits all, either for data analysts or for projects. You'll have to experiment to see what strategy works best for you and inevitably you'll get better with practice and feedback.","title":"Project organization and file &amp; resource management"},{"location":"arcadia-users-group/20220829-project-organization/lesson/#some-not-bad-ideas-to-consider","text":"","title":"Some not bad ideas to consider"},{"location":"arcadia-users-group/20220829-project-organization/lesson/#use-a-good-but-flexible-directory-structure","text":"The foundation to any computational project is a directory structure you use to store your data, notes, and analysis results. Directory structures are a place where you can choose your own adventure. Below we show two different strategies. Both are from real data analysis project but have been simplified to fit better on the screen (see repository for left project here and the right project here ). The images were generated using the linux program tree with parameter -L 2 to reduce the depth of folders and files shown to two levels. On the left side, the project created four top level directories for each of the four analyses they they undertook. Inside of each directory is a set of folder with data, models, and logs, and then a bunch of ordered jupyter notebooks that record the analyses themselves. There are two other folders as well, one to explore the data ( explor_simulation_approach ) and one with scripts that were used across all of the analyses ( generic_expression_patterns_modules ). On the right side, the project created a directory for inputs , outputs , software envs , notebooks , scripts , and a sandbox . The inputs folder holds all of the inputs to the data analysis workflow which includes data files produced in the lab or any file downloaded from the internet. The outputs folder holds all of the analysis outputs, named by input data type and the tool that worked on the data. The notebooks folder holds jupyter notebooks that analyze files in the outputs directory. The files are named by date and the type of analysis that takes place in them to help identify interesting notebooks at much later times. The scripts folder contains the scripts used over and over to produce the output files from the input files. The sandbox folder documents random analyses that haven't been fully baked yet but have been experimented with and so are documented. Lastly, the envs folder documents conda software environments for all of the software installed and used for this analysis. There are many other great organization strategies, and often they may vary by project. The important part is to think about the directory structure as you're starting a project to set yourself up for organizational success.","title":"Use a good but flexible directory structure"},{"location":"arcadia-users-group/20220829-project-organization/lesson/#use-consistent-self-documenting-names","text":"Using consistent and descriptive identifiers for your files, scripts, variables, workflows, projects, and even manuscripts helps keep your projects organized and interpretable for you and collaborators. For workflow systems, this strategy can be implemented by tagging output files with a descriptive identifier for each analysis step, either in the file name or by placing output files within a descriptive output folder. For example, the file shown in the figure below has been pre-processed with a quality control trimming step. For data analysis project with many steps, placing results from each step of your analysis in isolated, descriptive folders is one strategy for keeping your project work space clean and organized.","title":"Use consistent, self-documenting names"},{"location":"arcadia-users-group/20220829-project-organization/lesson/#document-data-and-analysis-exploration-using-computational-notebooks","text":"Computational notebooks allow users to combine narrative, code, and code output (e.g., visualizations) in a single location, enabling the user to conduct analysis and visually assess the results in a single file. These notebooks allow for fully documented iterative analysis development and are particularly useful for data exploration and developing visualizations prior to integration into a workflow or as a report generated by a workflow that can be shared with collaborators.","title":"Document data and analysis exploration using computational notebooks"},{"location":"arcadia-users-group/20220829-project-organization/lesson/#version-control-your-project","text":"As your project develops, version control allows you to keep track of changes over time. Several methods can be used to track changes even without version control software, including frequent hard drive backups or manually saving different versions of the same file -- e.g., by appending the date to a script name or appending \"version_1\" or \"version_FINAL\" to a manuscript draft. However, version control systems such as Git or Mercurial can both simplify and standardize this process, particularly as workflow length and complexity increase. These systems can keep track of all changes over time, even across multiple systems, scripting languages, and project contributors. They can help you keep your data analysis project clean -- you can safely delete code you no longer are using, knowing that previous versions are recorded. Furthermore, backing up your version-controlled analysis in an online repository such as GitHub can provide insurance against computer crashes or other accidents.","title":"Version control your project"},{"location":"arcadia-users-group/20220829-project-organization/lesson/#use-relative-paths","text":"A path is how you refer to a file on your computer so that the computer can do something with it. A relative path is a path that's specified from a specific location on your computer, in this case relative to where your analysis directory lives. In the figure below, if our analysis was taking place in the thing directory, we would look into the backup folder using the path backup . This is as opposed to it's absolute path, /Users/things/backup . Using relative paths ensures that if someone else picks up your analysis folder and tries to re-do what you have done, the paths will still be accurate on their system.","title":"Use relative paths"},{"location":"arcadia-users-group/20220829-project-organization/lesson/#systematically-document-your-project","text":"Pervasive documentation provides indispensable context for biological insights derived from an analysis, facilitates transparency in research, and increases re-usability of the analysis code. Good documentation covers all aspects of a project, including organization of files and results, clear and commented code, and accompanying explanatory documents for design decisions and metadata. Some good anchors to reach for: * Every project should have a descriptive README file that describes the purpose the project. * Each script should have at minimum a comment at the top describing what the script is used for. * File and folder names should be descriptive. * If possible, try and use a workflow manager. These tools document the connection between different analysis steps and are in and of themselves a minimal form of documentation.","title":"Systematically document your project"},{"location":"arcadia-users-group/20220829-project-organization/lesson/#credits","text":"The ideas presented in this document are from the following sources, sometimes verbatim: Streamlining data-intensive biology with workflow systems, GigaScience, Volume 10, Issue 1, January 2021, giaa140, https://doi.org/10.1093/gigascience/giaa140 The Data Carpentry Shell Genomics lesson","title":"Credits"},{"location":"arcadia-users-group/20220906-intro-to-shell1/lesson/","text":"Introduction to shell part 1 What is the shell and why do we care about it? A shell is a computer program that presents a command line interface which allows you to control your computer using commands entered with a keyboard. This is as opposed to controlling your computer using a graphical user interface (GUI) with a mouse/keyboard/touchscreen combination. Why should we care about learning the shell? Many bioinformatics tools can only be used through a command-line interface, and/or have extra capabilities in the command-line version that are not available in the GUI (e.g. BLAST). It makes our work less error-prone. When humans do the same thing a hundred different times (or even ten times), we're likely to make a mistake. Your computer can do the same thing a thousand times with no mistakes. This also frees us up to do other things we can't automate, like the science part. The shell makes our work more reproducible. When we carry out our work in a command-line interface (rather than a GUI), we can keep an exact record of all steps, which we can use to re-do our work when we need to. It also gives us a way to unambiguously communicate what we've done to others. Many bioinformatic tasks require large amounts of computing power and can't realistically be run on our own machines. These tasks are best performed using remote computers or cloud computing, which can only be accessed through a shell. Common shell terminology Many different terms are used to refer to the shell or its computing environment. Some commons ones are included below with their definitions. Term What it is shell what we use to talk to the computer; anything where you are pointing and clicking with a mouse is a G raphical U ser I nterface ( GUI ) shell; something with text only is a C ommand L ine I nterface ( CLI ) shell command line a text-based environment capable of taking input and providing output (a \"terminal\" is the same idea) Unix a family of operating systems bash the most common programming language used at a Unix command-line. This is also a shell. Accessing the shell for this lesson For this lesson, we need to have access to a Unix shell. Click the button below to launch a shell through binder . This will launch a computer in the cloud. You'll interact with this computer through your browser. Click the Terminal button to launch a Terminal that we will work with for the rest of the lesson. More information on binder and what happens when you click the launch binder button. Binder is a service that turns a Git repo into a collection of interactive notebooks. When a repository is configured to run as a binder, passing the GitHub repository URL to binder starts the binder-building process. Binder first builds a docker image that contains all of the software installations specified by a special set of files in the GitHub repository. A docker image is a set of instructions that are used to create a docker container. A docker container is a runnable instance of a docker image -- it's an encapsulated computing environment that can be used to reproducibly install sets of software on diverse computers. Armed with the docker container, binder launches an \"instance\" in the cloud (either on Google Cloud or AWS typically) on which it runs the docker container. Binder does some additional work in the background -- if no software configuration files are provided in the GitHub repo, or if those contain a minimal set of software, binder will by default include JupyterHub in the docker. When the cloud instance is launched, this is the screen you interact with. You interact with the cloud instance in your browser. Binders are ephemeral instances -- after a period of inactivity, the instance is automatically shut down, and any work you have done will be lost. You're able to download files from your work before the instance is shut down if you do want to save anything. You may notice that this instance already has a bunch of files on it. And that these files look suspiciously exactly like the files in the GitHub repository Arcadia-Science/arcadia-computational-training . That's because that's the repository we used to build the binder from. Running commands A \"command\" is a set of typed instructions entered at the command line prompt. The general syntax working at the command line goes like this: command argument . Arguments (which can also be referred to as \"flags\" or \"options\" or \"parameters\") can be optional or required based on the command being used. The dollar sign is a prompt , which shows us that the shell is waiting for input; your shell may use a different character as a prompt and may add information before the prompt. When typing commands, do not type the prompt, only the commands that follow it. In this lesson, we've omitted the prompt so you won't have to worrry about remembering to not include it. Let's find out where we are by running a command called pwd (which stands for \"print working directory\"). At any moment, our current working directory is our current default directory. This is the directory that the computer assumes we want to run commands in unless we explicitly specify something else. pwd We see: /home/jovyan In this case, jovyan is our user name. This was configured by the service that launched our instance. You can see more context about the name here . Looking at files with ls , head , and tail Let's look at how our file system is organized. We can see what files and subdirectories are in this directory by running ls , which stands for \"listing\": ls We see: docs LICENSE Makefile mkdocs.yml README.md ls prints the names of the files and directories in the current directory in alphabetical order, arranged neatly into columns. In this case, we have all the files that were in the GitHub repo that was used to build this binder. This repo contains all of the computational training material at Arcadia, as well as some directions to turn the repo into a nicely rendered website . (side note -- we'll have a training in a few weeks demonstrating how to turn a GitHub repo into a website). We can make the ls output more comprehensible by using the flag -F , which tells ls to add a trailing / to the names of directories: ls -F Which produces: docs/ LICENSE Makefile mkdocs.yml README.md If we want to preview the contents of these files, we can start by using head . head README.md This outputs the first 10 lines of the file README.md : # Arcadia Computational Training This repository houses computational training materials developed for or delivered at Arcadia. The repository is still a work in progress so things may shift around as we settle on an organizational structure. The content in this repository is meant to present a sensible set of defaults for common computational tasks in biology at the time that the content is added to the repo. ## Building and deploying a site with MkDocs What if we only wanted to see the first 5 lines? We could use the -n flag: head -n 5 README.md Which would then only print the first 5 lines of the README.md file to the terminal: # Arcadia Computational Training This repository houses computational training materials developed for or delivered at Arcadia. The repository is still a work in progress so things may shift around as we settle on an organizational structure. If we instead wanted to see the last line of a file, we could use tail : tail -n 1 README.md Which shows: Then in your browser, navigate to the URL printed to standard out. The Unix file system structure Your computer stores file locations in a hierarchical structure. You are likely already used to navigating through this stucture by clicking on various folders (also known as directories) in a Windows Explorer window or a Mac Finder window. Just like we need to select the appropriate files in the appropriate locations there (in a GUI), we need to do the same when working at a command-line interface. What this means in practice is that each file and directory has its own \"address\", and that address is called its \"path\". Here is an image of an example file-system structure: There are two special locations in all Unix-based systems: the \"root\" location and the current user\u2019s \"home\" location. \"Root\" is where the address system of the computer starts; \"home\" is where the current user\u2019s location starts. We tell the command line where files and directories are located by providing their address, their \"path\". If we use the pwd command like we did above, we can find out what the path is for the directory we are sitting in: pwd Absolute vs relative paths There are two ways to specify the path (address) of the file we want to do something to: An absolute path is an address that starts from one of those special locations: either the \"root\" / or the \"home\" ~/ location. A relative path is an address that starts from wherever we are currently sitting. For example, let's look again at the head command we ran above: head README.md What we are actually doing here is using a relative path to specify where the \"README.md\" file is located. This is because the command line automatically looks in the current working directory if we don't specify anything else about its location (it's starting from where we are). We can also run the same command on the same file using an absolute path : head ~/README.md head /home/joyvan/README.md The previous three commands all point to the same file. But the first way, head README.md , will only work if we are entering it while \"sitting\" in the directory that holds that file, while the second and third ways will work no matter where we happen to be in the computer. It is important to always think about where we are in the computer when working at the command line. One of the most common errors/easiest mistakes to make is trying to do something to a file that isn't where we think it is. Let's run head on the \"README.md\" file again, and then let's try it on another file: \"index.md\": head README.md head index.md Here the head command works fine on \"README.md\", but we get an error message when we call it on \"index.md\" telling us no such file or directory. If we run the ls command, we can see the computer is absolutely right \u2013 spoiler alert: it usually is \u2013 and there is no file here named \"index.md\". Here is how we can run head on \"index.md\" by specifying an accurate relative path to that file: head docs/index.md Tab-completion Tab-completion is a huge time-saver, but even more importantly it helps prevent mistakes. If we are trying to specify a file that's in our current working directory, we can begin typing its name and then press the tab key to complete it. If there is only one possible way to finish what we've started typing, it will complete it entirely for us. If there is more than one possible way to finish what we've started typing, it will complete as far as it can, and then hitting tab twice quickly will show all the possible options. If tab-complete does not do either of those things, then the file we're looking for is not in the directory we're pointing to. Navigating by the command line We can also move into the directory containing the file we want to work with by using the cd command ( c hange d irectory). This command takes a positional argument that is the path (address) of the directory we want to change into. This can be a relative path or an absolute path. Here we'll use the relative path of the subdirectory, \"docs\", to change into it (use tab-completion!): cd docs pwd ls head index.md Great. But now how do we get back \"up\" to the directory above us? One way would be to provide an absolute path, like cd /home/joyvan , but there is also a shortcut. .. are special characters that act as a relative path specifying \"up\" one level \u2013 one directory \u2013 from wherever we currently are. So we can provide that as the positional argument to cd to get back to where we started: cd .. pwd ls The .. can also be combined to specify going up multiple levels: ls ../.. Summary Terms introduced: Term What it is path the address system the computer uses to keep track of files and directories root where the address system of the computer starts, / home where the current user's location starts, ~/ absolute path an address that starts from a specified location, i.e. root, or home relative path an address that starts from wherever we are tab completion auto complete file paths once you start typing Commands introduced: Command Function head prints out the first lines of a file tail prints out the last lines of a file pwd tells us where we are in the computer ( p rint w orking d irectory) ls lists contents of a directory ( l i s t) cd c hange d irectories Special characters introduced: Characters Meaning / the computer's root location ~/ the user's home location ../ specifies a directory one level \"above\" the current working directory Credit This lesson was modified from: ANGUS 2019: https://angus.readthedocs.io/en/2019/shell_intro/shell-getting-started-01.html Data Carpentry Genomics: https://datacarpentry.org/shell-genomics/","title":"Introduction to shell part 1"},{"location":"arcadia-users-group/20220906-intro-to-shell1/lesson/#introduction-to-shell-part-1","text":"","title":"Introduction to shell part 1"},{"location":"arcadia-users-group/20220906-intro-to-shell1/lesson/#what-is-the-shell-and-why-do-we-care-about-it","text":"A shell is a computer program that presents a command line interface which allows you to control your computer using commands entered with a keyboard. This is as opposed to controlling your computer using a graphical user interface (GUI) with a mouse/keyboard/touchscreen combination. Why should we care about learning the shell? Many bioinformatics tools can only be used through a command-line interface, and/or have extra capabilities in the command-line version that are not available in the GUI (e.g. BLAST). It makes our work less error-prone. When humans do the same thing a hundred different times (or even ten times), we're likely to make a mistake. Your computer can do the same thing a thousand times with no mistakes. This also frees us up to do other things we can't automate, like the science part. The shell makes our work more reproducible. When we carry out our work in a command-line interface (rather than a GUI), we can keep an exact record of all steps, which we can use to re-do our work when we need to. It also gives us a way to unambiguously communicate what we've done to others. Many bioinformatic tasks require large amounts of computing power and can't realistically be run on our own machines. These tasks are best performed using remote computers or cloud computing, which can only be accessed through a shell.","title":"What is the shell and why do we care about it?"},{"location":"arcadia-users-group/20220906-intro-to-shell1/lesson/#common-shell-terminology","text":"Many different terms are used to refer to the shell or its computing environment. Some commons ones are included below with their definitions. Term What it is shell what we use to talk to the computer; anything where you are pointing and clicking with a mouse is a G raphical U ser I nterface ( GUI ) shell; something with text only is a C ommand L ine I nterface ( CLI ) shell command line a text-based environment capable of taking input and providing output (a \"terminal\" is the same idea) Unix a family of operating systems bash the most common programming language used at a Unix command-line. This is also a shell.","title":"Common shell terminology"},{"location":"arcadia-users-group/20220906-intro-to-shell1/lesson/#accessing-the-shell-for-this-lesson","text":"For this lesson, we need to have access to a Unix shell. Click the button below to launch a shell through binder . This will launch a computer in the cloud. You'll interact with this computer through your browser. Click the Terminal button to launch a Terminal that we will work with for the rest of the lesson. More information on binder and what happens when you click the launch binder button. Binder is a service that turns a Git repo into a collection of interactive notebooks. When a repository is configured to run as a binder, passing the GitHub repository URL to binder starts the binder-building process. Binder first builds a docker image that contains all of the software installations specified by a special set of files in the GitHub repository. A docker image is a set of instructions that are used to create a docker container. A docker container is a runnable instance of a docker image -- it's an encapsulated computing environment that can be used to reproducibly install sets of software on diverse computers. Armed with the docker container, binder launches an \"instance\" in the cloud (either on Google Cloud or AWS typically) on which it runs the docker container. Binder does some additional work in the background -- if no software configuration files are provided in the GitHub repo, or if those contain a minimal set of software, binder will by default include JupyterHub in the docker. When the cloud instance is launched, this is the screen you interact with. You interact with the cloud instance in your browser. Binders are ephemeral instances -- after a period of inactivity, the instance is automatically shut down, and any work you have done will be lost. You're able to download files from your work before the instance is shut down if you do want to save anything. You may notice that this instance already has a bunch of files on it. And that these files look suspiciously exactly like the files in the GitHub repository Arcadia-Science/arcadia-computational-training . That's because that's the repository we used to build the binder from.","title":"Accessing the shell for this lesson"},{"location":"arcadia-users-group/20220906-intro-to-shell1/lesson/#running-commands","text":"A \"command\" is a set of typed instructions entered at the command line prompt. The general syntax working at the command line goes like this: command argument . Arguments (which can also be referred to as \"flags\" or \"options\" or \"parameters\") can be optional or required based on the command being used. The dollar sign is a prompt , which shows us that the shell is waiting for input; your shell may use a different character as a prompt and may add information before the prompt. When typing commands, do not type the prompt, only the commands that follow it. In this lesson, we've omitted the prompt so you won't have to worrry about remembering to not include it. Let's find out where we are by running a command called pwd (which stands for \"print working directory\"). At any moment, our current working directory is our current default directory. This is the directory that the computer assumes we want to run commands in unless we explicitly specify something else. pwd We see: /home/jovyan In this case, jovyan is our user name. This was configured by the service that launched our instance. You can see more context about the name here .","title":"Running commands"},{"location":"arcadia-users-group/20220906-intro-to-shell1/lesson/#looking-at-files-with-ls-head-and-tail","text":"Let's look at how our file system is organized. We can see what files and subdirectories are in this directory by running ls , which stands for \"listing\": ls We see: docs LICENSE Makefile mkdocs.yml README.md ls prints the names of the files and directories in the current directory in alphabetical order, arranged neatly into columns. In this case, we have all the files that were in the GitHub repo that was used to build this binder. This repo contains all of the computational training material at Arcadia, as well as some directions to turn the repo into a nicely rendered website . (side note -- we'll have a training in a few weeks demonstrating how to turn a GitHub repo into a website). We can make the ls output more comprehensible by using the flag -F , which tells ls to add a trailing / to the names of directories: ls -F Which produces: docs/ LICENSE Makefile mkdocs.yml README.md If we want to preview the contents of these files, we can start by using head . head README.md This outputs the first 10 lines of the file README.md : # Arcadia Computational Training This repository houses computational training materials developed for or delivered at Arcadia. The repository is still a work in progress so things may shift around as we settle on an organizational structure. The content in this repository is meant to present a sensible set of defaults for common computational tasks in biology at the time that the content is added to the repo. ## Building and deploying a site with MkDocs What if we only wanted to see the first 5 lines? We could use the -n flag: head -n 5 README.md Which would then only print the first 5 lines of the README.md file to the terminal: # Arcadia Computational Training This repository houses computational training materials developed for or delivered at Arcadia. The repository is still a work in progress so things may shift around as we settle on an organizational structure. If we instead wanted to see the last line of a file, we could use tail : tail -n 1 README.md Which shows: Then in your browser, navigate to the URL printed to standard out.","title":"Looking at files with ls, head, and tail"},{"location":"arcadia-users-group/20220906-intro-to-shell1/lesson/#the-unix-file-system-structure","text":"Your computer stores file locations in a hierarchical structure. You are likely already used to navigating through this stucture by clicking on various folders (also known as directories) in a Windows Explorer window or a Mac Finder window. Just like we need to select the appropriate files in the appropriate locations there (in a GUI), we need to do the same when working at a command-line interface. What this means in practice is that each file and directory has its own \"address\", and that address is called its \"path\". Here is an image of an example file-system structure: There are two special locations in all Unix-based systems: the \"root\" location and the current user\u2019s \"home\" location. \"Root\" is where the address system of the computer starts; \"home\" is where the current user\u2019s location starts. We tell the command line where files and directories are located by providing their address, their \"path\". If we use the pwd command like we did above, we can find out what the path is for the directory we are sitting in: pwd","title":"The Unix file system structure"},{"location":"arcadia-users-group/20220906-intro-to-shell1/lesson/#absolute-vs-relative-paths","text":"There are two ways to specify the path (address) of the file we want to do something to: An absolute path is an address that starts from one of those special locations: either the \"root\" / or the \"home\" ~/ location. A relative path is an address that starts from wherever we are currently sitting. For example, let's look again at the head command we ran above: head README.md What we are actually doing here is using a relative path to specify where the \"README.md\" file is located. This is because the command line automatically looks in the current working directory if we don't specify anything else about its location (it's starting from where we are). We can also run the same command on the same file using an absolute path : head ~/README.md head /home/joyvan/README.md The previous three commands all point to the same file. But the first way, head README.md , will only work if we are entering it while \"sitting\" in the directory that holds that file, while the second and third ways will work no matter where we happen to be in the computer. It is important to always think about where we are in the computer when working at the command line. One of the most common errors/easiest mistakes to make is trying to do something to a file that isn't where we think it is. Let's run head on the \"README.md\" file again, and then let's try it on another file: \"index.md\": head README.md head index.md Here the head command works fine on \"README.md\", but we get an error message when we call it on \"index.md\" telling us no such file or directory. If we run the ls command, we can see the computer is absolutely right \u2013 spoiler alert: it usually is \u2013 and there is no file here named \"index.md\". Here is how we can run head on \"index.md\" by specifying an accurate relative path to that file: head docs/index.md","title":"Absolute vs relative paths"},{"location":"arcadia-users-group/20220906-intro-to-shell1/lesson/#tab-completion","text":"Tab-completion is a huge time-saver, but even more importantly it helps prevent mistakes. If we are trying to specify a file that's in our current working directory, we can begin typing its name and then press the tab key to complete it. If there is only one possible way to finish what we've started typing, it will complete it entirely for us. If there is more than one possible way to finish what we've started typing, it will complete as far as it can, and then hitting tab twice quickly will show all the possible options. If tab-complete does not do either of those things, then the file we're looking for is not in the directory we're pointing to.","title":"Tab-completion"},{"location":"arcadia-users-group/20220906-intro-to-shell1/lesson/#navigating-by-the-command-line","text":"We can also move into the directory containing the file we want to work with by using the cd command ( c hange d irectory). This command takes a positional argument that is the path (address) of the directory we want to change into. This can be a relative path or an absolute path. Here we'll use the relative path of the subdirectory, \"docs\", to change into it (use tab-completion!): cd docs pwd ls head index.md Great. But now how do we get back \"up\" to the directory above us? One way would be to provide an absolute path, like cd /home/joyvan , but there is also a shortcut. .. are special characters that act as a relative path specifying \"up\" one level \u2013 one directory \u2013 from wherever we currently are. So we can provide that as the positional argument to cd to get back to where we started: cd .. pwd ls The .. can also be combined to specify going up multiple levels: ls ../..","title":"Navigating by the command line"},{"location":"arcadia-users-group/20220906-intro-to-shell1/lesson/#summary","text":"","title":"Summary"},{"location":"arcadia-users-group/20220906-intro-to-shell1/lesson/#credit","text":"This lesson was modified from: ANGUS 2019: https://angus.readthedocs.io/en/2019/shell_intro/shell-getting-started-01.html Data Carpentry Genomics: https://datacarpentry.org/shell-genomics/","title":"Credit"},{"location":"arcadia-users-group/20220912-intro-to-shell2/lesson/","text":"Introduction to shell part 2 In the previous lesson , we introduced the concept of the Unix shell, showed how to navigate the shell, and introduced the commands head , tail , pwd , ls , and cd . In this lesson, we'll cover how to create and delete directories (folders), how to move, copy, and delete files, and how to search the contents of a file more systematically. Accessing the shell for this lesson For this lesson, we need to have access to a Unix shell. Click the button below to launch a shell through binder . This will launch a computer in the cloud. You'll interact with this computer through your browser. Click the Terminal button to launch a Terminal that we will work with for the rest of the lesson. More information on binder and what happens when you click the launch binder button. Binder is a service that turns a Git repo into a collection of interactive notebooks. When a repository is configured to run as a binder, passing the GitHub repository URL to binder starts the binder-building process. binder first builds a docker image that contains all of the software installations specified by a special set of files in the GitHub repository. A docker image is a set of instructions that are used to create a docker container. A docker container is a runnable instance of a docker image -- it's an encapsulated computing environment that can be used to reproducibly install sets of software on diverse computers. Armed with the docker container, binder launches an \"instance\" in the cloud (either on Google Cloud or AWS typically) on which it runs the docker container. Binder does some additional work in the background -- if no software configuration files are provided in the GitHub repo, or if those contain a minimal set of software, binder will by default include JupyterHub in the docker. When the cloud instance is launched, this is the screen you interact with. You interact with the cloud instance in your browser. Binders are ephemeral instances -- after a period of inactivity, the instance is automatically shut down, and any work you have done will be lost. You're able to download files from your work before the instance is shut down if you do want to save anything. You may notice that this instance already has a bunch of files on it. And that these files look suspiciously exactly like the files in the GitHub repository Arcadia-Science/arcadia-computational-training . That's because that's the repository we used to build the binder from. Interrogating the content of files Looking at whole files As we learned in the previous lesson, we can investigate the beginning and end of a file using head or tail . What if you want to see all of the contents of a file? The command cat will print the contents of a file to stdout -- the standard output stream that prints to the shell. Let's run ls and see what files we're working with. ls And then look at the contents of mkdocs.yml using cat : cat mkdocs.yml We see the following content: site_name: Arcadia Science Computational Training site_url: http://arcadia-science.github.io/arcadia-computational-training repo_name: arcadia-computational-training repo_url: https://github.com/arcadia-science/arcadia-computational-training edit_uri: \"\" copyright: 'Copyright &copy; 2022 <a href=\"https://www.arcadiascience.com\">Arcadia Science</a>' # change directory names here to reflect directories in the repository docs_dir: docs site_dir: site theme: name: lux extra_css: - css/extra.css # organize site structure and give a title for each page # paths are relative to the docs directory nav: - \"Home\": \"index.md\" - \"Arcadia Users Group\": \"arcadia-users-group/overview.md\" - \"Workshops\": \"workshops/overview.md\" - \"Contribute\": \"CONTRIBUTING.md\" More information on YAML ( .yml , .yaml ) file formats. YAML originally stood for Yet Another Markup Language as it was originally developed and released around the same time as many other markup languages (HTML, etc). Now, it stands for YAML Ain't A Markup Language. Unlike Markdown which strives to be human-readable and parseable into pretty documents, YAML is a data-oriented. YAML is a human-friendly data serialization language for all programming language. It's a file format commonly used to specify configuration files. Configuration files specify where a computer program can find files it needs, parameters for when the program runs, or other metadata for a program. In this case, the output of the file is palatable; we can grok the whole files contents by printing it all to stdout. Let's use cat on a longer file: cat docs/arcadia-users-group/20220822-intro-to-markdown-syntax/lesson.md When we run this command, the output takes up more than we can see without scrolling. If we were to run cat on a really long file, it may take seconds, minutes, or even hours to print all of the contents to the screen. Enter less . less is a terminal pager that shows a file's contents one screen at a time. less docs/arcadia-users-group/20220822-intro-to-markdown-syntax/lesson.md This allows us to interactivley view the contents of a file. To navigate the lesson screen, we can use key board arrows (line-by-line navigation), the space bar (page jump), or even special combinations of keys ( GG jumps to the bottom of the file). To exit out of less , press the q key. Looking for specific content in a file Sometimes, we care less about all the things in a file and instead want to find something specific. grep is a great command for this. grep ( g lobal r egular e x p ression) is a search tool. It looks through text files for strings (sequences of characters). In its default usage, grep will look for whatever string of characters you give it (1st positional argument), in whichever file you specify (2nd positional argument), and then print out the lines that contain what you searched for. Let's try it: grep \"Arcadia\" mkdocs.yml site_name: Arcadia Science Computational Training copyright: 'Copyright &copy; 2022 <a href=\"https://www.arcadiascience.com\">Arcadia Science</a>' - \"Arcadia Users Group\": \"arcadia-users-group/overview.md\" We see Arcadia appears three times. If we grep for a string that is not in the file, nothing will be printed to the screen: grep \"hippo\" mkdocs.yml Figuring out how long a file is Sometimes we care less about the specific contents of a file and instead we want a general overview of the contents of a file. This can be helpful when you download a large file -- you may know the number of lines you expect to see inside of it. We can use the wc w ord c ount command to get a summary of the number of lines, words, and characters in a file. wc mkdocs.yml This file has 25 lines, 75 words, and 782 characters. 25 75 782 mkdocs.yml wc also accepts flags -- the -l flag limits the output of wc to only the number of lines in a file. wc -l The options below allow you to select which counts are printed. -l , --lines - print the number of lines. -w , --words - print the number of words. -m , --chars - print the number of characters. -c , --bytes - print the number of bytes. -L , --max-line-length - print the length of the longest line. Copying, moving, and renaming files The commands cp and mv ( c o p y and m o v e) have the same basic structure. They both require two positional arguments \u2013 the first is the file you want to act on, and the second is where you want it to go (which can include the name you want to give it). To see how this works, let's make a copy of \"example.txt\": ls cp mkdocs.yml mkdocs_copy.yml ls By giving the second argument a name and nothing else (meaning no path in front of the name), we are implicitly saying we want it copied to where we currently are. To make a copy and put it somewhere else, like in our subdirectory docs , we could change the second positional argument using a relative path (\"relative\" because it starts from where we currently are): ls docs cp mkdocs.yml docs/mkdocs_copy.yml ls docs To copy a file to that subdirectory but keep the same name, we could type the whole name out, but we can also provide the directory but leave off the file name: cp mkdocs.yml docs/ ls docs/ If we wanted to copy something from somewhere else to our current working directory and keep the same name, we can use another special character, a period ( . ), which specifies the current working directory: ls cp docs/index.md . ls The mv command is used to move files. Let's move the README.md file into the docs subdirectory: ls ls docs/ mv README.md docs/ ls ls docs/ The mv command is also used to rename files. This may seem strange at first, but remember that the path (address) of a file actually includes its name too (otherwise everything in the same directory would have the same path). ls mv mkdocs_copy.yml mkdocs_new.yml ls To delete files there is the rm command ( r e m ove). This command requires at least one argument specifying the file we want to delete. Typically, this command does not ask you to confirm your actions and files are permanently deleted instead of being moved to a Trash directory. ls rm mkdocs_new.yml ls Creating and removing directories We can make a new directory with the command mkdir (for m a k e dir ectory): ls mkdir subset ls And similarly, directories can be deleted with rmdir (for r e m ove dir ectory): rmdir subset/ ls The command line is a little more forgiving when trying to delete a directory. If the directory is not empty, rmdir will give you an error. rmdir docs/ The rm command can also be used to delete a directory, but it needs the flags -rf added to. -r stands for recursive (meaning delete all files and folders located within the specified directory), while the f stands for for force. Unlike rmdir , this command will not check whether you want the directory deleted before removing it and it will remove a directory even if it isn't empty. It's good to get into the habit of mentally or actually checking where you are and what you're deleting before removing something with rm -rf as this command can delete A LOT of files very quickly if you enter the wrong thing. mkdir subset ls rm -rf subset Summary Commands introduced: Command Function cat prints the contents of a file to stdout less allows us to browse a file (exit with q key) wc count lines, words, and characters in a file cp copy a file or directory mv move or rename a file or directory rm delete a file or directory mkdir create a directory rmdir delete a directory Special characters introduced: Characters Meaning . specifies the current working directory Credits This lesson was modified from the following sources: ANGUS 2019: https://angus.readthedocs.io/en/2019/shell_intro/shell-six-glorious-commands-04.html https://angus.readthedocs.io/en/2019/shell_intro/shell-working-02.html","title":"Introduction to shell part 2"},{"location":"arcadia-users-group/20220912-intro-to-shell2/lesson/#introduction-to-shell-part-2","text":"In the previous lesson , we introduced the concept of the Unix shell, showed how to navigate the shell, and introduced the commands head , tail , pwd , ls , and cd . In this lesson, we'll cover how to create and delete directories (folders), how to move, copy, and delete files, and how to search the contents of a file more systematically.","title":"Introduction to shell part 2"},{"location":"arcadia-users-group/20220912-intro-to-shell2/lesson/#accessing-the-shell-for-this-lesson","text":"For this lesson, we need to have access to a Unix shell. Click the button below to launch a shell through binder . This will launch a computer in the cloud. You'll interact with this computer through your browser. Click the Terminal button to launch a Terminal that we will work with for the rest of the lesson. More information on binder and what happens when you click the launch binder button. Binder is a service that turns a Git repo into a collection of interactive notebooks. When a repository is configured to run as a binder, passing the GitHub repository URL to binder starts the binder-building process. binder first builds a docker image that contains all of the software installations specified by a special set of files in the GitHub repository. A docker image is a set of instructions that are used to create a docker container. A docker container is a runnable instance of a docker image -- it's an encapsulated computing environment that can be used to reproducibly install sets of software on diverse computers. Armed with the docker container, binder launches an \"instance\" in the cloud (either on Google Cloud or AWS typically) on which it runs the docker container. Binder does some additional work in the background -- if no software configuration files are provided in the GitHub repo, or if those contain a minimal set of software, binder will by default include JupyterHub in the docker. When the cloud instance is launched, this is the screen you interact with. You interact with the cloud instance in your browser. Binders are ephemeral instances -- after a period of inactivity, the instance is automatically shut down, and any work you have done will be lost. You're able to download files from your work before the instance is shut down if you do want to save anything. You may notice that this instance already has a bunch of files on it. And that these files look suspiciously exactly like the files in the GitHub repository Arcadia-Science/arcadia-computational-training . That's because that's the repository we used to build the binder from.","title":"Accessing the shell for this lesson"},{"location":"arcadia-users-group/20220912-intro-to-shell2/lesson/#interrogating-the-content-of-files","text":"","title":"Interrogating the content of files"},{"location":"arcadia-users-group/20220912-intro-to-shell2/lesson/#looking-at-whole-files","text":"As we learned in the previous lesson, we can investigate the beginning and end of a file using head or tail . What if you want to see all of the contents of a file? The command cat will print the contents of a file to stdout -- the standard output stream that prints to the shell. Let's run ls and see what files we're working with. ls And then look at the contents of mkdocs.yml using cat : cat mkdocs.yml We see the following content: site_name: Arcadia Science Computational Training site_url: http://arcadia-science.github.io/arcadia-computational-training repo_name: arcadia-computational-training repo_url: https://github.com/arcadia-science/arcadia-computational-training edit_uri: \"\" copyright: 'Copyright &copy; 2022 <a href=\"https://www.arcadiascience.com\">Arcadia Science</a>' # change directory names here to reflect directories in the repository docs_dir: docs site_dir: site theme: name: lux extra_css: - css/extra.css # organize site structure and give a title for each page # paths are relative to the docs directory nav: - \"Home\": \"index.md\" - \"Arcadia Users Group\": \"arcadia-users-group/overview.md\" - \"Workshops\": \"workshops/overview.md\" - \"Contribute\": \"CONTRIBUTING.md\" More information on YAML ( .yml , .yaml ) file formats. YAML originally stood for Yet Another Markup Language as it was originally developed and released around the same time as many other markup languages (HTML, etc). Now, it stands for YAML Ain't A Markup Language. Unlike Markdown which strives to be human-readable and parseable into pretty documents, YAML is a data-oriented. YAML is a human-friendly data serialization language for all programming language. It's a file format commonly used to specify configuration files. Configuration files specify where a computer program can find files it needs, parameters for when the program runs, or other metadata for a program. In this case, the output of the file is palatable; we can grok the whole files contents by printing it all to stdout. Let's use cat on a longer file: cat docs/arcadia-users-group/20220822-intro-to-markdown-syntax/lesson.md When we run this command, the output takes up more than we can see without scrolling. If we were to run cat on a really long file, it may take seconds, minutes, or even hours to print all of the contents to the screen. Enter less . less is a terminal pager that shows a file's contents one screen at a time. less docs/arcadia-users-group/20220822-intro-to-markdown-syntax/lesson.md This allows us to interactivley view the contents of a file. To navigate the lesson screen, we can use key board arrows (line-by-line navigation), the space bar (page jump), or even special combinations of keys ( GG jumps to the bottom of the file). To exit out of less , press the q key.","title":"Looking at whole files"},{"location":"arcadia-users-group/20220912-intro-to-shell2/lesson/#looking-for-specific-content-in-a-file","text":"Sometimes, we care less about all the things in a file and instead want to find something specific. grep is a great command for this. grep ( g lobal r egular e x p ression) is a search tool. It looks through text files for strings (sequences of characters). In its default usage, grep will look for whatever string of characters you give it (1st positional argument), in whichever file you specify (2nd positional argument), and then print out the lines that contain what you searched for. Let's try it: grep \"Arcadia\" mkdocs.yml site_name: Arcadia Science Computational Training copyright: 'Copyright &copy; 2022 <a href=\"https://www.arcadiascience.com\">Arcadia Science</a>' - \"Arcadia Users Group\": \"arcadia-users-group/overview.md\" We see Arcadia appears three times. If we grep for a string that is not in the file, nothing will be printed to the screen: grep \"hippo\" mkdocs.yml","title":"Looking for specific content in a file"},{"location":"arcadia-users-group/20220912-intro-to-shell2/lesson/#figuring-out-how-long-a-file-is","text":"Sometimes we care less about the specific contents of a file and instead we want a general overview of the contents of a file. This can be helpful when you download a large file -- you may know the number of lines you expect to see inside of it. We can use the wc w ord c ount command to get a summary of the number of lines, words, and characters in a file. wc mkdocs.yml This file has 25 lines, 75 words, and 782 characters. 25 75 782 mkdocs.yml wc also accepts flags -- the -l flag limits the output of wc to only the number of lines in a file. wc -l The options below allow you to select which counts are printed. -l , --lines - print the number of lines. -w , --words - print the number of words. -m , --chars - print the number of characters. -c , --bytes - print the number of bytes. -L , --max-line-length - print the length of the longest line.","title":"Figuring out how long a file is"},{"location":"arcadia-users-group/20220912-intro-to-shell2/lesson/#copying-moving-and-renaming-files","text":"The commands cp and mv ( c o p y and m o v e) have the same basic structure. They both require two positional arguments \u2013 the first is the file you want to act on, and the second is where you want it to go (which can include the name you want to give it). To see how this works, let's make a copy of \"example.txt\": ls cp mkdocs.yml mkdocs_copy.yml ls By giving the second argument a name and nothing else (meaning no path in front of the name), we are implicitly saying we want it copied to where we currently are. To make a copy and put it somewhere else, like in our subdirectory docs , we could change the second positional argument using a relative path (\"relative\" because it starts from where we currently are): ls docs cp mkdocs.yml docs/mkdocs_copy.yml ls docs To copy a file to that subdirectory but keep the same name, we could type the whole name out, but we can also provide the directory but leave off the file name: cp mkdocs.yml docs/ ls docs/ If we wanted to copy something from somewhere else to our current working directory and keep the same name, we can use another special character, a period ( . ), which specifies the current working directory: ls cp docs/index.md . ls The mv command is used to move files. Let's move the README.md file into the docs subdirectory: ls ls docs/ mv README.md docs/ ls ls docs/ The mv command is also used to rename files. This may seem strange at first, but remember that the path (address) of a file actually includes its name too (otherwise everything in the same directory would have the same path). ls mv mkdocs_copy.yml mkdocs_new.yml ls To delete files there is the rm command ( r e m ove). This command requires at least one argument specifying the file we want to delete. Typically, this command does not ask you to confirm your actions and files are permanently deleted instead of being moved to a Trash directory. ls rm mkdocs_new.yml ls","title":"Copying, moving, and renaming files"},{"location":"arcadia-users-group/20220912-intro-to-shell2/lesson/#creating-and-removing-directories","text":"We can make a new directory with the command mkdir (for m a k e dir ectory): ls mkdir subset ls And similarly, directories can be deleted with rmdir (for r e m ove dir ectory): rmdir subset/ ls The command line is a little more forgiving when trying to delete a directory. If the directory is not empty, rmdir will give you an error. rmdir docs/ The rm command can also be used to delete a directory, but it needs the flags -rf added to. -r stands for recursive (meaning delete all files and folders located within the specified directory), while the f stands for for force. Unlike rmdir , this command will not check whether you want the directory deleted before removing it and it will remove a directory even if it isn't empty. It's good to get into the habit of mentally or actually checking where you are and what you're deleting before removing something with rm -rf as this command can delete A LOT of files very quickly if you enter the wrong thing. mkdir subset ls rm -rf subset","title":"Creating and removing directories"},{"location":"arcadia-users-group/20220912-intro-to-shell2/lesson/#summary","text":"","title":"Summary"},{"location":"arcadia-users-group/20220912-intro-to-shell2/lesson/#credits","text":"This lesson was modified from the following sources: ANGUS 2019: https://angus.readthedocs.io/en/2019/shell_intro/shell-six-glorious-commands-04.html https://angus.readthedocs.io/en/2019/shell_intro/shell-working-02.html","title":"Credits"},{"location":"arcadia-users-group/20220919-aws-s3-cli/lesson/","text":"How to use the AWS S3 command line tool Accessing the shell for this lesson For this lesson, we need to have access to a Unix shell. If you're not sure how to open a terminal on your computer, see these instructions . What is the AWS S3 CLI and why do we care about it? Arcadia uses Amazon Web Services (AWS) S3 to store our files remotely, sometimes as backup, sometimes to host files temporarily before we deposit them to a FAIR data repository. Some of us may already know how to interact with AWS S3 through its own user interface or using a tool like Cyberduck . While these options work fine, they limit the speed at which you can interact with S3 (they put explicit caps on upload/download speeds) and may be burdensome for large scale changes (think: updating 100s of files). This is where the AWS S3 command line tool comes into play. During this workshop, we'll learn about two tools: the official AWS CLI and s5cmd . Command line tools may be intimidating but great news: if you attended the AUG workshops in the last 2 weeks focusing on shell commands, you already know the shell versions of all the commands ( ls , cp , mv , rm etc.) that we'll go through in this workshop. So let's get started! Downloading and installing the AWS CLI The download and installation instructions for the AWS CLI can be found here . If you're on a Unix machine, you can use these two commands to start the installation process: On Mac: curl \"https://awscli.amazonaws.com/AWSCLIV2.pkg\" -o \"AWSCLIV2.pkg\" sudo installer -pkg AWSCLIV2.pkg -target / rm AWSCLIV2.pkg On Linux: curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" unzip awscliv2.zip sudo ./aws/install rm awscliv2.zip Once the installation process is complete run which aws to make sure installation worked as expected. Configuring the AWS CLI to work with your credentials Getting your AWS credentials to use with the AWS CLI This section assumes you are part of the Arcadia Science AWS account. If you're not, no worries. I'll securely share credentials with you. For AWS users, detailed instructions can be found here . The summary: Login to the AWS console . Click on your username on the top right and select \"Security credentials\" Scroll down to the \"Access keys for CLI, SDK, & API access\" section, select \"Create access key\" and go through the steps. Record your access key and access secret in a secure place (or download it as a CSV temporarily) and do not share with anyone! Now we need to tell the AWS CLI how to use your credentials. With aws configure This section assumes you have not run aws configure before. If you have, you can create a new AWS credential profile by passing the --profile arcadia to the aws configure command. Run aws configure . It'll ask for your AWS Access Key ID , AWS Secret Access Key , Default region name and Default output format one command at a time. Enter the credentials you got in the previous section for the first two settings. Our Default region name is us-west-1 and Default output format is json . It should look like this: aws configure AWS Access Key ID [None]: <YOUR_ACCESS_KEY> AWS Secret Access Key [None]: <YOUR_ACCESS_SECRET> Default region name [None]: us-west-1 Default output format [None]: json Once the setup is complete you can inspect the contents of your AWS configuration file with cat ~/.aws/credentials . You'll most likely only see the [default] profile. These are the credentials the AWS CLI will use by default when you run a command. If you created an arcadia profile and want to use those, you have to explicitly pass them as part of the CLI commands below. Since these credentials only belong to the Arcadia Science organization, when you run a command (think: list all the S3 buckets), it'll only run them within the Arcadia organization context (think: the CLI will only show the S3 buckets of the Arcadia Science organization). You can also create credentials that only have access to parts of the organization (think: a single EC2 instance or a single S3 bucket), and in those cases the CLI context will again change. [Advanced] When working with an EC2 instance This section is out of the scope of this workshop, but it is important to note: When working with an AWS EC2 instance you can enable S3 access by using IAM roles. Instructions for that can be found here . Creating test data Let's create some basic test data by copying the following commands: cd ~/Desktop mkdir s3-test cd s3-test echo \"copy\" >> copy.txt echo \"move\" >> move.txt mkdir sync cd sync echo \"sync 1\" >> sync1.txt echo \"sync 2\" >> sync2.txt echo \"sync 3\" >> sync3.txt cd .. ls S3 CLI command structure The general structure of an AWS S3 CLI command follows the following structure: aws s3 <COMMAND> <FLAGS> <SOURCE_PATH> <TARGET_PATH> <COMMAND> could be cp , mv , sync etc. <FLAGS> could be something like --dryrun to display the operations that would be performed using the specified <COMMAND> without actually running them. <SOURCE_PATH> and <TARGET_PATH> could be a path to a local file/directory, or an S3 file/directory path. For local files, the paths can be absolute or relative. As an example, something like aws s3 cp lesson.md s3://aug-workshop-demo/lesson.md would copy the local lesson.md file from my computer to the S3 bucket aug-workshop-demo and create a file called lesson.md . S3 commands we'll work with For all the following commands, we'll be working with an S3 bucket called aug-workshop-demo hosted through Arcadia's AWS account. For the workshop, we'll focus on a small but useful subset of commands, but you can find the full reference documentation here . ls In the first shell workshop , we learned that ls lists contents of a directory ( l i s t). It serves the same function for the S3 CLI. Let's start with listing all the Arcadia Science S3 buckets: aws s3 ls Now let's explore what one of these buckets looks like, starting with the aug-workshop-demo bucket. aws s3 ls s3://aug-workshop-demo If everything went correctly, you should see the demo folder under the aug-workshop-demo bucket. cp In the second shell workshop , we learned that cp copies a file or directory. The S3 command also serves a similar purpose. Let's try it out by trying to copy the file we created earlier copy.txt to the S3 bucket under a folder with your initials. For me, this would be in s3://aug-workshop-demo/fmc/ . aws s3 cp copy.txt s3://aug-workshop-demo/fmc/copy.txt If you ignore the last part of the S3 path, it'll create a file with the same name: aws s3 cp copy.txt s3://aug-workshop-demo/fmc/ Alternatively, you can give a new name to the file on S3: aws s3 cp copy.txt s3://aug-workshop-demo/fmc/new-copy.txt Let's see the changes: ls The file is still in our directory, it hasn't moved. aws s3 ls s3://aug-workshop-demo aws s3 ls s3://aug-workshop-demo/fmc/ Now let's try copying the sync directory by: aws s3 cp sync/ s3://aug-workshop-demo/fmc/ That failed! This is because the cp command directly works with individual files. For copying folders, we need to use the --recursive flag. So, let's try this: aws s3 cp --recursive sync/ s3://aug-workshop-demo/fmc/ aws s3 ls s3://aug-workshop-demo/fmc/ mv In the second shell workshop , we learned that mv moves or renames a file or directory. The S3 command also serves a similar purpose. Let's start with a moving example by trying to move the move.txt file we created earlier to the S3 bucket under a folder with your name (not your initials this time, since we'll do another move later). For me, this would be in s3://aug-workshop-demo/mert/ . aws s3 mv move.txt s3://aug-workshop-demo/mert/move.txt Let's see the changes: ls This time, the file is no longer in our directory. Let's check AWS S3: aws s3 ls s3://aug-workshop-demo aws s3 ls s3://aug-workshop-demo/mert/ Now, let's do a move within S3 by moving the file from s3://aug-workshop-demo/mert/move.txt to s3://aug-workshop-demo/fmc/move.txt : aws s3 mv s3://aug-workshop-demo/mert/move.txt s3://aug-workshop-demo/fmc/move.txt And now let's visulize it: aws s3 ls s3://aug-workshop-demo/mert/ aws s3 ls s3://aug-workshop-demo/fmc/ The former folder is empty and the latter now has the move.txt . rm In the second shell workshop , we learned that rm deletes a file or directory. The S3 command also serves a similar purpose. Let's delete the copy.txt file: aws s3 rm s3://aug-workshop-demo/fmc/copy.txt aws s3 ls s3://aug-workshop-demo/fmc/ Let's try to delete the contents of s3://aug-workshop-demo/fmc/ : aws s3 rm s3://aug-workshop-demo/fmc/ aws s3 ls s3://aug-workshop-demo/fmc/ You'll see that didn't work. This is because similar to cp , rm works with individual files by default. For folder deletions, you need to use the --recursive flag: aws s3 rm --recursive s3://aug-workshop-demo/fmc/ aws s3 ls s3://aug-workshop-demo/fmc/ sync sync is a convenient command for syncing directories (local to S3, S3 to local, or S3 to S3). It recursively copies new and updated files from the source directory to the destination. Here new and updated are important key words. If a file already exists, it'll not copy it to S3. Let's see it in action: aws s3 sync . s3://aug-workshop-demo/fmc/ aws s3 ls s3://aug-workshop-demo/fmc/ Now, everything is in S3. Let's try it again: aws s3 sync . s3://aug-workshop-demo/fmc/ Nothing happens! This is because all of the files are as they are on S3. Let's make a change to the copy.txt file: echo \"new copy\" >> copy.txt aws s3 sync . s3://aug-workshop-demo/fmc/ aws s3 ls s3://aug-workshop-demo/fmc/ A note on data integrity For upload related commands ( cp , mv , sync ), the AWS CLI will calculate and validate the MD5 checksums. If the checksum doesn't match the expected values, the upload command will fail. In the case of mv , the local files you're moving will not be deleted. By default, the AWS CLI will re-try the upload up to 5 times and only then exit the operation. More details can be found here . Downloading data from S3 Our examples so far have been about uploading data to S3, but we can easily switch the direction of operations to download data from S3. The main change is the argument order. To download data from S3, we need S3 to be our source and local file system to be the target. Let's copy and move some data from S3 as an example: aws s3 cp s3://aug-workshop-demo/fmc/copy.txt new-copy.txt aws s3 mv s3://aug-workshop-demo/fmc/move.txt new-move.txt aws s3 ls s3://aug-workshop-demo/fmc/ --dryrun In using all of these commands, if a command writes or modifies data, it's a good idea to visualize what changes will be made to your local system or the S3 storage system before making them. For this the --dryrun flag is your friend. When used, it displays the operations that would be performed using the specified command without actually executing them. So, if you're in the wrong directory or about to modify or delete a file irreversibly by mistake, it allows you to catch these issues ahead of time. Let's see it in action: aws s3 rm --dryrun --recursive s3://aug-workshop-demo/fmc/ [Optional] Advanced usage with s5cmd s5cmd is an unofficial tool to interact with AWS S3 through the command-line. In this section, we'll talk about the installation instructions and the differences to the official AWS S3 CLI. But first, why do we care about s5cmd ? Why? Great news: if your machine is configured to work with the AWS CLI, it's by default configured to work with s5cmd! It has two benefits over the official S3 CLI: 1. It's much faster. This is due to effective parallelization and bandwidth saturation. See this blog post for benchmarking data. 2. It also is compatible with Google Cloud Storage (GCS) in case you have to work with any databases that are hosted on GCS (think: Alphafold). Downloading and installing the s5cmd CLI On Mac, you can install s5cmd with Homebrew: brew install peak/tap/s5cmd . Alternatively, you can use the command-line on both Mac and Linux: cd ~/Desktop curl -L \"https://github.com/peak/s5cmd/releases/download/v2.0.0/s5cmd_2.0.0_macOS-64bit.tar.gz\" --output s5cmd.tar.gz tar -xvf s5cmd.tar.gz mv s5cmd /usr/local/bin On Linux: curl -L \"https://github.com/peak/s5cmd/releases/download/v2.0.0/s5cmd_2.0.0_macOS-64bit.tar.gz\" --output s5cmd.tar.gz tar -xvf s5cmd.tar.gz mv s5cmd /usr/local/bin After this you should be able to run s5cmd version . For viewing full functionality run s5cmd -h . Differences to the official CLI Order of commands You may remember the order of commands for the AWS CLI looked like this: aws s3 <COMMAND> <FLAGS> <SOURCE_PATH> <TARGET_PATH> For s5cmd it is slightly different: s5cmd <FLAGS> <COMMAND> <SOURCE_PATH> <TARGET_PATH> --dryrun vs --dry-run The S3 CLI uses the former, s5cmd uses the latter.","title":"How to use the AWS S3 command line tool"},{"location":"arcadia-users-group/20220919-aws-s3-cli/lesson/#how-to-use-the-aws-s3-command-line-tool","text":"","title":"How to use the AWS S3 command line tool"},{"location":"arcadia-users-group/20220919-aws-s3-cli/lesson/#accessing-the-shell-for-this-lesson","text":"For this lesson, we need to have access to a Unix shell. If you're not sure how to open a terminal on your computer, see these instructions .","title":"Accessing the shell for this lesson"},{"location":"arcadia-users-group/20220919-aws-s3-cli/lesson/#what-is-the-aws-s3-cli-and-why-do-we-care-about-it","text":"Arcadia uses Amazon Web Services (AWS) S3 to store our files remotely, sometimes as backup, sometimes to host files temporarily before we deposit them to a FAIR data repository. Some of us may already know how to interact with AWS S3 through its own user interface or using a tool like Cyberduck . While these options work fine, they limit the speed at which you can interact with S3 (they put explicit caps on upload/download speeds) and may be burdensome for large scale changes (think: updating 100s of files). This is where the AWS S3 command line tool comes into play. During this workshop, we'll learn about two tools: the official AWS CLI and s5cmd . Command line tools may be intimidating but great news: if you attended the AUG workshops in the last 2 weeks focusing on shell commands, you already know the shell versions of all the commands ( ls , cp , mv , rm etc.) that we'll go through in this workshop. So let's get started!","title":"What is the AWS S3 CLI and why do we care about it?"},{"location":"arcadia-users-group/20220919-aws-s3-cli/lesson/#downloading-and-installing-the-aws-cli","text":"The download and installation instructions for the AWS CLI can be found here . If you're on a Unix machine, you can use these two commands to start the installation process: On Mac: curl \"https://awscli.amazonaws.com/AWSCLIV2.pkg\" -o \"AWSCLIV2.pkg\" sudo installer -pkg AWSCLIV2.pkg -target / rm AWSCLIV2.pkg On Linux: curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" unzip awscliv2.zip sudo ./aws/install rm awscliv2.zip Once the installation process is complete run which aws to make sure installation worked as expected.","title":"Downloading and installing the AWS CLI"},{"location":"arcadia-users-group/20220919-aws-s3-cli/lesson/#configuring-the-aws-cli-to-work-with-your-credentials","text":"","title":"Configuring the AWS CLI to work with your credentials"},{"location":"arcadia-users-group/20220919-aws-s3-cli/lesson/#getting-your-aws-credentials-to-use-with-the-aws-cli","text":"This section assumes you are part of the Arcadia Science AWS account. If you're not, no worries. I'll securely share credentials with you. For AWS users, detailed instructions can be found here . The summary: Login to the AWS console . Click on your username on the top right and select \"Security credentials\" Scroll down to the \"Access keys for CLI, SDK, & API access\" section, select \"Create access key\" and go through the steps. Record your access key and access secret in a secure place (or download it as a CSV temporarily) and do not share with anyone! Now we need to tell the AWS CLI how to use your credentials.","title":"Getting your AWS credentials to use with the AWS CLI"},{"location":"arcadia-users-group/20220919-aws-s3-cli/lesson/#with-aws-configure","text":"This section assumes you have not run aws configure before. If you have, you can create a new AWS credential profile by passing the --profile arcadia to the aws configure command. Run aws configure . It'll ask for your AWS Access Key ID , AWS Secret Access Key , Default region name and Default output format one command at a time. Enter the credentials you got in the previous section for the first two settings. Our Default region name is us-west-1 and Default output format is json . It should look like this: aws configure AWS Access Key ID [None]: <YOUR_ACCESS_KEY> AWS Secret Access Key [None]: <YOUR_ACCESS_SECRET> Default region name [None]: us-west-1 Default output format [None]: json Once the setup is complete you can inspect the contents of your AWS configuration file with cat ~/.aws/credentials . You'll most likely only see the [default] profile. These are the credentials the AWS CLI will use by default when you run a command. If you created an arcadia profile and want to use those, you have to explicitly pass them as part of the CLI commands below. Since these credentials only belong to the Arcadia Science organization, when you run a command (think: list all the S3 buckets), it'll only run them within the Arcadia organization context (think: the CLI will only show the S3 buckets of the Arcadia Science organization). You can also create credentials that only have access to parts of the organization (think: a single EC2 instance or a single S3 bucket), and in those cases the CLI context will again change.","title":"With aws configure"},{"location":"arcadia-users-group/20220919-aws-s3-cli/lesson/#advanced-when-working-with-an-ec2-instance","text":"This section is out of the scope of this workshop, but it is important to note: When working with an AWS EC2 instance you can enable S3 access by using IAM roles. Instructions for that can be found here .","title":"[Advanced] When working with an EC2 instance"},{"location":"arcadia-users-group/20220919-aws-s3-cli/lesson/#creating-test-data","text":"Let's create some basic test data by copying the following commands: cd ~/Desktop mkdir s3-test cd s3-test echo \"copy\" >> copy.txt echo \"move\" >> move.txt mkdir sync cd sync echo \"sync 1\" >> sync1.txt echo \"sync 2\" >> sync2.txt echo \"sync 3\" >> sync3.txt cd .. ls","title":"Creating test data"},{"location":"arcadia-users-group/20220919-aws-s3-cli/lesson/#s3-cli-command-structure","text":"The general structure of an AWS S3 CLI command follows the following structure: aws s3 <COMMAND> <FLAGS> <SOURCE_PATH> <TARGET_PATH> <COMMAND> could be cp , mv , sync etc. <FLAGS> could be something like --dryrun to display the operations that would be performed using the specified <COMMAND> without actually running them. <SOURCE_PATH> and <TARGET_PATH> could be a path to a local file/directory, or an S3 file/directory path. For local files, the paths can be absolute or relative. As an example, something like aws s3 cp lesson.md s3://aug-workshop-demo/lesson.md would copy the local lesson.md file from my computer to the S3 bucket aug-workshop-demo and create a file called lesson.md .","title":"S3 CLI command structure"},{"location":"arcadia-users-group/20220919-aws-s3-cli/lesson/#s3-commands-well-work-with","text":"For all the following commands, we'll be working with an S3 bucket called aug-workshop-demo hosted through Arcadia's AWS account. For the workshop, we'll focus on a small but useful subset of commands, but you can find the full reference documentation here .","title":"S3 commands we'll work with"},{"location":"arcadia-users-group/20220919-aws-s3-cli/lesson/#ls","text":"In the first shell workshop , we learned that ls lists contents of a directory ( l i s t). It serves the same function for the S3 CLI. Let's start with listing all the Arcadia Science S3 buckets: aws s3 ls Now let's explore what one of these buckets looks like, starting with the aug-workshop-demo bucket. aws s3 ls s3://aug-workshop-demo If everything went correctly, you should see the demo folder under the aug-workshop-demo bucket.","title":"ls"},{"location":"arcadia-users-group/20220919-aws-s3-cli/lesson/#cp","text":"In the second shell workshop , we learned that cp copies a file or directory. The S3 command also serves a similar purpose. Let's try it out by trying to copy the file we created earlier copy.txt to the S3 bucket under a folder with your initials. For me, this would be in s3://aug-workshop-demo/fmc/ . aws s3 cp copy.txt s3://aug-workshop-demo/fmc/copy.txt If you ignore the last part of the S3 path, it'll create a file with the same name: aws s3 cp copy.txt s3://aug-workshop-demo/fmc/ Alternatively, you can give a new name to the file on S3: aws s3 cp copy.txt s3://aug-workshop-demo/fmc/new-copy.txt Let's see the changes: ls The file is still in our directory, it hasn't moved. aws s3 ls s3://aug-workshop-demo aws s3 ls s3://aug-workshop-demo/fmc/ Now let's try copying the sync directory by: aws s3 cp sync/ s3://aug-workshop-demo/fmc/ That failed! This is because the cp command directly works with individual files. For copying folders, we need to use the --recursive flag. So, let's try this: aws s3 cp --recursive sync/ s3://aug-workshop-demo/fmc/ aws s3 ls s3://aug-workshop-demo/fmc/","title":"cp"},{"location":"arcadia-users-group/20220919-aws-s3-cli/lesson/#mv","text":"In the second shell workshop , we learned that mv moves or renames a file or directory. The S3 command also serves a similar purpose. Let's start with a moving example by trying to move the move.txt file we created earlier to the S3 bucket under a folder with your name (not your initials this time, since we'll do another move later). For me, this would be in s3://aug-workshop-demo/mert/ . aws s3 mv move.txt s3://aug-workshop-demo/mert/move.txt Let's see the changes: ls This time, the file is no longer in our directory. Let's check AWS S3: aws s3 ls s3://aug-workshop-demo aws s3 ls s3://aug-workshop-demo/mert/ Now, let's do a move within S3 by moving the file from s3://aug-workshop-demo/mert/move.txt to s3://aug-workshop-demo/fmc/move.txt : aws s3 mv s3://aug-workshop-demo/mert/move.txt s3://aug-workshop-demo/fmc/move.txt And now let's visulize it: aws s3 ls s3://aug-workshop-demo/mert/ aws s3 ls s3://aug-workshop-demo/fmc/ The former folder is empty and the latter now has the move.txt .","title":"mv"},{"location":"arcadia-users-group/20220919-aws-s3-cli/lesson/#rm","text":"In the second shell workshop , we learned that rm deletes a file or directory. The S3 command also serves a similar purpose. Let's delete the copy.txt file: aws s3 rm s3://aug-workshop-demo/fmc/copy.txt aws s3 ls s3://aug-workshop-demo/fmc/ Let's try to delete the contents of s3://aug-workshop-demo/fmc/ : aws s3 rm s3://aug-workshop-demo/fmc/ aws s3 ls s3://aug-workshop-demo/fmc/ You'll see that didn't work. This is because similar to cp , rm works with individual files by default. For folder deletions, you need to use the --recursive flag: aws s3 rm --recursive s3://aug-workshop-demo/fmc/ aws s3 ls s3://aug-workshop-demo/fmc/","title":"rm"},{"location":"arcadia-users-group/20220919-aws-s3-cli/lesson/#sync","text":"sync is a convenient command for syncing directories (local to S3, S3 to local, or S3 to S3). It recursively copies new and updated files from the source directory to the destination. Here new and updated are important key words. If a file already exists, it'll not copy it to S3. Let's see it in action: aws s3 sync . s3://aug-workshop-demo/fmc/ aws s3 ls s3://aug-workshop-demo/fmc/ Now, everything is in S3. Let's try it again: aws s3 sync . s3://aug-workshop-demo/fmc/ Nothing happens! This is because all of the files are as they are on S3. Let's make a change to the copy.txt file: echo \"new copy\" >> copy.txt aws s3 sync . s3://aug-workshop-demo/fmc/ aws s3 ls s3://aug-workshop-demo/fmc/","title":"sync"},{"location":"arcadia-users-group/20220919-aws-s3-cli/lesson/#a-note-on-data-integrity","text":"For upload related commands ( cp , mv , sync ), the AWS CLI will calculate and validate the MD5 checksums. If the checksum doesn't match the expected values, the upload command will fail. In the case of mv , the local files you're moving will not be deleted. By default, the AWS CLI will re-try the upload up to 5 times and only then exit the operation. More details can be found here .","title":"A note on data integrity"},{"location":"arcadia-users-group/20220919-aws-s3-cli/lesson/#downloading-data-from-s3","text":"Our examples so far have been about uploading data to S3, but we can easily switch the direction of operations to download data from S3. The main change is the argument order. To download data from S3, we need S3 to be our source and local file system to be the target. Let's copy and move some data from S3 as an example: aws s3 cp s3://aug-workshop-demo/fmc/copy.txt new-copy.txt aws s3 mv s3://aug-workshop-demo/fmc/move.txt new-move.txt aws s3 ls s3://aug-workshop-demo/fmc/","title":"Downloading data from S3"},{"location":"arcadia-users-group/20220919-aws-s3-cli/lesson/#-dryrun","text":"In using all of these commands, if a command writes or modifies data, it's a good idea to visualize what changes will be made to your local system or the S3 storage system before making them. For this the --dryrun flag is your friend. When used, it displays the operations that would be performed using the specified command without actually executing them. So, if you're in the wrong directory or about to modify or delete a file irreversibly by mistake, it allows you to catch these issues ahead of time. Let's see it in action: aws s3 rm --dryrun --recursive s3://aug-workshop-demo/fmc/","title":"--dryrun"},{"location":"arcadia-users-group/20220919-aws-s3-cli/lesson/#optional-advanced-usage-with-s5cmd","text":"s5cmd is an unofficial tool to interact with AWS S3 through the command-line. In this section, we'll talk about the installation instructions and the differences to the official AWS S3 CLI. But first, why do we care about s5cmd ?","title":"[Optional] Advanced usage with s5cmd"},{"location":"arcadia-users-group/20220919-aws-s3-cli/lesson/#why","text":"Great news: if your machine is configured to work with the AWS CLI, it's by default configured to work with s5cmd! It has two benefits over the official S3 CLI: 1. It's much faster. This is due to effective parallelization and bandwidth saturation. See this blog post for benchmarking data. 2. It also is compatible with Google Cloud Storage (GCS) in case you have to work with any databases that are hosted on GCS (think: Alphafold).","title":"Why?"},{"location":"arcadia-users-group/20220919-aws-s3-cli/lesson/#downloading-and-installing-the-s5cmd-cli","text":"On Mac, you can install s5cmd with Homebrew: brew install peak/tap/s5cmd . Alternatively, you can use the command-line on both Mac and Linux: cd ~/Desktop curl -L \"https://github.com/peak/s5cmd/releases/download/v2.0.0/s5cmd_2.0.0_macOS-64bit.tar.gz\" --output s5cmd.tar.gz tar -xvf s5cmd.tar.gz mv s5cmd /usr/local/bin On Linux: curl -L \"https://github.com/peak/s5cmd/releases/download/v2.0.0/s5cmd_2.0.0_macOS-64bit.tar.gz\" --output s5cmd.tar.gz tar -xvf s5cmd.tar.gz mv s5cmd /usr/local/bin After this you should be able to run s5cmd version . For viewing full functionality run s5cmd -h .","title":"Downloading and installing the s5cmd CLI"},{"location":"arcadia-users-group/20220919-aws-s3-cli/lesson/#differences-to-the-official-cli","text":"","title":"Differences to the official CLI"},{"location":"arcadia-users-group/20220919-aws-s3-cli/lesson/#order-of-commands","text":"You may remember the order of commands for the AWS CLI looked like this: aws s3 <COMMAND> <FLAGS> <SOURCE_PATH> <TARGET_PATH> For s5cmd it is slightly different: s5cmd <FLAGS> <COMMAND> <SOURCE_PATH> <TARGET_PATH>","title":"Order of commands"},{"location":"arcadia-users-group/20220919-aws-s3-cli/lesson/#-dryrun-vs-dry-run","text":"The S3 CLI uses the former, s5cmd uses the latter.","title":"--dryrun vs --dry-run"},{"location":"arcadia-users-group/20221011-developer-productivity/lesson/","text":"Increasing developer productivity with IDEs and terminal customizations In the last couple of weeks, we learned many shell and git commands. This week, we'll take a step back to learn about using an integrated development environment (IDE) to develop our scripts/pipelines and customizing our terminals with shortcuts and git visualizations. Accessing the shell for this lesson For this lesson, we need to have access to a Unix shell. If you're not sure how to open a terminal on your computer, see these instructions . Caveats You don't really have to do any of these customizations to get good at programming. But these are designed to make your life easier. A good chunk of these customizations are personal. The goal is to learn how to customize and to empower you to customize as you wish. A good chunk of these customizations are stylistic. As a programmar, you spend a lot of time in your terminal or text editor. I think it's important to make these tools beautiful. I like bash because I've been using it for 10 years. This workshop will focus on using bash in more detail, and give basic examples of zsh customizations. But there are parallels between the two and learning to customize one will help you customize the other. IDEs can sometimes get RAM-intensive. So, beware if you have a laptop with <16GB of RAM. Any customizations to your terminal should be fine. Most of the instructions today are for Mac devices. This is because 95% of Arcadia Science scientists use Macs for development work. Demo repository For demo purposes, we'll use the contents of the AUG repository . So, let's download the contents of it. Our clone code assumes you have git configured to use an ssh key like we set up in the workshop. If you don't, head over to this lesson for instructions on how to set it up. cd ~/Desktop # or cd wherever you keep git repositories on your computer git clone git@github.com:Arcadia-Science/arcadia-computational-training.git cd arcadia-computational-training ls IDEs What are IDEs? People think of programming as mostly editing the source code of a web application or a script or a computational pipeline. But there are many other aspects of programming that happen all the time that we take for granted: debugging, testing, checking the code for errors, typos etc. Integrated development environments or IDEs bring all these aspects of programming together and offer convenient features like autocomplete, syntax highlighting, linting, code execution etc. IDEs are not necessary for programming, but with basic configuration, they can significantly increase coding productivity. Which IDE should I use? There are many IDEs or IDE-like text editors out there. Some are language-specific (think RStudio for R). Some are generic like Sublime Text . Up until very recently, I've exclusively used Sublime Text and still love it for light-weight programming. Today we'll cover Visual Studio Code (aka VS Code). There are couple reasons for this: It has a tight-integration with git and Github. It has a rich marketplace of free extensions, enabling you to configure your IDE to your heart's content. It works cross-platform and has a straightforward way of syncing your settings across devices. It is quite easy to configure because it suggests improvements on its own as needed. It has access to a built-in terminal. Try hitting CTRL + ` when you install it. It has tooling built-in to enable remote SSH access. So you can use an EC2 instance and navigate it through these tools. Installing VS Code Download and install VS Code by going to this URL . Once installed, turn on sync across devices and login with your GitHub account. This will only sync your settings and extensions across devices and will not sync any files you work with. Customizing VS Code You can use VS Code as is. This section suggests a couple base-level customizations to make your life a little easier. Settings Note: There are hundreds of settings you can customize, we'll only focus on the essentials. Open up the settings panel by hitting Command + , or by selecting \"Code\" -> \"Preferences\" -> \"Settings\" from the top left corner. There are two ways to adjust settings: You can search through them using the search bar You can edit the underlying JSON blob that specifies the user settings. Today, we'll mostly do #1, but if you're interested in following the second method, a simplified version of all my settings can be found in this file . So, let's get started. Auto Save: Search for \"Auto Save\" in the settings menu. And adjust it to the non-default value (which is none). Mine is \"afterDelay\". This will make sure your changes are saved automatically 1000ms after you stop typing. Font Size: Search for \"Font Size\" and adjust it as you wish. I like 14pts font. Trim Trailing Whitespace: : Search for it and turn it on, so your code doesn't have trailing spaces or tabs. Insert Final Newline: Search for it and turn it on. This will prevent you from receiving PR review comments on this . Trim Final Newlines: Again, turn it on. This will make sure there's only a single newline at the end of each file. Format on Save: Search for \"Format on Save\" and turn it on. This will make sure, whatever code formatter you use formats the code upon saving it. Default Terminal application: You only need to change this if you use iTerm2. If you're on Mac, search for \"Osx Exec\" and type \"iTerm.app\". For Linux users, you'd have to search for \"Linux Exec\". Default Terminal profile: If you're on Mac, search for \"Default Profile: Osx\" and choose \"bash\" or \"zsh\" depending on your preferences. Adding code in the PATH: Open the command palette with Command + Shift + P. Search for \"code\" and choose the \"Install 'code' command in PATH\". This will make it so that you can open any file/directory from the terminal with code . Extensions Before we go through this section, let's quit VS Code. Renavigate to the correct demo folder and re-open with VS Code: cd ~/Desktop/arcadia-computational-training code . For this section, we'll only go through the installation instructions of a few VS Code extensions and give you a list of other extensions you can install on your own time. For downloading and installing extensions, click on the \"Extensions\" tab from the left tab, search for the name of the extension and hit \"install\". Here are the extensions: Prettier: Customization with \"Format Document With\" Markdown All in One: Opening Markdown previews with Command + Shift + V Excel Viewer: Opening CSV and Excel previews with Command + Shift + V GitHub Pull Requests and Issues Git Graph Extension recommendations Python Pylance - for basic linting R Jupyter - for Jupyter notebooks. These tools will allow to specify your Python version/kernel you use. Jupyter Cell Tags Jupyter Keymap Jupyter Notebook Renderers Jupyter Slide Show One Dark Pro - for color schemes Nextflow specific extensions recommended by the nf-core team Customizing the terminal There are 3 main ways, we'll customize our terminals. (Optional) Switching from zsh to bash if you haven't already. Installing iTerm2 , a better version of Apple's built-in Terminal application. Customizing bash with .bash_profile or customizing zsh with .zshrc . (Optional) Switching from zsh to bash Note: I like bash because I've been using it for 10 years. You don't have to. If you want to learn more about the differences, you can read more here . The main differences are: zsh is more configurable and support more customizable plug-ins (like auto-complete!). bash is the default shell you'll get access on Linux devices (like AWS instances). If you want to keep using zsh, that's perfectly fine! You can check this blog post and this tool for ways to configure it. And we'll go through an example customization below. If you want to start using bash, open the Terminal application, run chsh -s /bin/bash and quit the application. Next time you open, you should see bash. You'll see an alert that zsh the new Apple default when you open the terminal Installing and using iTerm2 Download and install iTerm2 by going to this URL . There aren't too many ways to customize iTerm2. Or at least, I only customize the color scheme. My color scheme can be found here . If you want to download something unique for you, there are many options here . Once you select the color scheme you'd like to use, just do the following: Launch iTerm2 Type CMD+i Navigate to Colors tab Click on Load Presets Click on Import Select your file Click on Load Presets and choose a color scheme Tweaking zsh There are two files as part of this repository that should give you a sense of how to configure zsh. First one is the .zshrc file and the other is the iTerm2 zsh configuration file . As you'll see most of the configurations rely on the ohmyzsh repository. So let's start with installing that: On Unix systems, you can run sh -c \"$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\" . Note that any previous .zshrc will be renamed to .zshrc.pre-oh-my-zsh . You can make sure the installation worked as expected with: cat ~/.zshrc This file should look awfully similar to this one . You can copy the iTerm2 zsh configuration file in this repository to the root directory ( ~ ). And copy the contents of the sample .zshrc file to the newly created ~/.zshrc file. And you should be all set! Tweaking bash In general, most customizations go to the .bash_profile or .bashrc files. Here, we'll show a slightly modular approach, so we can simplify the files before they get too long and difficult to manage. Once you change the .bash_profile or the underlying configuration files, you need to make sure those changes are persistant. You can do that by \"sourcing\" the .bash_profile with source .bash_profile or restarting your terminal. Tweaking the bash prompt For this and the following sections, we need to create the following files: .bash_profile (main entry point the Terminal application searches for), .bash_prompt (customizes the look of the prompt), .aliases (adds new shortcuts/aliases for common commands) and .gitconfig (customizations for git commands). Let's start with creating these: cd ~ touch .bash_profile .bash_prompt .aliases .gitconfig code .bash_profile .bash_prompt .aliases .gitconfig These commands will not recreate the files if they already exist. If they already exist, we'll edit them. My versions of all these files can be found in the config-and-dotfiles directory. You can just copy and paste them, making sure to not delete anything in your existing files. We'll go through them one by one, starting with the .bash_prompt . This file basically shapes the main prompt that shows up when you use bash. For it to be activated, make sure you also copy the .bash_profile file. And run source ~/.bash_profile . Using aliases for commonly used commands Aliases are shortcuts for commands that you frequently use. Some of these provide stylistic changes. Some of these provide useful shortcuts for common git commands. You can customize this however you want, but a simplified version of my aliases can be found here . Tweaking git Git uses the .gitconfig file to configure the behavior of the git CLI. My simplified version of the configuration file can be found here . The two big components of this configuration file are: Colorful view for git branches, stages etc. Better visualization of the git history through the command-line What will happen in the future? We got a glimpse of basic forms of linting with Pylance and basic forms of formatting with Prettier. We'll be doing a more in-depth workshop on Python and R specific linters (and ways of automating the linting process) so you can easily catch errors and have consistent styling for your code Updating bash to its latest version. This can get a little bit complicated and is error-prone because you have to work with Homebrew. Instructions are here , but we won't cover it in-depth for brevity. Adding auto-complete to git on bash. Originally, this was going to be included in this workshop, but was cut out for breviy purposes. This is actually quite straightforward to do, but is better done with the latest version of bash. If we have a decent reception to this workshop, we may host another one on broader productivity tools for Mac and Chrome.","title":"Increasing developer productivity with IDEs and terminal customizations"},{"location":"arcadia-users-group/20221011-developer-productivity/lesson/#increasing-developer-productivity-with-ides-and-terminal-customizations","text":"In the last couple of weeks, we learned many shell and git commands. This week, we'll take a step back to learn about using an integrated development environment (IDE) to develop our scripts/pipelines and customizing our terminals with shortcuts and git visualizations.","title":"Increasing developer productivity with IDEs and terminal customizations"},{"location":"arcadia-users-group/20221011-developer-productivity/lesson/#accessing-the-shell-for-this-lesson","text":"For this lesson, we need to have access to a Unix shell. If you're not sure how to open a terminal on your computer, see these instructions .","title":"Accessing the shell for this lesson"},{"location":"arcadia-users-group/20221011-developer-productivity/lesson/#caveats","text":"You don't really have to do any of these customizations to get good at programming. But these are designed to make your life easier. A good chunk of these customizations are personal. The goal is to learn how to customize and to empower you to customize as you wish. A good chunk of these customizations are stylistic. As a programmar, you spend a lot of time in your terminal or text editor. I think it's important to make these tools beautiful. I like bash because I've been using it for 10 years. This workshop will focus on using bash in more detail, and give basic examples of zsh customizations. But there are parallels between the two and learning to customize one will help you customize the other. IDEs can sometimes get RAM-intensive. So, beware if you have a laptop with <16GB of RAM. Any customizations to your terminal should be fine. Most of the instructions today are for Mac devices. This is because 95% of Arcadia Science scientists use Macs for development work.","title":"Caveats"},{"location":"arcadia-users-group/20221011-developer-productivity/lesson/#demo-repository","text":"For demo purposes, we'll use the contents of the AUG repository . So, let's download the contents of it. Our clone code assumes you have git configured to use an ssh key like we set up in the workshop. If you don't, head over to this lesson for instructions on how to set it up. cd ~/Desktop # or cd wherever you keep git repositories on your computer git clone git@github.com:Arcadia-Science/arcadia-computational-training.git cd arcadia-computational-training ls","title":"Demo repository"},{"location":"arcadia-users-group/20221011-developer-productivity/lesson/#ides","text":"","title":"IDEs"},{"location":"arcadia-users-group/20221011-developer-productivity/lesson/#what-are-ides","text":"People think of programming as mostly editing the source code of a web application or a script or a computational pipeline. But there are many other aspects of programming that happen all the time that we take for granted: debugging, testing, checking the code for errors, typos etc. Integrated development environments or IDEs bring all these aspects of programming together and offer convenient features like autocomplete, syntax highlighting, linting, code execution etc. IDEs are not necessary for programming, but with basic configuration, they can significantly increase coding productivity.","title":"What are IDEs?"},{"location":"arcadia-users-group/20221011-developer-productivity/lesson/#which-ide-should-i-use","text":"There are many IDEs or IDE-like text editors out there. Some are language-specific (think RStudio for R). Some are generic like Sublime Text . Up until very recently, I've exclusively used Sublime Text and still love it for light-weight programming. Today we'll cover Visual Studio Code (aka VS Code). There are couple reasons for this: It has a tight-integration with git and Github. It has a rich marketplace of free extensions, enabling you to configure your IDE to your heart's content. It works cross-platform and has a straightforward way of syncing your settings across devices. It is quite easy to configure because it suggests improvements on its own as needed. It has access to a built-in terminal. Try hitting CTRL + ` when you install it. It has tooling built-in to enable remote SSH access. So you can use an EC2 instance and navigate it through these tools.","title":"Which IDE should I use?"},{"location":"arcadia-users-group/20221011-developer-productivity/lesson/#installing-vs-code","text":"Download and install VS Code by going to this URL . Once installed, turn on sync across devices and login with your GitHub account. This will only sync your settings and extensions across devices and will not sync any files you work with.","title":"Installing VS Code"},{"location":"arcadia-users-group/20221011-developer-productivity/lesson/#customizing-vs-code","text":"You can use VS Code as is. This section suggests a couple base-level customizations to make your life a little easier.","title":"Customizing VS Code"},{"location":"arcadia-users-group/20221011-developer-productivity/lesson/#settings","text":"Note: There are hundreds of settings you can customize, we'll only focus on the essentials. Open up the settings panel by hitting Command + , or by selecting \"Code\" -> \"Preferences\" -> \"Settings\" from the top left corner. There are two ways to adjust settings: You can search through them using the search bar You can edit the underlying JSON blob that specifies the user settings. Today, we'll mostly do #1, but if you're interested in following the second method, a simplified version of all my settings can be found in this file . So, let's get started. Auto Save: Search for \"Auto Save\" in the settings menu. And adjust it to the non-default value (which is none). Mine is \"afterDelay\". This will make sure your changes are saved automatically 1000ms after you stop typing. Font Size: Search for \"Font Size\" and adjust it as you wish. I like 14pts font. Trim Trailing Whitespace: : Search for it and turn it on, so your code doesn't have trailing spaces or tabs. Insert Final Newline: Search for it and turn it on. This will prevent you from receiving PR review comments on this . Trim Final Newlines: Again, turn it on. This will make sure there's only a single newline at the end of each file. Format on Save: Search for \"Format on Save\" and turn it on. This will make sure, whatever code formatter you use formats the code upon saving it. Default Terminal application: You only need to change this if you use iTerm2. If you're on Mac, search for \"Osx Exec\" and type \"iTerm.app\". For Linux users, you'd have to search for \"Linux Exec\". Default Terminal profile: If you're on Mac, search for \"Default Profile: Osx\" and choose \"bash\" or \"zsh\" depending on your preferences. Adding code in the PATH: Open the command palette with Command + Shift + P. Search for \"code\" and choose the \"Install 'code' command in PATH\". This will make it so that you can open any file/directory from the terminal with code .","title":"Settings"},{"location":"arcadia-users-group/20221011-developer-productivity/lesson/#extensions","text":"Before we go through this section, let's quit VS Code. Renavigate to the correct demo folder and re-open with VS Code: cd ~/Desktop/arcadia-computational-training code . For this section, we'll only go through the installation instructions of a few VS Code extensions and give you a list of other extensions you can install on your own time. For downloading and installing extensions, click on the \"Extensions\" tab from the left tab, search for the name of the extension and hit \"install\". Here are the extensions: Prettier: Customization with \"Format Document With\" Markdown All in One: Opening Markdown previews with Command + Shift + V Excel Viewer: Opening CSV and Excel previews with Command + Shift + V GitHub Pull Requests and Issues Git Graph","title":"Extensions"},{"location":"arcadia-users-group/20221011-developer-productivity/lesson/#extension-recommendations","text":"Python Pylance - for basic linting R Jupyter - for Jupyter notebooks. These tools will allow to specify your Python version/kernel you use. Jupyter Cell Tags Jupyter Keymap Jupyter Notebook Renderers Jupyter Slide Show One Dark Pro - for color schemes Nextflow specific extensions recommended by the nf-core team","title":"Extension recommendations"},{"location":"arcadia-users-group/20221011-developer-productivity/lesson/#customizing-the-terminal","text":"There are 3 main ways, we'll customize our terminals. (Optional) Switching from zsh to bash if you haven't already. Installing iTerm2 , a better version of Apple's built-in Terminal application. Customizing bash with .bash_profile or customizing zsh with .zshrc .","title":"Customizing the terminal"},{"location":"arcadia-users-group/20221011-developer-productivity/lesson/#optional-switching-from-zsh-to-bash","text":"Note: I like bash because I've been using it for 10 years. You don't have to. If you want to learn more about the differences, you can read more here . The main differences are: zsh is more configurable and support more customizable plug-ins (like auto-complete!). bash is the default shell you'll get access on Linux devices (like AWS instances). If you want to keep using zsh, that's perfectly fine! You can check this blog post and this tool for ways to configure it. And we'll go through an example customization below. If you want to start using bash, open the Terminal application, run chsh -s /bin/bash and quit the application. Next time you open, you should see bash. You'll see an alert that zsh the new Apple default when you open the terminal","title":"(Optional) Switching from zsh to bash"},{"location":"arcadia-users-group/20221011-developer-productivity/lesson/#installing-and-using-iterm2","text":"Download and install iTerm2 by going to this URL . There aren't too many ways to customize iTerm2. Or at least, I only customize the color scheme. My color scheme can be found here . If you want to download something unique for you, there are many options here . Once you select the color scheme you'd like to use, just do the following: Launch iTerm2 Type CMD+i Navigate to Colors tab Click on Load Presets Click on Import Select your file Click on Load Presets and choose a color scheme","title":"Installing and using iTerm2"},{"location":"arcadia-users-group/20221011-developer-productivity/lesson/#tweaking-zsh","text":"There are two files as part of this repository that should give you a sense of how to configure zsh. First one is the .zshrc file and the other is the iTerm2 zsh configuration file . As you'll see most of the configurations rely on the ohmyzsh repository. So let's start with installing that: On Unix systems, you can run sh -c \"$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\" . Note that any previous .zshrc will be renamed to .zshrc.pre-oh-my-zsh . You can make sure the installation worked as expected with: cat ~/.zshrc This file should look awfully similar to this one . You can copy the iTerm2 zsh configuration file in this repository to the root directory ( ~ ). And copy the contents of the sample .zshrc file to the newly created ~/.zshrc file. And you should be all set!","title":"Tweaking zsh"},{"location":"arcadia-users-group/20221011-developer-productivity/lesson/#tweaking-bash","text":"In general, most customizations go to the .bash_profile or .bashrc files. Here, we'll show a slightly modular approach, so we can simplify the files before they get too long and difficult to manage. Once you change the .bash_profile or the underlying configuration files, you need to make sure those changes are persistant. You can do that by \"sourcing\" the .bash_profile with source .bash_profile or restarting your terminal.","title":"Tweaking bash"},{"location":"arcadia-users-group/20221011-developer-productivity/lesson/#tweaking-the-bash-prompt","text":"For this and the following sections, we need to create the following files: .bash_profile (main entry point the Terminal application searches for), .bash_prompt (customizes the look of the prompt), .aliases (adds new shortcuts/aliases for common commands) and .gitconfig (customizations for git commands). Let's start with creating these: cd ~ touch .bash_profile .bash_prompt .aliases .gitconfig code .bash_profile .bash_prompt .aliases .gitconfig These commands will not recreate the files if they already exist. If they already exist, we'll edit them. My versions of all these files can be found in the config-and-dotfiles directory. You can just copy and paste them, making sure to not delete anything in your existing files. We'll go through them one by one, starting with the .bash_prompt . This file basically shapes the main prompt that shows up when you use bash. For it to be activated, make sure you also copy the .bash_profile file. And run source ~/.bash_profile .","title":"Tweaking the bash prompt"},{"location":"arcadia-users-group/20221011-developer-productivity/lesson/#using-aliases-for-commonly-used-commands","text":"Aliases are shortcuts for commands that you frequently use. Some of these provide stylistic changes. Some of these provide useful shortcuts for common git commands. You can customize this however you want, but a simplified version of my aliases can be found here .","title":"Using aliases for commonly used commands"},{"location":"arcadia-users-group/20221011-developer-productivity/lesson/#tweaking-git","text":"Git uses the .gitconfig file to configure the behavior of the git CLI. My simplified version of the configuration file can be found here . The two big components of this configuration file are: Colorful view for git branches, stages etc. Better visualization of the git history through the command-line","title":"Tweaking git"},{"location":"arcadia-users-group/20221011-developer-productivity/lesson/#what-will-happen-in-the-future","text":"We got a glimpse of basic forms of linting with Pylance and basic forms of formatting with Prettier. We'll be doing a more in-depth workshop on Python and R specific linters (and ways of automating the linting process) so you can easily catch errors and have consistent styling for your code Updating bash to its latest version. This can get a little bit complicated and is error-prone because you have to work with Homebrew. Instructions are here , but we won't cover it in-depth for brevity. Adding auto-complete to git on bash. Originally, this was going to be included in this workshop, but was cut out for breviy purposes. This is actually quite straightforward to do, but is better done with the latest version of bash. If we have a decent reception to this workshop, we may host another one on broader productivity tools for Mac and Chrome.","title":"What will happen in the future?"},{"location":"arcadia-users-group/20221017-conda/lesson/","text":"Introduction to conda for software installation and environment management Why should I use a package and environment management system? Package managers and why we use them. A package manager is a software tool that automates the process of installing, upgrading, configuring, or removing software from your computer. Pip (python), BiocManager (Bioconductor R packages), and APT (ubuntu) are three commonly encountered package managers. Package managers make installation software easier. Environments and why we use them. Environment managers address the problems created when software is installed system-wide. System-wide installs create complex dependencies between disparate projects that are difficult to entangle to version compute environments and that can create dependcency conflicts. An environment is a directory that contains a specific collection of packages/tools that you have installed. An environment manager is a software tool that organizes where and how software is installed on a computer. For example, you may have one environment with Python 2.7 and its dependencies, and another environment with Python 3.4 for legacy testing. If you change one environment, your other environments are not affected. You can easily activate or deactivate environments, which is how you switch between them. What is Conda? Conda is open source package and runs on Windows, Mac OS and Linux. Conda can quickly install, run, and update packages and their dependencies. Conda can create, save, load, and switch between project specific software environments on your local computer. Conda as a package manager helps you find and install packages. If you need a package that requires a different version of Python, you do not need to switch to a different environment manager, because Conda is also an environment manager. With a few commands, you can set up a totally separate environment to run that different version of Python, while continuing to run a different version of Python in another environment. How does Conda work? Cartoon of conda environments by Gergely Szerovay www.freecodecamp.org Installing Conda Cartoon of decision points for conda installation by Gergely Szerovay www.freecodecamp.org For this lesson, we'll install miniconda. We've included the latest installation instructions below. You can access the latest and legacy installation links here . Installing conda on an Apple machine Apple now has two processor types, the Intel x64 and the Apple M1 (or M2) ARM64. As of October 2022, many of the packages available via conda-forge and other conda installation channels are only available on the Intel x64 processor. Therefore, even if you have an M1 (or M2) Mac, we currently recommend that you use the Intel x64 installation and take advantage of Apple's translation program Rosetta. This requires a little bit of pre-configuration to make sure your Terminal application also runs with Rosetta. To set this up, open Finder -> navigate to your Applications folder -> right click on your Terminal application (either Terminal or iTerm2) -> select Get Info -> check the box for Open using Rosetta . Running Terminal/iTerm using Rosetta After that is configured, open your Terminal application and copy and paste the following commands to download and install miniconda. curl -JLO https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh bash Miniconda3-latest-MacOSX-x86_64.sh This will prompt you to accept the miniconda license, ask you whether you accept the default installation location, and whether you want the installation script to intialize conda. Accept all of the defaults and then open and close your terminal for the installation to take effect. Installing conda on a Linux machine Each time you start a new AWS (or many other cloud) machine, you'll have to install miniconda. The installation process is the same as it is on Mac except you'll need to use a different download URL. curl -JLO https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh This will prompt you to accept the miniconda license, ask you whether you accept the default installation location, and whether you want the installation script to intialize conda. Accept all of the defaults and then open and close your terminal for the installation to take effect. To activate conda, you can either log out and log back in or run: source ~/.bashrc Additional notes about conda installations Versioning conda installations. The conda API has changed over the years. You can access legacy installers associated with specific versions of python here . If you need a specific version of conda, you can access the conda package through the conda-forge channel and install it using the instructions for installing specific versions of a package detailed later in the lesson. Conda intialization. By default, the Conda installer \"initializes\" conda. This allows you to access Conda commands by putting conda in your path any time you log into a terminal. Conda acheives this by appending code to you ~/.zshrc or ~/.bashrc file. These lines of code look like this: # >>> conda initialize >>> # !! Contents within this block are managed by 'conda init' !! __conda_setup=\"$('/Users/taylorreiter/miniconda3/bin/conda' 'shell.zsh' 'hook' 2> /dev/null)\" if [ $? -eq 0 ]; then eval \"$__conda_setup\" else if [ -f \"/Users/taylorreiter/miniconda3/etc/profile.d/conda.sh\" ]; then . \"/Users/taylorreiter/miniconda3/etc/profile.d/conda.sh\" else export PATH=\"/Users/taylorreiter/miniconda3/bin:$PATH\" fi fi unset __conda_setup # <<< conda initialize <<< There is a command, conda init , that you can use to initialize conda if you choose to skip this step during installation. You can always remove these lines if you change your mind about wanting conda in your PATH. Activating conda and the (base) environment Conda needs to be activated for it to be usable. During the installation process, we configured conda to automatically activate when the shell is started. If conda is activated, you should see (base) prepend your path. If at any point you don't want the base environment to be active when you log in to a shell, you can disable this behavior using the command: conda config --set auto_activate_base false The base environment contains all of the software needed for conda to run. This includes a python installation and some core system libraries and dependencies of Conda. Installing mamba While you can install new packages with conda, sometimes it's...slow. Mamba helps solve this problem. Mamba is a drop in replacement for conda and it improves installation speeds by decreasing the time it takes to solve dependency conflicts. You can install mamba with: conda install -c conda-forge mamba Conda channels Channels are the locations of the repositories (directories) online containing Conda packages. Upon Conda\u2019s installation, Continuum\u2019s (Conda\u2019s developer) channels are set by default, so without any further modification, these are the locations where your Conda will start searching for packages. Channels in Conda are ordered. The channel with the highest priority is the first one that Conda checks, looking for the package you asked for. You can change this order, and also add channels to it (and set their priority as well). You should follow the channel order below -- otherwise, sometimes you'll encounter weird bugs because packages will be installed from the wrong location. conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge Once we have our channel order configured, you will no longer need to use the explicit channel parameter -c when you install new packages. Cartoon of conda channels by Gergely Szerovay www.freecodecamp.org If multiple channels contain a package, and one channel contains a newer version than the other one, the order of the channels determines which of these two versions is going to be installed, even if the higher priority channel contains the older version. Cartoon of conda channel priorities by Gergely Szerovay www.freecodecamp.org Creating a new environment Other than mamba, we recommend you keep you base environment free from all other installations. If you install everything in base , you'll end up with lots of dependencies that need to be solved which will decrease installation speeds. It also becomes more difficult to document software installations and versions required for specific project. To create a new environment named, for instance mynewenv (you can name it what ever you like), run: conda create --name mynewenv Navigating environments through activate and deactivate If you want to use an environment (for instance manage packages, or run Python scripts inside it) you need to first activate it. conda activate mynewenv If this command gives you an error, you might not selected for conda to initialize during installation. You can initialize conda with: conda init Once you've successfully run conda activate , the command prompt should be prepended by (mynewenv) instead of (base) . The command prompt is a useful tool to orient yourself as to which environment you're currently using. If you install a new tool while in an environment, it only gets installed in that environment. mamba install samtools Even better, you can specify which version of samtools you would like to install: mamba install samtools=1.9 We just installed a very old version. We can ask mamba to automatically update our package installation to the latest version: mamba update samtools The directories of the active environment\u2019s executable files are added to the system path (this means that you can now access them more easily). which samtools You can leave an environment with this command: conda deactivate Now, if we run the same command, we'll get a different result: which samtools Searching, installing, and removing packages To list out all the installed packages in the currently active environment, run: conda list To search for all the available versions of a certain package, you can use the search command. For instance, to list out all the versions of samtools, run: conda search samtools Similarly to the conda list command, this one results in a list of the matching package names, versions, and channels: Loading channels: done # Name Version Build Channel samtools 0.1.12 0 bioconda samtools 0.1.12 1 bioconda samtools 0.1.12 2 bioconda samtools 0.1.12 hdd8ed8b_3 bioconda Freezing an environment Keeping track of the software versions you used to run your analyses is a cornerstone of reproducible science. Conda facilitates this in many ways. One handy way is to record the packages you've installed into a text file. This will save the list of conda-installed software you have in a particular environment to the file packages.txt : conda activate mynewenv conda env export > mynewenv.yml conda deactivate (it will not record the software versions for software not installed by conda.) conda env create -n mynewenv2 --file mynewenv.yml will install those packages in your local environment. Encoding environments in and installing environments from text files Freezing an environment with conda env export is a perfectly viable option to record your compute state for a given environment. This approach can be brittle to operating system because so there are other more flexible ways to achieve similar things. Instead, you can keep track of software and versions you install in a YAML file. Open a text file using nano, vim, VS Code, or your favorite text editor and paste the following text. channels: - conda-forge - bioconda - defaults dependencies: - samtools=1.9 Save the file as samtools.yml . Then, you can install: mamba env create -n samtools -f samtools.yml Using small environments The performance of conda environments decreases with increasing size. We hinted at this above when we recommended to keep the base environment free of software installations other than mamba . The larger a conda environment becomes, the longer it takes to activate the environment and to install new software. You'll have to experiment with a strategy that works for you as far as environment size goes, but we recommend a strategy that keeps environments small. One starting place might be to have one environment per project. If you want to test new installations of software before you're committed to using it, you could create a sandbox environment and install software for experimenting there. Then, when that environment gets too big, you can delete it and start over. To delete an environment, run: conda env remove -n sandbox Keeping an eye on the size of conda environments Conda environments can become quite large depending on how many packages are installed. We can check how large any of our Conda enviroments are by finding the path to the environment directory and then estimating the file space usage of that directory. First, let's find where we put out mynewenv directory conda env list This will print out a list of the locations of our Conda environments. We installed conda in our home directory, so we can use the ~ shortcut to access our environment paths. Next, let's use the command du to estimate the space our mynewenv directory is taking up! du -sh ~/miniconda3/envs/mynewenv/ We can see our mynewenv environment is taking up about 12K of space. Summary of Conda Commands Conda commands action conda install install a package conda search search for a package conda info list of information about the environment conda list list out all the installed packages in the currently active environment conda remove remove a conda package conda config --get channels list out the active channels and their priorities conda update update all the installed packages conda env list list the different environments you have set up and their locations conda activate mynewenv activate the mynewenv Conda environment (this also works for activating our base environment A note about the conda ecosystem Conda has truly been a gift to the scientific community, taking software installs that used to take days and bringing it down to minutes. It also simplified versioning and environment management by integrating software encoded in many different languages into its ecosystem (python, R, perl, rust...). It has therefore attracted contributions from many parties over the years that have lead to a diversity of modular pieces that participate in the conda ecosystem. This can be especially confusing for conda newcomers. This section outlines many of the key pieces in the conda ecosytem to reduce confusion as people onboard to conda. Conda itself is a piece of software that is a package and environment manager. To get access to the conda software, you need to install it. The most popular way to install conda is via miniconda or miniforge. miniforge is the community (conda-forge) driven minimalistic conda installer. By default, subsequent package installations come from conda-forge channel. miniconda is the Anaconda (company) driven minimalistic conda installer. By default, subsequent package installations come from anaconda channels (default or otherwise). The default channel order can be changed for either conda installation type. miniforge started when miniconda didn't support the linux aarch64 system architectures and was quickly adopted by many conda user. It's stuck around even though both miniconda and miniforge support most system architectures. There's another distinction between miniconda and anaconda . Anaconda comes with all of the contents of miniconda, plus a bunch of other packages that are used frequently with scientific computing. Anaconda also comes with a graphical user interface. We use miniconda because its the minimum set of tools that we need to install packages and manage environments and we can use it locally and on remote computers. One of the problems conda addresses is resolving dependency conflicts betweens many pieces of software installed in the same environment. This is an NP-complete problem meaning it gets slower as more software is added to an environment and it's a hard problem to solve. mamba is a drop-in replacement for conda that offers higher speed and more reliable environment solutions. However, the best way to install mamba at the moment is via conda. Mamba is worth the confusion it has caused -- it decreases install times by orders of magnitude thus saving time. Extra content Conda and workflow managers Conda is integrated into workflow managers like snakemake and nextflow. Using environment files or other directives that point toward the conda packages that are needed for a specific rule or process, the workflow manager will use conda to build the necessary environments and will automatically activate and deactivate them for each rule or process as it runs the workflow. In this case, it's best practice to have only the tools needed for a specific rule or process in a given environment -- often, this will mean only including one tool per an environment. For snakemake, conda environments are encoded in YAML files and fed to a given rule using the conda: directive and the path to the YAML file. For nextflow, conda environments are actually the least-recommended way to execute a pipeline. While you can give a process a conda package name and its build hash, it's better to use the biocontainer build for that conda package on docker. This overcomes issues with reproducibility that stem from differences in operating systems . Building a conda package from an R library on CRAN Once you become reliant on conda to manage all of your packages, it can be disappointing when your favorite new pacakge isn't available on the conda-forge channel. If the R package is available on CRAN, it's relatively straightforward to build the conda package yourself and get it onto conda-forge. You can follow instructions in this blog post if you ever want to do this. This only works for CRAN packages that do not depend on Bioconductor packages -- Bioconductor packages and the packages that depend on them are released on bioconda, while CRAN packages are released on conda-forge. Installing packages with pip when they're in PyPi but not in a conda channel Sometimes the package you're interested in installation is available in PyPi but not in a conda channel. You can use pip to install things into a conda environment to overcome this. To do this, you need to install pip via conda, and then you can use pip install . Whatever packages you pip install will only be available in the conda environment you have active at time of installation. To document pip installations in a YAML file, use the following syntax: channels: - conda-forge - bioconda - defaults dependencies: - pip - pip: - genome-grist==0.7 Installing PyPi-compliant packages from GitHub into a conda channel using pip Similar to installing packages in PyPi via pip, you can install PyPi-compliant packages that are on GitHub directly from GitHub. We include a YAML file demonstrating this below. While this is handy, especially for development, it's can be difficult to version software installations so you should use this approach with caution and be sure to document versions (or commit hashes) elsewhere. channels: - conda-forge - bioconda - defaults dependencies: - sourmash - pip - pip: - git+https://github.com/czbiohub/orpheum@master Conda-lock Conda-lock is a lightweight library generates lock files for conda environments. These lock files document the results of conda solve for each platform that you designate. This can help to exactly reproduce an environment across operating systems and to reduce environment creation times.","title":"Introduction to conda for software installation and environment management"},{"location":"arcadia-users-group/20221017-conda/lesson/#introduction-to-conda-for-software-installation-and-environment-management","text":"","title":"Introduction to conda for software installation and environment management"},{"location":"arcadia-users-group/20221017-conda/lesson/#why-should-i-use-a-package-and-environment-management-system","text":"Package managers and why we use them. A package manager is a software tool that automates the process of installing, upgrading, configuring, or removing software from your computer. Pip (python), BiocManager (Bioconductor R packages), and APT (ubuntu) are three commonly encountered package managers. Package managers make installation software easier. Environments and why we use them. Environment managers address the problems created when software is installed system-wide. System-wide installs create complex dependencies between disparate projects that are difficult to entangle to version compute environments and that can create dependcency conflicts. An environment is a directory that contains a specific collection of packages/tools that you have installed. An environment manager is a software tool that organizes where and how software is installed on a computer. For example, you may have one environment with Python 2.7 and its dependencies, and another environment with Python 3.4 for legacy testing. If you change one environment, your other environments are not affected. You can easily activate or deactivate environments, which is how you switch between them.","title":"Why should I use a package and environment management system?"},{"location":"arcadia-users-group/20221017-conda/lesson/#what-is-conda","text":"Conda is open source package and runs on Windows, Mac OS and Linux. Conda can quickly install, run, and update packages and their dependencies. Conda can create, save, load, and switch between project specific software environments on your local computer. Conda as a package manager helps you find and install packages. If you need a package that requires a different version of Python, you do not need to switch to a different environment manager, because Conda is also an environment manager. With a few commands, you can set up a totally separate environment to run that different version of Python, while continuing to run a different version of Python in another environment.","title":"What is Conda?"},{"location":"arcadia-users-group/20221017-conda/lesson/#how-does-conda-work","text":"Cartoon of conda environments by Gergely Szerovay www.freecodecamp.org","title":"How does Conda work?"},{"location":"arcadia-users-group/20221017-conda/lesson/#installing-conda","text":"Cartoon of decision points for conda installation by Gergely Szerovay www.freecodecamp.org For this lesson, we'll install miniconda. We've included the latest installation instructions below. You can access the latest and legacy installation links here .","title":"Installing Conda"},{"location":"arcadia-users-group/20221017-conda/lesson/#installing-conda-on-an-apple-machine","text":"Apple now has two processor types, the Intel x64 and the Apple M1 (or M2) ARM64. As of October 2022, many of the packages available via conda-forge and other conda installation channels are only available on the Intel x64 processor. Therefore, even if you have an M1 (or M2) Mac, we currently recommend that you use the Intel x64 installation and take advantage of Apple's translation program Rosetta. This requires a little bit of pre-configuration to make sure your Terminal application also runs with Rosetta. To set this up, open Finder -> navigate to your Applications folder -> right click on your Terminal application (either Terminal or iTerm2) -> select Get Info -> check the box for Open using Rosetta . Running Terminal/iTerm using Rosetta After that is configured, open your Terminal application and copy and paste the following commands to download and install miniconda. curl -JLO https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh bash Miniconda3-latest-MacOSX-x86_64.sh This will prompt you to accept the miniconda license, ask you whether you accept the default installation location, and whether you want the installation script to intialize conda. Accept all of the defaults and then open and close your terminal for the installation to take effect.","title":"Installing conda on an Apple machine"},{"location":"arcadia-users-group/20221017-conda/lesson/#installing-conda-on-a-linux-machine","text":"Each time you start a new AWS (or many other cloud) machine, you'll have to install miniconda. The installation process is the same as it is on Mac except you'll need to use a different download URL. curl -JLO https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh bash Miniconda3-latest-Linux-x86_64.sh This will prompt you to accept the miniconda license, ask you whether you accept the default installation location, and whether you want the installation script to intialize conda. Accept all of the defaults and then open and close your terminal for the installation to take effect. To activate conda, you can either log out and log back in or run: source ~/.bashrc","title":"Installing conda on a Linux machine"},{"location":"arcadia-users-group/20221017-conda/lesson/#additional-notes-about-conda-installations","text":"Versioning conda installations. The conda API has changed over the years. You can access legacy installers associated with specific versions of python here . If you need a specific version of conda, you can access the conda package through the conda-forge channel and install it using the instructions for installing specific versions of a package detailed later in the lesson. Conda intialization. By default, the Conda installer \"initializes\" conda. This allows you to access Conda commands by putting conda in your path any time you log into a terminal. Conda acheives this by appending code to you ~/.zshrc or ~/.bashrc file. These lines of code look like this: # >>> conda initialize >>> # !! Contents within this block are managed by 'conda init' !! __conda_setup=\"$('/Users/taylorreiter/miniconda3/bin/conda' 'shell.zsh' 'hook' 2> /dev/null)\" if [ $? -eq 0 ]; then eval \"$__conda_setup\" else if [ -f \"/Users/taylorreiter/miniconda3/etc/profile.d/conda.sh\" ]; then . \"/Users/taylorreiter/miniconda3/etc/profile.d/conda.sh\" else export PATH=\"/Users/taylorreiter/miniconda3/bin:$PATH\" fi fi unset __conda_setup # <<< conda initialize <<< There is a command, conda init , that you can use to initialize conda if you choose to skip this step during installation. You can always remove these lines if you change your mind about wanting conda in your PATH.","title":"Additional notes about conda installations"},{"location":"arcadia-users-group/20221017-conda/lesson/#activating-conda-and-the-base-environment","text":"Conda needs to be activated for it to be usable. During the installation process, we configured conda to automatically activate when the shell is started. If conda is activated, you should see (base) prepend your path. If at any point you don't want the base environment to be active when you log in to a shell, you can disable this behavior using the command: conda config --set auto_activate_base false The base environment contains all of the software needed for conda to run. This includes a python installation and some core system libraries and dependencies of Conda.","title":"Activating conda and the (base) environment"},{"location":"arcadia-users-group/20221017-conda/lesson/#installing-mamba","text":"While you can install new packages with conda, sometimes it's...slow. Mamba helps solve this problem. Mamba is a drop in replacement for conda and it improves installation speeds by decreasing the time it takes to solve dependency conflicts. You can install mamba with: conda install -c conda-forge mamba","title":"Installing mamba"},{"location":"arcadia-users-group/20221017-conda/lesson/#conda-channels","text":"Channels are the locations of the repositories (directories) online containing Conda packages. Upon Conda\u2019s installation, Continuum\u2019s (Conda\u2019s developer) channels are set by default, so without any further modification, these are the locations where your Conda will start searching for packages. Channels in Conda are ordered. The channel with the highest priority is the first one that Conda checks, looking for the package you asked for. You can change this order, and also add channels to it (and set their priority as well). You should follow the channel order below -- otherwise, sometimes you'll encounter weird bugs because packages will be installed from the wrong location. conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge Once we have our channel order configured, you will no longer need to use the explicit channel parameter -c when you install new packages. Cartoon of conda channels by Gergely Szerovay www.freecodecamp.org If multiple channels contain a package, and one channel contains a newer version than the other one, the order of the channels determines which of these two versions is going to be installed, even if the higher priority channel contains the older version. Cartoon of conda channel priorities by Gergely Szerovay www.freecodecamp.org","title":"Conda channels"},{"location":"arcadia-users-group/20221017-conda/lesson/#creating-a-new-environment","text":"Other than mamba, we recommend you keep you base environment free from all other installations. If you install everything in base , you'll end up with lots of dependencies that need to be solved which will decrease installation speeds. It also becomes more difficult to document software installations and versions required for specific project. To create a new environment named, for instance mynewenv (you can name it what ever you like), run: conda create --name mynewenv","title":"Creating a new environment"},{"location":"arcadia-users-group/20221017-conda/lesson/#navigating-environments-through-activate-and-deactivate","text":"If you want to use an environment (for instance manage packages, or run Python scripts inside it) you need to first activate it. conda activate mynewenv If this command gives you an error, you might not selected for conda to initialize during installation. You can initialize conda with: conda init Once you've successfully run conda activate , the command prompt should be prepended by (mynewenv) instead of (base) . The command prompt is a useful tool to orient yourself as to which environment you're currently using. If you install a new tool while in an environment, it only gets installed in that environment. mamba install samtools Even better, you can specify which version of samtools you would like to install: mamba install samtools=1.9 We just installed a very old version. We can ask mamba to automatically update our package installation to the latest version: mamba update samtools The directories of the active environment\u2019s executable files are added to the system path (this means that you can now access them more easily). which samtools You can leave an environment with this command: conda deactivate Now, if we run the same command, we'll get a different result: which samtools","title":"Navigating environments through activate and deactivate"},{"location":"arcadia-users-group/20221017-conda/lesson/#searching-installing-and-removing-packages","text":"To list out all the installed packages in the currently active environment, run: conda list To search for all the available versions of a certain package, you can use the search command. For instance, to list out all the versions of samtools, run: conda search samtools Similarly to the conda list command, this one results in a list of the matching package names, versions, and channels: Loading channels: done # Name Version Build Channel samtools 0.1.12 0 bioconda samtools 0.1.12 1 bioconda samtools 0.1.12 2 bioconda samtools 0.1.12 hdd8ed8b_3 bioconda","title":"Searching, installing, and removing packages"},{"location":"arcadia-users-group/20221017-conda/lesson/#freezing-an-environment","text":"Keeping track of the software versions you used to run your analyses is a cornerstone of reproducible science. Conda facilitates this in many ways. One handy way is to record the packages you've installed into a text file. This will save the list of conda-installed software you have in a particular environment to the file packages.txt : conda activate mynewenv conda env export > mynewenv.yml conda deactivate (it will not record the software versions for software not installed by conda.) conda env create -n mynewenv2 --file mynewenv.yml will install those packages in your local environment.","title":"Freezing an environment"},{"location":"arcadia-users-group/20221017-conda/lesson/#encoding-environments-in-and-installing-environments-from-text-files","text":"Freezing an environment with conda env export is a perfectly viable option to record your compute state for a given environment. This approach can be brittle to operating system because so there are other more flexible ways to achieve similar things. Instead, you can keep track of software and versions you install in a YAML file. Open a text file using nano, vim, VS Code, or your favorite text editor and paste the following text. channels: - conda-forge - bioconda - defaults dependencies: - samtools=1.9 Save the file as samtools.yml . Then, you can install: mamba env create -n samtools -f samtools.yml","title":"Encoding environments in and installing environments from text files"},{"location":"arcadia-users-group/20221017-conda/lesson/#using-small-environments","text":"The performance of conda environments decreases with increasing size. We hinted at this above when we recommended to keep the base environment free of software installations other than mamba . The larger a conda environment becomes, the longer it takes to activate the environment and to install new software. You'll have to experiment with a strategy that works for you as far as environment size goes, but we recommend a strategy that keeps environments small. One starting place might be to have one environment per project. If you want to test new installations of software before you're committed to using it, you could create a sandbox environment and install software for experimenting there. Then, when that environment gets too big, you can delete it and start over. To delete an environment, run: conda env remove -n sandbox","title":"Using small environments"},{"location":"arcadia-users-group/20221017-conda/lesson/#keeping-an-eye-on-the-size-of-conda-environments","text":"Conda environments can become quite large depending on how many packages are installed. We can check how large any of our Conda enviroments are by finding the path to the environment directory and then estimating the file space usage of that directory. First, let's find where we put out mynewenv directory conda env list This will print out a list of the locations of our Conda environments. We installed conda in our home directory, so we can use the ~ shortcut to access our environment paths. Next, let's use the command du to estimate the space our mynewenv directory is taking up! du -sh ~/miniconda3/envs/mynewenv/ We can see our mynewenv environment is taking up about 12K of space.","title":"Keeping an eye on the size of conda environments"},{"location":"arcadia-users-group/20221017-conda/lesson/#summary-of-conda-commands","text":"Conda commands action conda install install a package conda search search for a package conda info list of information about the environment conda list list out all the installed packages in the currently active environment conda remove remove a conda package conda config --get channels list out the active channels and their priorities conda update update all the installed packages conda env list list the different environments you have set up and their locations conda activate mynewenv activate the mynewenv Conda environment (this also works for activating our base environment","title":"Summary of Conda Commands"},{"location":"arcadia-users-group/20221017-conda/lesson/#a-note-about-the-conda-ecosystem","text":"Conda has truly been a gift to the scientific community, taking software installs that used to take days and bringing it down to minutes. It also simplified versioning and environment management by integrating software encoded in many different languages into its ecosystem (python, R, perl, rust...). It has therefore attracted contributions from many parties over the years that have lead to a diversity of modular pieces that participate in the conda ecosystem. This can be especially confusing for conda newcomers. This section outlines many of the key pieces in the conda ecosytem to reduce confusion as people onboard to conda. Conda itself is a piece of software that is a package and environment manager. To get access to the conda software, you need to install it. The most popular way to install conda is via miniconda or miniforge. miniforge is the community (conda-forge) driven minimalistic conda installer. By default, subsequent package installations come from conda-forge channel. miniconda is the Anaconda (company) driven minimalistic conda installer. By default, subsequent package installations come from anaconda channels (default or otherwise). The default channel order can be changed for either conda installation type. miniforge started when miniconda didn't support the linux aarch64 system architectures and was quickly adopted by many conda user. It's stuck around even though both miniconda and miniforge support most system architectures. There's another distinction between miniconda and anaconda . Anaconda comes with all of the contents of miniconda, plus a bunch of other packages that are used frequently with scientific computing. Anaconda also comes with a graphical user interface. We use miniconda because its the minimum set of tools that we need to install packages and manage environments and we can use it locally and on remote computers. One of the problems conda addresses is resolving dependency conflicts betweens many pieces of software installed in the same environment. This is an NP-complete problem meaning it gets slower as more software is added to an environment and it's a hard problem to solve. mamba is a drop-in replacement for conda that offers higher speed and more reliable environment solutions. However, the best way to install mamba at the moment is via conda. Mamba is worth the confusion it has caused -- it decreases install times by orders of magnitude thus saving time.","title":"A note about the conda ecosystem"},{"location":"arcadia-users-group/20221017-conda/lesson/#extra-content","text":"","title":"Extra content"},{"location":"arcadia-users-group/20221017-conda/lesson/#conda-and-workflow-managers","text":"Conda is integrated into workflow managers like snakemake and nextflow. Using environment files or other directives that point toward the conda packages that are needed for a specific rule or process, the workflow manager will use conda to build the necessary environments and will automatically activate and deactivate them for each rule or process as it runs the workflow. In this case, it's best practice to have only the tools needed for a specific rule or process in a given environment -- often, this will mean only including one tool per an environment. For snakemake, conda environments are encoded in YAML files and fed to a given rule using the conda: directive and the path to the YAML file. For nextflow, conda environments are actually the least-recommended way to execute a pipeline. While you can give a process a conda package name and its build hash, it's better to use the biocontainer build for that conda package on docker. This overcomes issues with reproducibility that stem from differences in operating systems .","title":"Conda and workflow managers"},{"location":"arcadia-users-group/20221017-conda/lesson/#building-a-conda-package-from-an-r-library-on-cran","text":"Once you become reliant on conda to manage all of your packages, it can be disappointing when your favorite new pacakge isn't available on the conda-forge channel. If the R package is available on CRAN, it's relatively straightforward to build the conda package yourself and get it onto conda-forge. You can follow instructions in this blog post if you ever want to do this. This only works for CRAN packages that do not depend on Bioconductor packages -- Bioconductor packages and the packages that depend on them are released on bioconda, while CRAN packages are released on conda-forge.","title":"Building a conda package from an R library on CRAN"},{"location":"arcadia-users-group/20221017-conda/lesson/#installing-packages-with-pip-when-theyre-in-pypi-but-not-in-a-conda-channel","text":"Sometimes the package you're interested in installation is available in PyPi but not in a conda channel. You can use pip to install things into a conda environment to overcome this. To do this, you need to install pip via conda, and then you can use pip install . Whatever packages you pip install will only be available in the conda environment you have active at time of installation. To document pip installations in a YAML file, use the following syntax: channels: - conda-forge - bioconda - defaults dependencies: - pip - pip: - genome-grist==0.7","title":"Installing packages with pip when they're in PyPi but not in a conda channel"},{"location":"arcadia-users-group/20221017-conda/lesson/#installing-pypi-compliant-packages-from-github-into-a-conda-channel-using-pip","text":"Similar to installing packages in PyPi via pip, you can install PyPi-compliant packages that are on GitHub directly from GitHub. We include a YAML file demonstrating this below. While this is handy, especially for development, it's can be difficult to version software installations so you should use this approach with caution and be sure to document versions (or commit hashes) elsewhere. channels: - conda-forge - bioconda - defaults dependencies: - sourmash - pip - pip: - git+https://github.com/czbiohub/orpheum@master","title":"Installing PyPi-compliant packages from GitHub into a conda channel using pip"},{"location":"arcadia-users-group/20221017-conda/lesson/#conda-lock","text":"Conda-lock is a lightweight library generates lock files for conda environments. These lock files document the results of conda solve for each platform that you designate. This can help to exactly reproduce an environment across operating systems and to reduce environment creation times.","title":"Conda-lock"},{"location":"arcadia-users-group/20221024-jupyter-notebooks/lesson/","text":"Introduction to Jupyter notebooks Jumping in with Jupyter notebooks We'll start today's tutorial using a Binder installation of jupyter notebook in the cloud. After we take a tour of jupyter notebooks, we'll learn how to install and interact with them on our local computers. Click the button below to launch a jupyter notebook through binder . This will launch a computer in the cloud. You'll interact with this computer through your browser. Click the Python 3 button under the Notebook section to launch a jupyter notebook. More information on binder and what happens when you click the launch binder button. Binder is a service that turns a Git repo into a collection of interactive notebooks. When a repository is configured to run as a binder, passing the GitHub repository URL to binder starts the binder-building process. Binder first builds a docker image that contains all of the software installations specified by a special set of files in the GitHub repository. A docker image is a set of instructions that are used to create a docker container. A docker container is a runnable instance of a docker image -- it's an encapsulated computing environment that can be used to reproducibly install sets of software on diverse computers. Armed with the docker container, binder launches an \"instance\" in the cloud (either on Google Cloud or AWS typically) on which it runs the docker container. Binder does some additional work in the background -- if no software configuration files are provided in the GitHub repo, or if those contain a minimal set of software, binder will by default include JupyterHub in the docker. When the cloud instance is launched, this is the screen you interact with. You interact with the cloud instance in your browser. Binders are ephemeral instances -- after a period of inactivity, the instance is automatically shut down, and any work you have done will be lost. You're able to download files from your work before the instance is shut down if you do want to save anything. You may notice that this instance already has a bunch of files on it. And that these files look suspiciously exactly like the files in the GitHub repository Arcadia-Science/arcadia-computational-training . That's because that's the repository we used to build the binder from. Jupyter notebook vs. JupyterLab Technically, our entry point for this tutorial is JupyterLab. JupyterLab is the next-generation user interface that includes notebooks. For an in depth comparison between Jupyter notebooks and JupyterLab, see this article . The image below depicts the JupyterLab (left) and Jupyter notebook (right) start up screens. JupyterLab provides a richer set of entry points for computation. Once you start a notebook, the notebook interface is identical. How the Jupyter notebook works Click in the first cell and type some Python code. This is a Code cell (see the cell type dropdown with the word Code ). To run the cell, type Shift + Return . Let's look at a Markdown cell. Markdown is a text manipulation language that is readable yet offers additional formatting. Don't forget to select Markdown from the cell type dropdown. Click in the cell and enter the markdown text. To run the cell, type Shift + Return . This workflow has several advantages: You can easily type, edit, and copy and paste blocks of code. Tab completion allows you to easily access the names of things you are using and learn more about them. It allows you to annotate your code with links, different sized text, bullets, etc. to make information more accessible to you and your collaborators. It allows you to display figures next to the code that produces them to tell a complete story of the analysis. How the notebook is stored The notebook file is stored in a format called JSON and has the suffix .ipynb . Just like HTML for a webpage, what's saved in a notebook file looks different from what you see in your browser. This format allows Jupyter to mix software (in several languages) with documentation and graphics, all in one file. Viewing and interacting with jupyter notebooks While jupyter notebooks are saved in a plain text file in JSON format, the files themselves are quite large (often exceeding a megabyte). This text-rich format isn't very rewarding to look at it in its raw format. Historically, one needed to use jupyter notebook to open, view (render), and execute a jupyter notebook. This could be a heavy lift when you wanted to share a notebook with collaborators. This is probably still the most popular way to interact with notebooks but there are now lighter weight alternatives. Viewing and interacting with jupyter notebooks. To view a jupyter notebook, you can use any of the following strategies. Use a local installation of jupyter notebook or jupyter hub to open the notebook in your browser. You'll have full functionality of the notebook with this strategy. Use VS Code to open and render the notebook. You'll still need jupyter installed locally for this to work, and you'll have full functionality of the notebook but it will be rendered directly in VS Code. Only viewing jupyter notebooks. If your goal is view the notebook, or allow others to view the notebook, you can use the following strategies. Export the notebook to html, markdown, or PDF. You can then share the file with others who would like to view it. Upload the notebook to a GitHub repository. GitHub will automatically render the notebook, allowing others to view it without having to download it and use a local installation of jupyter notebook to open it. You can use nbviewer or other third party applications that render and distribute jupyter notebooks. Notebook modes: Control and Edit The notebook has two modes of operation: Control and Edit. Control mode lets you edit notebook level features; while, Edit mode lets you change the contents of a notebook cell. Remember a notebook is made up of a number of cells which can contain code, markdown, html, visualizations, and more. Executing shell commands in a jupyter notebook To execute a line of bash code in a python jupyter notebook, prepend the line of code with an ! : !ls Running jupyter notebooks locally: installation and startup Until this point, we've been using a JupyterLab installation on a cloud computer. This allowed us to learn how to use Jupyter notebooks without having to install the program ourselves. In this section, we'll learn to install jupyter locally using conda. If you don't have conda install, head over to the miniconda lesson to get it set up on your machine. Managing jupyter notebook installations with conda Jupyter brings along a lot of dependencies, so we'll create a new environment just for jupyter. In the long run, you'll also need to install any dependencies you need into this environment. For practice, we'll install pandas and seaborn to demonstrate how to do this. mamba create -n jupyter jupyter pandas After we've created our environment, we need to activate it to be able to access the software we just installed. conda activate jupyter The command to start a jupyter notebook is jupyter notebook . jupyter notebook Starting a jupyter notebook and an explanation of what jupyter is doing After typing the command jupyter notebook , the following happens: A Jupyter Notebook server is automatically created on your local machine. The Jupyter Notebook server runs locally on your machine only and does not use an internet connection. The Jupyter Notebook server opens the Jupyter notebook client, also known as the notebook user interface, in your default web browser. To create a new Python notebook select the \"New\" dropdown on the upper right of the screen. When you can create a new notebook and type code into the browser, the web browser and the Jupyter notebook server communicate with each other. The Jupyter Notebook server does the work and calculations, and the web browser renders the notebook. The web browser then displays the updated notebook to you. Managing other software used in jupyter notebooks When we installed jupyter notebook, we also installed pandas. We can check that our installation worked by importing pandas. import pandas as pd It works! This used the pandas library we installed when we created the environment. If we later realize we need another piece of software, say seaborn, we can check if it's installed: import seaborn Since we haven't installed it yet, it's not available and we get the error: --------------------------------------------------------------------------- ModuleNotFoundError Traceback (most recent call last) Cell In [2], line 1 ----> 1 import seaborn ModuleNotFoundError: No module named 'seaborn' We need to install it into our jupyter environment and then it will be accessible. We can do this from our jupyter notebook using the ! to access bash from the notebook. We'll add the -y flag to our installation command to autamatically accept the install -- we do this because jupyter notebooks aren't interactive in the same way that the terminal would be during conda/mamba installations. !mamba install seaborn -y Then seaborn will be installed for us to use. import seaborn You can also install things from the terminal into the environment when your jupyter environment is activated. Check your prompt to make sure it's prepended with (jupyter) , and then install the software you're interested in obtaining: mamba install scikit-learn Then, in our notebook, we could run: import sklearn Shutting down a jupyter notebook You can use the jupyter notebook interface to shut down specific notebooks. To stop jupyter from running, you need to run Ctrl + c in the same terminal that you originall ran jupyter notebook in. Jupyter will prompt you to confirm the shut down. Press y and then Enter . Setting up an R kernel to run R code in a jupyter notebook Jupyter notebooks can run languages other than python but require additional configuration. To run R, we need to install a package called irkernel. We'll create a new environment, and also install popular R packages like tidyverse. conda deactivate mamba create -n tidyjupyter jupyter r-irkernel r-tidyverse Activate the new environment conda activate tidyjupyter And start jupyter notebook jupyter notebook Now when we start a new notebook from jupyter, we have the option to start it with an R kernel. Reviewing pull requests on GitHub that contain jupyter notebooks The Arcadia Science organization has integration ReviewNB into our pull requests. Because jupyter notebooks are voluminous json text files, than can be really hard to review -- small changes in code (the thing we care about) can lead to huge changes in the metadata in the notebook (less interesting stuff we have to wade through). The ReviewNB tool automatically comments on PRs with jupyter notebooks and takes to you an external page where you can review rendered notebooks with side-by-side diffs. Attribution This lesson was modified from the Data Carpentry python ecology lesson on jupyter notebooks .","title":"Introduction to Jupyter notebooks"},{"location":"arcadia-users-group/20221024-jupyter-notebooks/lesson/#introduction-to-jupyter-notebooks","text":"","title":"Introduction to Jupyter notebooks"},{"location":"arcadia-users-group/20221024-jupyter-notebooks/lesson/#jumping-in-with-jupyter-notebooks","text":"We'll start today's tutorial using a Binder installation of jupyter notebook in the cloud. After we take a tour of jupyter notebooks, we'll learn how to install and interact with them on our local computers. Click the button below to launch a jupyter notebook through binder . This will launch a computer in the cloud. You'll interact with this computer through your browser. Click the Python 3 button under the Notebook section to launch a jupyter notebook. More information on binder and what happens when you click the launch binder button. Binder is a service that turns a Git repo into a collection of interactive notebooks. When a repository is configured to run as a binder, passing the GitHub repository URL to binder starts the binder-building process. Binder first builds a docker image that contains all of the software installations specified by a special set of files in the GitHub repository. A docker image is a set of instructions that are used to create a docker container. A docker container is a runnable instance of a docker image -- it's an encapsulated computing environment that can be used to reproducibly install sets of software on diverse computers. Armed with the docker container, binder launches an \"instance\" in the cloud (either on Google Cloud or AWS typically) on which it runs the docker container. Binder does some additional work in the background -- if no software configuration files are provided in the GitHub repo, or if those contain a minimal set of software, binder will by default include JupyterHub in the docker. When the cloud instance is launched, this is the screen you interact with. You interact with the cloud instance in your browser. Binders are ephemeral instances -- after a period of inactivity, the instance is automatically shut down, and any work you have done will be lost. You're able to download files from your work before the instance is shut down if you do want to save anything. You may notice that this instance already has a bunch of files on it. And that these files look suspiciously exactly like the files in the GitHub repository Arcadia-Science/arcadia-computational-training . That's because that's the repository we used to build the binder from.","title":"Jumping in with Jupyter notebooks"},{"location":"arcadia-users-group/20221024-jupyter-notebooks/lesson/#jupyter-notebook-vs-jupyterlab","text":"Technically, our entry point for this tutorial is JupyterLab. JupyterLab is the next-generation user interface that includes notebooks. For an in depth comparison between Jupyter notebooks and JupyterLab, see this article . The image below depicts the JupyterLab (left) and Jupyter notebook (right) start up screens. JupyterLab provides a richer set of entry points for computation. Once you start a notebook, the notebook interface is identical.","title":"Jupyter notebook vs. JupyterLab"},{"location":"arcadia-users-group/20221024-jupyter-notebooks/lesson/#how-the-jupyter-notebook-works","text":"Click in the first cell and type some Python code. This is a Code cell (see the cell type dropdown with the word Code ). To run the cell, type Shift + Return . Let's look at a Markdown cell. Markdown is a text manipulation language that is readable yet offers additional formatting. Don't forget to select Markdown from the cell type dropdown. Click in the cell and enter the markdown text. To run the cell, type Shift + Return . This workflow has several advantages: You can easily type, edit, and copy and paste blocks of code. Tab completion allows you to easily access the names of things you are using and learn more about them. It allows you to annotate your code with links, different sized text, bullets, etc. to make information more accessible to you and your collaborators. It allows you to display figures next to the code that produces them to tell a complete story of the analysis.","title":"How the Jupyter notebook works"},{"location":"arcadia-users-group/20221024-jupyter-notebooks/lesson/#how-the-notebook-is-stored","text":"The notebook file is stored in a format called JSON and has the suffix .ipynb . Just like HTML for a webpage, what's saved in a notebook file looks different from what you see in your browser. This format allows Jupyter to mix software (in several languages) with documentation and graphics, all in one file.","title":"How the notebook is stored"},{"location":"arcadia-users-group/20221024-jupyter-notebooks/lesson/#viewing-and-interacting-with-jupyter-notebooks","text":"While jupyter notebooks are saved in a plain text file in JSON format, the files themselves are quite large (often exceeding a megabyte). This text-rich format isn't very rewarding to look at it in its raw format. Historically, one needed to use jupyter notebook to open, view (render), and execute a jupyter notebook. This could be a heavy lift when you wanted to share a notebook with collaborators. This is probably still the most popular way to interact with notebooks but there are now lighter weight alternatives. Viewing and interacting with jupyter notebooks. To view a jupyter notebook, you can use any of the following strategies. Use a local installation of jupyter notebook or jupyter hub to open the notebook in your browser. You'll have full functionality of the notebook with this strategy. Use VS Code to open and render the notebook. You'll still need jupyter installed locally for this to work, and you'll have full functionality of the notebook but it will be rendered directly in VS Code. Only viewing jupyter notebooks. If your goal is view the notebook, or allow others to view the notebook, you can use the following strategies. Export the notebook to html, markdown, or PDF. You can then share the file with others who would like to view it. Upload the notebook to a GitHub repository. GitHub will automatically render the notebook, allowing others to view it without having to download it and use a local installation of jupyter notebook to open it. You can use nbviewer or other third party applications that render and distribute jupyter notebooks.","title":"Viewing and interacting with jupyter notebooks"},{"location":"arcadia-users-group/20221024-jupyter-notebooks/lesson/#notebook-modes-control-and-edit","text":"The notebook has two modes of operation: Control and Edit. Control mode lets you edit notebook level features; while, Edit mode lets you change the contents of a notebook cell. Remember a notebook is made up of a number of cells which can contain code, markdown, html, visualizations, and more.","title":"Notebook modes: Control and Edit"},{"location":"arcadia-users-group/20221024-jupyter-notebooks/lesson/#executing-shell-commands-in-a-jupyter-notebook","text":"To execute a line of bash code in a python jupyter notebook, prepend the line of code with an ! : !ls","title":"Executing shell commands in a jupyter notebook"},{"location":"arcadia-users-group/20221024-jupyter-notebooks/lesson/#running-jupyter-notebooks-locally-installation-and-startup","text":"Until this point, we've been using a JupyterLab installation on a cloud computer. This allowed us to learn how to use Jupyter notebooks without having to install the program ourselves. In this section, we'll learn to install jupyter locally using conda. If you don't have conda install, head over to the miniconda lesson to get it set up on your machine.","title":"Running jupyter notebooks locally: installation and startup"},{"location":"arcadia-users-group/20221024-jupyter-notebooks/lesson/#managing-jupyter-notebook-installations-with-conda","text":"Jupyter brings along a lot of dependencies, so we'll create a new environment just for jupyter. In the long run, you'll also need to install any dependencies you need into this environment. For practice, we'll install pandas and seaborn to demonstrate how to do this. mamba create -n jupyter jupyter pandas After we've created our environment, we need to activate it to be able to access the software we just installed. conda activate jupyter The command to start a jupyter notebook is jupyter notebook . jupyter notebook","title":"Managing jupyter notebook installations with conda"},{"location":"arcadia-users-group/20221024-jupyter-notebooks/lesson/#starting-a-jupyter-notebook-and-an-explanation-of-what-jupyter-is-doing","text":"After typing the command jupyter notebook , the following happens: A Jupyter Notebook server is automatically created on your local machine. The Jupyter Notebook server runs locally on your machine only and does not use an internet connection. The Jupyter Notebook server opens the Jupyter notebook client, also known as the notebook user interface, in your default web browser. To create a new Python notebook select the \"New\" dropdown on the upper right of the screen. When you can create a new notebook and type code into the browser, the web browser and the Jupyter notebook server communicate with each other. The Jupyter Notebook server does the work and calculations, and the web browser renders the notebook. The web browser then displays the updated notebook to you.","title":"Starting a jupyter notebook and an explanation of what jupyter is doing"},{"location":"arcadia-users-group/20221024-jupyter-notebooks/lesson/#managing-other-software-used-in-jupyter-notebooks","text":"When we installed jupyter notebook, we also installed pandas. We can check that our installation worked by importing pandas. import pandas as pd It works! This used the pandas library we installed when we created the environment. If we later realize we need another piece of software, say seaborn, we can check if it's installed: import seaborn Since we haven't installed it yet, it's not available and we get the error: --------------------------------------------------------------------------- ModuleNotFoundError Traceback (most recent call last) Cell In [2], line 1 ----> 1 import seaborn ModuleNotFoundError: No module named 'seaborn' We need to install it into our jupyter environment and then it will be accessible. We can do this from our jupyter notebook using the ! to access bash from the notebook. We'll add the -y flag to our installation command to autamatically accept the install -- we do this because jupyter notebooks aren't interactive in the same way that the terminal would be during conda/mamba installations. !mamba install seaborn -y Then seaborn will be installed for us to use. import seaborn You can also install things from the terminal into the environment when your jupyter environment is activated. Check your prompt to make sure it's prepended with (jupyter) , and then install the software you're interested in obtaining: mamba install scikit-learn Then, in our notebook, we could run: import sklearn","title":"Managing other software used in jupyter notebooks"},{"location":"arcadia-users-group/20221024-jupyter-notebooks/lesson/#shutting-down-a-jupyter-notebook","text":"You can use the jupyter notebook interface to shut down specific notebooks. To stop jupyter from running, you need to run Ctrl + c in the same terminal that you originall ran jupyter notebook in. Jupyter will prompt you to confirm the shut down. Press y and then Enter .","title":"Shutting down a jupyter notebook"},{"location":"arcadia-users-group/20221024-jupyter-notebooks/lesson/#setting-up-an-r-kernel-to-run-r-code-in-a-jupyter-notebook","text":"Jupyter notebooks can run languages other than python but require additional configuration. To run R, we need to install a package called irkernel. We'll create a new environment, and also install popular R packages like tidyverse. conda deactivate mamba create -n tidyjupyter jupyter r-irkernel r-tidyverse Activate the new environment conda activate tidyjupyter And start jupyter notebook jupyter notebook Now when we start a new notebook from jupyter, we have the option to start it with an R kernel.","title":"Setting up an R kernel to run R code in a jupyter notebook"},{"location":"arcadia-users-group/20221024-jupyter-notebooks/lesson/#reviewing-pull-requests-on-github-that-contain-jupyter-notebooks","text":"The Arcadia Science organization has integration ReviewNB into our pull requests. Because jupyter notebooks are voluminous json text files, than can be really hard to review -- small changes in code (the thing we care about) can lead to huge changes in the metadata in the notebook (less interesting stuff we have to wade through). The ReviewNB tool automatically comments on PRs with jupyter notebooks and takes to you an external page where you can review rendered notebooks with side-by-side diffs.","title":"Reviewing pull requests on GitHub that contain jupyter notebooks"},{"location":"arcadia-users-group/20221024-jupyter-notebooks/lesson/#attribution","text":"This lesson was modified from the Data Carpentry python ecology lesson on jupyter notebooks .","title":"Attribution"},{"location":"arcadia-users-group/20221031-binder/lesson/","text":"Turning a GitHub repo into a collection of interactive notebooks with Binder A Binder (also called a Binder-ready repository) is a code repository that contains at least two things: Code or content that you\u2019d like people to run. This might be a Jupyter Notebook that explains an idea, or an R script that makes a visualization. Configuration files for your environment. These files are used by Binder to build the environment needed to run your code. For a list of all configuration files available, see the Configuration Files page . A Binder repository can be built by a BinderHub, which will generate a link that you can share with others, allowing them to interact with the content in your repository. mybinder.org and pangeo-binder are two free online BinderHubs that build the shareable reproducible and interactive computational environments from binder-compatible online repositories. In this lesson, we will create a Binder project from scratch: we will first make a repository on GitHub and then launch it on mybinder.org. This lesson will tie together pieces from all of our previous lessons. We'll use our new knowledge about the command line, Git and Github, conda, markdown, jupyter notebooks, and reproducible computing in general. Creating a Binderize-able python repository Interested in creating a Binder repo for a different language (like R or Julia)? See this lesson . We'll start by creating a python Binder repository on GitHub. Create a new repo on GitHub called 2022-my-py-binder Make sure the repository is public , not private Initialize the repo with a README Create a file called hello.py via the web interface with print(\"Hello from Binder!\") on the first line and commit to the main branch Why does the repo have to be public? mybinder.org cannot access private repositories as this would require a secret token. The Binder team choose not to take on the responsibility of handling secret tokens as mybinder.org is a public service and proof of technological concept. If accessing private repositories is a feature you/your team need, you can look into building your own BinderHub. Launching your Binderized repository on mybinder.org Go to https://mybinder.org Type the URL of your repo into the \"GitHub repo or URL\" box. It should look like this: https://github.com/YOUR-USERNAME/2022-my-py-binder ` As you type, the webpage generates a link in the \"Copy the URL below...\" box It should look like this: https://mybinder.org/v2/gh/YOUR-USERNAME/2022-my-py-binder/HEAD Copy it, open a new browser tab and visit that URL You will see a \"spinner\" as Binder launches the repo If everything ran smoothly, you'll see a JupyterLab interface. What happens when you launch a Binder? In the background, BinderHub (the backend of Binder) fetches your repo from GitHub and analyzes the contents of the files in the repo. It then builds a docker image based on your repo and launches the docker image in the cloud. Lastly, BinderHub connects you to the instance running the docker image via your browser. Interacting with files in Binder The Binder environment is fully executable. Let's execute our python script from the command line. Click the Terminal button to open the Terminal app, then run: python hello.py Hello from Binder! should be printed to the terminal. Computational limits of Binder While running, users are guaranteed at least 1GB of RAM, with a maximum of 2GB. This means you will always have 1GB, you may occasionally have between 1 and 2GB, and if you go over 2GB your kernel will be restarted. Binder is meant for interactive and ephemeral interactive coding, meaning that it is ideally suited for relatively short sessions. Binder will automatically shut down user sessions that have more than 10 minutes of inactivity (if you leave a jupyterlab window open in the foreground, this will generally be counted as \"activity\"). Binder aims to provide up to six hours of session time per user session, or up to one cpu-hour for more computationally intensive sessions. Beyond that, Binder does not guarantee that the session will remain running. For up-to-date information on Binder, see here . Building out the environment and pinning dependencies By including a .py file in our repository, BinderHub knew that we wanted to launch a Binder with python installed. We can provide more explicit instructions to customize the installation experience. We'll do this using conda because conda allows us to install python, R, ... dependencies. The environment.yml is the standard configuration file used by conda that lets you install any kind of package, including Python, R, and C/C++ packages. BinderHub will use this file to update the base conda environment with the packages listed in your environment.yml -- it does not create and activate a new conda environment. This means that the environment will always have the same default name, not the name specified in your environment.yml . You can install files from pip in your environment.yml as well. For example, see the binder-examples environment.yml file . We'll start by creating an environment.yml and pinning a dependency. In your repo, create a file called environment.yml Add the following lines: channels: - conda-forge - bioconda - defaults dependencies: - numpy=1.14.5 Check for typos! Then commit to the main branch Visit https://mybinder.org/v2/gh/YOUR-USERNAME/2022-my-py-binder/HEAD again in a new tab This time, BinderHub will read the configuration file you added and install the specific version of the package you requested using conda. Using dependencies in the notebook environment From the launch panel, select \"Python 3\" from the Notebook section to open a new notebook Type the following into a new cell and run it with Shift + Enter import numpy print(numpy.__version__) numpy.random.randn() If you save this notebook, it will not be automatically saved to the GitHub repo. Any changes you have made to files inside the Binder will be lost once you close the browser window. You can always download the file locally if you would like to keep a copy of the changes you made. If you have write access to the repository you're working with in Binder, you can set up a key file and ssh access for the repository and push changes you make. Sharing your Binder with others Binder is all about sharing your work easily and there are two ways to do it: Share the https://mybinder.org/v2/gh/YOUR-USERNAME/2022-my-py-binder/HEAD URL directly Visit https://mybinder.org , type in the URL of your repo and copy the Markdown or ReStructured Text snippet into your README.md file. This snippet will render a badge that people can click, which looks like this: Add the Markdown snippet from https://mybinder.org to the README.md file in your repo. The grey bar displaying a binder badge will unfold to reveal the snippets. Click the clipboard icon next to the box marked with \"m\" to automatically copy the Markdown snippet. Click the badge to make sure it works! Accessing data from Binder Another kind of dependency for projects is data . There are different ways to make data available in your Binder depending on the size of your data and your preferences for sharing it. Small public files. The simplest approach for small, public data files is to add them directly into your GitHub repository. They are then directly encapsulated into the environment and versioned along with your code. This is ideal for files up to 10MB . Medium public files. To access medium files from a few 10s MB up to a few hundred MB , you can add a file called postBuild to your repo. A postBuild file is a shell script that is executed as part of the image construction and is only executed once when a new image is built, not every time the Binder is launched. See Binder's postBuild example for more uses of the postBuild script. Large public files. It is not practical to place large files in your GitHub repo or include them directly in the image that Binder builds. The best option for large files is to use a library specific to the data format to stream the data as you're using it or to download it on demand as part of your code. For security reasons, the outgoing traffic of your Binder is restricted to HTTP/S or GitHub connections only. You will not be able to use FTP sites to fetch data on mybinder.org. Get data with postBuild Go to your GitHub repo and create a file called postBuild In postBuild , add a single line reading: wget -q -O gapminder.csv http://bit.ly/2uh4s3g wget is a program which retrieves content from web servers. This line extracts the content from the bitly URL and saves it to the filename denoted by the -O flag (capital \"O\", not zero), in this case gapminder.csv . The -q flag tells wget to do this quietly, meaning it won't print anything to the console. Update your environment.yml file by adding a new line with pandas on it and another new line with matplotlib on it. These packages aren't necessary to download the data but we will use them to read the CSV file and make a plot. Click the binder badge in your README to launch your Binder Once the Binder has launched, you should see a new file has appeared that was not part of your repo when you clicked the badge. Now visualise the data by creating a new notebook (selecting \"Python 3\" from the Notebook section) and run the following code in a cell. % matplotlib inline import pandas data = pandas . read_csv ( \"gapminder.csv\" , index_col = \"country\" ) years = data . columns . str . strip ( \"gdpPercap_\" ) # Extract year from last 4 characters of each column name data . columns = years . astype ( int ) # Convert year values to integers, saving results back to dataframe data . loc [ \"Australia\" ] . plot () Creating a Binderize-able R repository Creating an R repository is similar to creating a python repository. There are multiple ways to do this, but we'll do it using conda. Create a new repo on GitHub called 2022-my-r-binder Make sure the repository is public , not private Initialize the repo with a README Create a file called hello.R via the web interface with print(\"Hello from Binder!\") on the first line and commit to the main branch Create a file called runtime.txt with r-2022-01-01 on the first line. This date represents the snapshot of CRAN hosted on the RStudio Package Manager we will use. Commit this file to the main branch. Then, follow the launch binder instructions: Go to https://mybinder.org Type the URL of your repo into the \"GitHub repo or URL\" box. It should look like this: https://github.com/YOUR-USERNAME/2022-my-r-binder ` As you type, the webpage generates a link in the \"Copy the URL below...\" box It should look like this: https://mybinder.org/v2/gh/YOUR-USERNAME/2022-my-r-binder/HEAD Copy it, open a new browser tab and visit that URL You will see a \"spinner\" as Binder launches the repo If everything ran smoothly, you'll see a JupyterLab interface, but this time you should be able to open the RStudio interface. Note that it takes much longer to build an R binder than it does to build a python binder. You can use the same conda approach of creating an environment.yml to install additional R packages. Attributions This lesson was modified in part from the following sources: Binder user guide The Turing Way Zero-to-Binder ( license ).","title":"Turning a GitHub repo into a collection of interactive notebooks with Binder"},{"location":"arcadia-users-group/20221031-binder/lesson/#turning-a-github-repo-into-a-collection-of-interactive-notebooks-with-binder","text":"A Binder (also called a Binder-ready repository) is a code repository that contains at least two things: Code or content that you\u2019d like people to run. This might be a Jupyter Notebook that explains an idea, or an R script that makes a visualization. Configuration files for your environment. These files are used by Binder to build the environment needed to run your code. For a list of all configuration files available, see the Configuration Files page . A Binder repository can be built by a BinderHub, which will generate a link that you can share with others, allowing them to interact with the content in your repository. mybinder.org and pangeo-binder are two free online BinderHubs that build the shareable reproducible and interactive computational environments from binder-compatible online repositories. In this lesson, we will create a Binder project from scratch: we will first make a repository on GitHub and then launch it on mybinder.org. This lesson will tie together pieces from all of our previous lessons. We'll use our new knowledge about the command line, Git and Github, conda, markdown, jupyter notebooks, and reproducible computing in general.","title":"Turning a GitHub repo into a collection of interactive notebooks with Binder"},{"location":"arcadia-users-group/20221031-binder/lesson/#creating-a-binderize-able-python-repository","text":"Interested in creating a Binder repo for a different language (like R or Julia)? See this lesson . We'll start by creating a python Binder repository on GitHub. Create a new repo on GitHub called 2022-my-py-binder Make sure the repository is public , not private Initialize the repo with a README Create a file called hello.py via the web interface with print(\"Hello from Binder!\") on the first line and commit to the main branch Why does the repo have to be public? mybinder.org cannot access private repositories as this would require a secret token. The Binder team choose not to take on the responsibility of handling secret tokens as mybinder.org is a public service and proof of technological concept. If accessing private repositories is a feature you/your team need, you can look into building your own BinderHub.","title":"Creating a Binderize-able python repository"},{"location":"arcadia-users-group/20221031-binder/lesson/#launching-your-binderized-repository-on-mybinderorg","text":"Go to https://mybinder.org Type the URL of your repo into the \"GitHub repo or URL\" box. It should look like this: https://github.com/YOUR-USERNAME/2022-my-py-binder ` As you type, the webpage generates a link in the \"Copy the URL below...\" box It should look like this: https://mybinder.org/v2/gh/YOUR-USERNAME/2022-my-py-binder/HEAD Copy it, open a new browser tab and visit that URL You will see a \"spinner\" as Binder launches the repo If everything ran smoothly, you'll see a JupyterLab interface. What happens when you launch a Binder? In the background, BinderHub (the backend of Binder) fetches your repo from GitHub and analyzes the contents of the files in the repo. It then builds a docker image based on your repo and launches the docker image in the cloud. Lastly, BinderHub connects you to the instance running the docker image via your browser.","title":"Launching your Binderized repository on mybinder.org"},{"location":"arcadia-users-group/20221031-binder/lesson/#interacting-with-files-in-binder","text":"The Binder environment is fully executable. Let's execute our python script from the command line. Click the Terminal button to open the Terminal app, then run: python hello.py Hello from Binder! should be printed to the terminal.","title":"Interacting with files in Binder"},{"location":"arcadia-users-group/20221031-binder/lesson/#computational-limits-of-binder","text":"While running, users are guaranteed at least 1GB of RAM, with a maximum of 2GB. This means you will always have 1GB, you may occasionally have between 1 and 2GB, and if you go over 2GB your kernel will be restarted. Binder is meant for interactive and ephemeral interactive coding, meaning that it is ideally suited for relatively short sessions. Binder will automatically shut down user sessions that have more than 10 minutes of inactivity (if you leave a jupyterlab window open in the foreground, this will generally be counted as \"activity\"). Binder aims to provide up to six hours of session time per user session, or up to one cpu-hour for more computationally intensive sessions. Beyond that, Binder does not guarantee that the session will remain running. For up-to-date information on Binder, see here .","title":"Computational limits of Binder"},{"location":"arcadia-users-group/20221031-binder/lesson/#building-out-the-environment-and-pinning-dependencies","text":"By including a .py file in our repository, BinderHub knew that we wanted to launch a Binder with python installed. We can provide more explicit instructions to customize the installation experience. We'll do this using conda because conda allows us to install python, R, ... dependencies. The environment.yml is the standard configuration file used by conda that lets you install any kind of package, including Python, R, and C/C++ packages. BinderHub will use this file to update the base conda environment with the packages listed in your environment.yml -- it does not create and activate a new conda environment. This means that the environment will always have the same default name, not the name specified in your environment.yml . You can install files from pip in your environment.yml as well. For example, see the binder-examples environment.yml file . We'll start by creating an environment.yml and pinning a dependency. In your repo, create a file called environment.yml Add the following lines: channels: - conda-forge - bioconda - defaults dependencies: - numpy=1.14.5 Check for typos! Then commit to the main branch Visit https://mybinder.org/v2/gh/YOUR-USERNAME/2022-my-py-binder/HEAD again in a new tab This time, BinderHub will read the configuration file you added and install the specific version of the package you requested using conda.","title":"Building out the environment and pinning dependencies"},{"location":"arcadia-users-group/20221031-binder/lesson/#using-dependencies-in-the-notebook-environment","text":"From the launch panel, select \"Python 3\" from the Notebook section to open a new notebook Type the following into a new cell and run it with Shift + Enter import numpy print(numpy.__version__) numpy.random.randn() If you save this notebook, it will not be automatically saved to the GitHub repo. Any changes you have made to files inside the Binder will be lost once you close the browser window. You can always download the file locally if you would like to keep a copy of the changes you made. If you have write access to the repository you're working with in Binder, you can set up a key file and ssh access for the repository and push changes you make.","title":"Using dependencies in the notebook environment"},{"location":"arcadia-users-group/20221031-binder/lesson/#sharing-your-binder-with-others","text":"Binder is all about sharing your work easily and there are two ways to do it: Share the https://mybinder.org/v2/gh/YOUR-USERNAME/2022-my-py-binder/HEAD URL directly Visit https://mybinder.org , type in the URL of your repo and copy the Markdown or ReStructured Text snippet into your README.md file. This snippet will render a badge that people can click, which looks like this: Add the Markdown snippet from https://mybinder.org to the README.md file in your repo. The grey bar displaying a binder badge will unfold to reveal the snippets. Click the clipboard icon next to the box marked with \"m\" to automatically copy the Markdown snippet. Click the badge to make sure it works!","title":"Sharing your Binder with others"},{"location":"arcadia-users-group/20221031-binder/lesson/#accessing-data-from-binder","text":"Another kind of dependency for projects is data . There are different ways to make data available in your Binder depending on the size of your data and your preferences for sharing it. Small public files. The simplest approach for small, public data files is to add them directly into your GitHub repository. They are then directly encapsulated into the environment and versioned along with your code. This is ideal for files up to 10MB . Medium public files. To access medium files from a few 10s MB up to a few hundred MB , you can add a file called postBuild to your repo. A postBuild file is a shell script that is executed as part of the image construction and is only executed once when a new image is built, not every time the Binder is launched. See Binder's postBuild example for more uses of the postBuild script. Large public files. It is not practical to place large files in your GitHub repo or include them directly in the image that Binder builds. The best option for large files is to use a library specific to the data format to stream the data as you're using it or to download it on demand as part of your code. For security reasons, the outgoing traffic of your Binder is restricted to HTTP/S or GitHub connections only. You will not be able to use FTP sites to fetch data on mybinder.org.","title":"Accessing data from Binder"},{"location":"arcadia-users-group/20221031-binder/lesson/#get-data-with-postbuild","text":"Go to your GitHub repo and create a file called postBuild In postBuild , add a single line reading: wget -q -O gapminder.csv http://bit.ly/2uh4s3g wget is a program which retrieves content from web servers. This line extracts the content from the bitly URL and saves it to the filename denoted by the -O flag (capital \"O\", not zero), in this case gapminder.csv . The -q flag tells wget to do this quietly, meaning it won't print anything to the console. Update your environment.yml file by adding a new line with pandas on it and another new line with matplotlib on it. These packages aren't necessary to download the data but we will use them to read the CSV file and make a plot. Click the binder badge in your README to launch your Binder Once the Binder has launched, you should see a new file has appeared that was not part of your repo when you clicked the badge. Now visualise the data by creating a new notebook (selecting \"Python 3\" from the Notebook section) and run the following code in a cell. % matplotlib inline import pandas data = pandas . read_csv ( \"gapminder.csv\" , index_col = \"country\" ) years = data . columns . str . strip ( \"gdpPercap_\" ) # Extract year from last 4 characters of each column name data . columns = years . astype ( int ) # Convert year values to integers, saving results back to dataframe data . loc [ \"Australia\" ] . plot ()","title":"Get data with postBuild"},{"location":"arcadia-users-group/20221031-binder/lesson/#creating-a-binderize-able-r-repository","text":"Creating an R repository is similar to creating a python repository. There are multiple ways to do this, but we'll do it using conda. Create a new repo on GitHub called 2022-my-r-binder Make sure the repository is public , not private Initialize the repo with a README Create a file called hello.R via the web interface with print(\"Hello from Binder!\") on the first line and commit to the main branch Create a file called runtime.txt with r-2022-01-01 on the first line. This date represents the snapshot of CRAN hosted on the RStudio Package Manager we will use. Commit this file to the main branch. Then, follow the launch binder instructions: Go to https://mybinder.org Type the URL of your repo into the \"GitHub repo or URL\" box. It should look like this: https://github.com/YOUR-USERNAME/2022-my-r-binder ` As you type, the webpage generates a link in the \"Copy the URL below...\" box It should look like this: https://mybinder.org/v2/gh/YOUR-USERNAME/2022-my-r-binder/HEAD Copy it, open a new browser tab and visit that URL You will see a \"spinner\" as Binder launches the repo If everything ran smoothly, you'll see a JupyterLab interface, but this time you should be able to open the RStudio interface. Note that it takes much longer to build an R binder than it does to build a python binder. You can use the same conda approach of creating an environment.yml to install additional R packages.","title":"Creating a Binderize-able R repository"},{"location":"arcadia-users-group/20221031-binder/lesson/#attributions","text":"This lesson was modified in part from the following sources: Binder user guide The Turing Way Zero-to-Binder ( license ).","title":"Attributions"},{"location":"arcadia-users-group/20221205-keyboard-shortcuts-terminal/lesson/","text":"Keyboard shortcuts for Terminal The Terminal (or iTerm) is the MacOS application to access the command line (see lessons on the introduction to the command line here and here . Today we'll cover keyboard shortcuts that can be used to navigate around iTerm -- fnding old commands you've run, navigating to different parts of the command quickly, opening new terminal windows, etc. We'll highlight a few key commands below, but Apple has curated an extensive, operating-system specific list of keyboard shortcuts here . The commands shown below are for Mac OS 13.0, but many will be operating system agnostic and will also work in Linux. Edit a command line Action Shortcut Reposition the insertion point Press and hold Option while moving the pointer to a new insertion point Move the insertion point to the beginning of the line Control + a Move the insertion point to the end of the line Control + e Move the insertion point forward one character Right Arrow Move the insertion point backward one character Left Arrow Move the insertion point forward one word Option + Right Arrow Move the insertion point backward one word Option + Left Arrow Re-running and stopping commands Action Shortcut Find previously run commands Up Arrow Cancel a command Control + c Work with Terminal windows and tabs Action Shortcut New window Command + n New tab Command + t Next Tab Control + Tab Previous Tab Control + Shift + Tab Make fonts bigger Command + Plus (+) Make fonts smaller Command + Minus (\u2013) Split window into two panes Command + d Close split panel Shift + Command + d","title":"Keyboard shortcuts for Terminal"},{"location":"arcadia-users-group/20221205-keyboard-shortcuts-terminal/lesson/#keyboard-shortcuts-for-terminal","text":"The Terminal (or iTerm) is the MacOS application to access the command line (see lessons on the introduction to the command line here and here . Today we'll cover keyboard shortcuts that can be used to navigate around iTerm -- fnding old commands you've run, navigating to different parts of the command quickly, opening new terminal windows, etc. We'll highlight a few key commands below, but Apple has curated an extensive, operating-system specific list of keyboard shortcuts here . The commands shown below are for Mac OS 13.0, but many will be operating system agnostic and will also work in Linux.","title":"Keyboard shortcuts for Terminal"},{"location":"arcadia-users-group/20221205-keyboard-shortcuts-terminal/lesson/#edit-a-command-line","text":"Action Shortcut Reposition the insertion point Press and hold Option while moving the pointer to a new insertion point Move the insertion point to the beginning of the line Control + a Move the insertion point to the end of the line Control + e Move the insertion point forward one character Right Arrow Move the insertion point backward one character Left Arrow Move the insertion point forward one word Option + Right Arrow Move the insertion point backward one word Option + Left Arrow","title":"Edit a command line"},{"location":"arcadia-users-group/20221205-keyboard-shortcuts-terminal/lesson/#re-running-and-stopping-commands","text":"Action Shortcut Find previously run commands Up Arrow Cancel a command Control + c","title":"Re-running and stopping commands"},{"location":"arcadia-users-group/20221205-keyboard-shortcuts-terminal/lesson/#work-with-terminal-windows-and-tabs","text":"Action Shortcut New window Command + n New tab Command + t Next Tab Control + Tab Previous Tab Control + Shift + Tab Make fonts bigger Command + Plus (+) Make fonts smaller Command + Minus (\u2013) Split window into two panes Command + d Close split panel Shift + Command + d","title":"Work with Terminal windows and tabs"},{"location":"arcadia-users-group/20230214-code-review-for-pubs/lesson/","text":"Code review during the pub process At Arcadia, one of our central tenets is that our science should be maximally useful. We believe that useful computing is innovative, usable, reproducible, and timely. Increase reproducibility and usability of the computational products we put out by making sure: Our code is readable and well-documented. The software (and versions!) and resources we used are documented. It's clear how data inputs and outputs relate to the code. Code review FAQ and discussion Question: Is code review really necessary for things like figures, or things that just use existing methods and don't write new analysis code? Thoughts: We think so! For command line tools, the parameters you specify or the versions of tools you use can make a pretty big impact on results, so writing these things down explicity can be really helpful for interpretation of the results or for comparing to new results generated by the same tools. It's also super helpful for other people to see what you ran so they can use it as a recipe for a reasonable way to analyze their data -- think of it like a tutorial in disguise. For figures, we think filtering steps or transformations are important to capture. Even if you don't do any of these steps, your figure code is also like a tutorial in disguise \u2013 since we provide our input data and pubs openly, providing the code use to make a plot allows other people to interface with our results or to accomplish something similar to what we did. We get that coding a pub-ready figure is a big task \u2013 it's totally fine to edit titles, labels, and fonts in Illustrator/Photoshop after making the plot (just be careful not to edit the data!). Imagine a world in which you pick up a manuscript that has some really interesting new result. Wouldn't it be cool if you could you could access the code for the figures inline, and recreate them all, but zooming in on the results you specifically care about? Or even cooler \u2013 what if you could follow the exact recipe documented in the manuscript for your data, and then quickly add your observations to the plots in the original manuscript? Executable manuscripts are a thing, but they're hard for reasons beyond code and data availability. Even still, providing the code we use \u2013 including for our figures \u2013 is both a key component to achieving these products and a suitable minimum replacement when the full thing isn't possible. Question: Could code review happen after version 1 of the pub? Thoughts: The goal of code review in the pub release process is to make sure someone inside or outside of Arcadia could pick up the code you used and with minimal effort, reproduce the results you got and reuse the code you wrote. We think that code review prior to the release of the first version of a pub will increase the usefulness of the computational products we're releasing to the world. Our goal is to have the code review component of the pub release process become a formality; we want code review to happen quickly after a unit of code is written (a new function, a new analysis notebook, a new figure) so that the feedback you get is actually useful and doesn't slow you down. If we can get to that point, the checks in the pub release cycle will be super quick. To make this process faster and to reduce the heat around version 1 pub releases, we've created a checklist of the five things that are needed to approve code associated with the first release of a pub. Any other suggestions you get are just that \u2013 suggestions \u2013 and can be punted to later pub releases or ignored. Overview of the pub process and how it relates to code review Check out the What to expect from code review during the pub process Notion page for an up-to-date guide through this process. Minimums for passing code review: a checklist All software packages and their versions are documented. Data inputs and outputs are documented. Only relative paths (not absolute paths) are used and the relative paths reference files in the repo or there is documentation for how to get the file. Enough comments and/or documentation are provided so it's clear what the code does. DOI for pub is linked in the README. Most common pain points with code review Pushing to main or master instead of to a branch and opening a pull request Requesting review of the entire repo instead of requesting periodic reviews GitHub refresher Figure by Allison Horst. www.twitter.com/allison_horst Make a new folder wherever your GitHub repos live. Name it tmp-yourinitials-aug . Download this file as our pretend code that we've been working on. Save it in your new folder. Create an empty, private repository in the Arcadia-Science organization. Use the same name as the local folder we made. Copy and paste the instructions given to you on the screen. Create and checkout a new branch with git checkout -b ter/init-pr . Name it yourinitials/init-pr . Use git add , git commit , and git push to add our file to a branch in your new repository. Open a pull request and explain your changes. Convert your PR to a draft. Convert it back to \"ready for review\" and request review from someone. Resources to help you thrive through code review Your code review partner, AUG office hours, and the #software-questions channel Arcadia software handbook GitHub workshop and recording Future directions We really want the code review process to be as painless as possible. When we identify things that people do over and over again \u2013 building a binder from an R repo, writing a Nextflow pipeline \u2013 we can build templates that you can clone at the start of a project and use to cut down on repetitive work. If you have any other ideas of how we can help, we're always happy to try new things!","title":"Code review during the pub process"},{"location":"arcadia-users-group/20230214-code-review-for-pubs/lesson/#code-review-during-the-pub-process","text":"At Arcadia, one of our central tenets is that our science should be maximally useful. We believe that useful computing is innovative, usable, reproducible, and timely. Increase reproducibility and usability of the computational products we put out by making sure: Our code is readable and well-documented. The software (and versions!) and resources we used are documented. It's clear how data inputs and outputs relate to the code.","title":"Code review during the pub process"},{"location":"arcadia-users-group/20230214-code-review-for-pubs/lesson/#code-review-faq-and-discussion","text":"Question: Is code review really necessary for things like figures, or things that just use existing methods and don't write new analysis code? Thoughts: We think so! For command line tools, the parameters you specify or the versions of tools you use can make a pretty big impact on results, so writing these things down explicity can be really helpful for interpretation of the results or for comparing to new results generated by the same tools. It's also super helpful for other people to see what you ran so they can use it as a recipe for a reasonable way to analyze their data -- think of it like a tutorial in disguise. For figures, we think filtering steps or transformations are important to capture. Even if you don't do any of these steps, your figure code is also like a tutorial in disguise \u2013 since we provide our input data and pubs openly, providing the code use to make a plot allows other people to interface with our results or to accomplish something similar to what we did. We get that coding a pub-ready figure is a big task \u2013 it's totally fine to edit titles, labels, and fonts in Illustrator/Photoshop after making the plot (just be careful not to edit the data!). Imagine a world in which you pick up a manuscript that has some really interesting new result. Wouldn't it be cool if you could you could access the code for the figures inline, and recreate them all, but zooming in on the results you specifically care about? Or even cooler \u2013 what if you could follow the exact recipe documented in the manuscript for your data, and then quickly add your observations to the plots in the original manuscript? Executable manuscripts are a thing, but they're hard for reasons beyond code and data availability. Even still, providing the code we use \u2013 including for our figures \u2013 is both a key component to achieving these products and a suitable minimum replacement when the full thing isn't possible. Question: Could code review happen after version 1 of the pub? Thoughts: The goal of code review in the pub release process is to make sure someone inside or outside of Arcadia could pick up the code you used and with minimal effort, reproduce the results you got and reuse the code you wrote. We think that code review prior to the release of the first version of a pub will increase the usefulness of the computational products we're releasing to the world. Our goal is to have the code review component of the pub release process become a formality; we want code review to happen quickly after a unit of code is written (a new function, a new analysis notebook, a new figure) so that the feedback you get is actually useful and doesn't slow you down. If we can get to that point, the checks in the pub release cycle will be super quick. To make this process faster and to reduce the heat around version 1 pub releases, we've created a checklist of the five things that are needed to approve code associated with the first release of a pub. Any other suggestions you get are just that \u2013 suggestions \u2013 and can be punted to later pub releases or ignored.","title":"Code review FAQ and discussion"},{"location":"arcadia-users-group/20230214-code-review-for-pubs/lesson/#overview-of-the-pub-process-and-how-it-relates-to-code-review","text":"Check out the What to expect from code review during the pub process Notion page for an up-to-date guide through this process.","title":"Overview of the pub process and how it relates to code review"},{"location":"arcadia-users-group/20230214-code-review-for-pubs/lesson/#minimums-for-passing-code-review-a-checklist","text":"All software packages and their versions are documented. Data inputs and outputs are documented. Only relative paths (not absolute paths) are used and the relative paths reference files in the repo or there is documentation for how to get the file. Enough comments and/or documentation are provided so it's clear what the code does. DOI for pub is linked in the README.","title":"Minimums for passing code review: a checklist"},{"location":"arcadia-users-group/20230214-code-review-for-pubs/lesson/#most-common-pain-points-with-code-review","text":"Pushing to main or master instead of to a branch and opening a pull request Requesting review of the entire repo instead of requesting periodic reviews","title":"Most common pain points with code review"},{"location":"arcadia-users-group/20230214-code-review-for-pubs/lesson/#github-refresher","text":"Figure by Allison Horst. www.twitter.com/allison_horst Make a new folder wherever your GitHub repos live. Name it tmp-yourinitials-aug . Download this file as our pretend code that we've been working on. Save it in your new folder. Create an empty, private repository in the Arcadia-Science organization. Use the same name as the local folder we made. Copy and paste the instructions given to you on the screen. Create and checkout a new branch with git checkout -b ter/init-pr . Name it yourinitials/init-pr . Use git add , git commit , and git push to add our file to a branch in your new repository. Open a pull request and explain your changes. Convert your PR to a draft. Convert it back to \"ready for review\" and request review from someone.","title":"GitHub refresher"},{"location":"arcadia-users-group/20230214-code-review-for-pubs/lesson/#resources-to-help-you-thrive-through-code-review","text":"Your code review partner, AUG office hours, and the #software-questions channel Arcadia software handbook GitHub workshop and recording","title":"Resources to help you thrive through code review"},{"location":"arcadia-users-group/20230214-code-review-for-pubs/lesson/#future-directions","text":"We really want the code review process to be as painless as possible. When we identify things that people do over and over again \u2013 building a binder from an R repo, writing a Nextflow pipeline \u2013 we can build templates that you can clone at the start of a project and use to cut down on repetitive work. If you have any other ideas of how we can help, we're always happy to try new things!","title":"Future directions"},{"location":"arcadia-users-group/20230228-intro-to-python-1/lesson/","text":"Introduction to Python, Part 1 Follow along in Binder by clicking the badge below: 0. Fundamentals of Python Python is a popular programming language. It was designed to be easy to read in comparison to other programming languages, so it's a great first programming language to learn. This lesson covers the following basics of Python, such as: - variables - data types - built-in functions - creating custom functions - running Python from the command line Over the following lessons, we'll cover more basics of Python, including: - data structures - loops - conditional expressions - packages - methods - debugging - ... and more! At the end of this workshop series, you'll have enough experience with Python to be able to write your own basic Python scripts. You'll also have enough knowledge of Python to be able to use the OpenTrons API (Application-Program Interface) to control our in-house liquid handling robot, covered in the upcoming Intro to OpenTrons series. 0.1 Jupyter Notebooks For the first workshop, we'll work in an interactive computing environment called a Jupyter notebook. To run a \"cell\" of code in Jupyter, you'll press Shift + Return from inside the cell. The result of the cell - the output - will display just below the cell. You can tell if a cell has successfully executed based on the [ ] text to the left of the cell. A cell that is in progress has an asterisk [*] , and a cell that has completed has a number [1] . For more on Jupyter notebooks, you can check out this AUG Lesson . Fun fact! The Python programming language is not named for snakes, but instead for Monty Python, a signifier of the idea that the language should be fun to use . Despite this, many Python tools and packages make reference to snakes, such as packages you may have heard us talk about before: Anaconda , conda , mamba , snakemake , etc. 0.2 Python Version This lesson is written for Python version 3 and above. You can figure out what version of Python you're running using the cell below. (We'll explain how the cell below works at the end of the lesson.) ! python -- version Python 3.11.0 1. Variables Variables in programming languages are containers for assigning values. Below, we assign the value 1 to the variable x and the value 4 to the variable y . x = 1 y = 4 When we ask Python to evalute x + y , it substitutes the value 1 for x and the value 4 for y . Running the cell below returns the output value of the expression x + y . x + y 5 2. Data types Variables in Python come in a variety of types, such as: - int or integer : whole numbers such as 1 , 2 , 3 , etc. - float or floating-point number : numbers including decimals, e.g. 1.03 . - str or string : chains of alphanumeric characters flanked by quotation marks, e.g. \"apple\" . Double-quotes are stylistically perferred, but single-quotes also work. If you need to include quotation marks in a string, you can use the other option to \"escape\" the quotation mark. Python is able to natively perform certain operations using these datatypes, such as basic addition + , subtraction - , multiplication * , division / , etc. 1 + 2 * 3.4 / 5 2.36 Python also has some helpful intuitive operations \u2013 for example, you can join two strings together with a + operator as follows: \"pine\" + \"apple\" 'pineapple' Certain operations are not supported by default in Python - for example, you can't add a str with an int . Running the code below will cause a TypeError . Python is quite helpful in explaining errors you might encounter. Read the error message below and see if it makes sense to you. In general, you should expect to encounter lots of these errors, or \"bugs\", when you start getting into programming. This is totally normal! If you're ever confused about the source of an error, websites such as StackOverflow are great places to look for answers. One skill in learning to programming is learning what part of an error message to Google. 3 + \"5\" --------------------------------------------------------------------------- TypeError Traceback (most recent call last) Cell In[17], line 1 ----> 1 3 + \"5\" TypeError: unsupported operand type(s) for +: 'int' and 'str' Fun Fact! The term \"bug\" predates modern computers, but there are real examples of actual insects interfering with the function of a computer, such as this \"bug\" found at Harvard in 1947 . 3. Using variables You assign variables in Python using the variable = value syntax. You can perform operations on a mix of hard-coded values and variables interchangeably. This is very useful when you expect to use a variable in many different places in your script - Python keeps track of its value so you don't have to. If you use variable names that correspond to the value the variable holds (think wind_mph for wind speed), this can also help make your code more readable for yourself and others. a = 1 b = 6 + a c = a * b a + b + c 15 You can assign a different value to a variable later in your code, which replaces the original value. This can be useful, but it's also important to be careful not to overwrite variables you want to keep! a = 4 a + b 11 4. Built-in Python functions In addition to managing variables and values, Python also provides a variety of built-in functions . A function is a block of organized, reusable code that accomplishes a specific task. Functions are \"called\" by chaining the function name with parentheses () . Parameters of that function can be typed between the parentheses to tell the function what to do. For example, the print() function takes any value and prints it as output. print ( \"Hello, world!\" ) print ( a + b ) Hello, world! 11 We can call a function from within another function call. For example, the type() function gives the type of a Python value. print ( type ( 4 )) print ( type ( \"Hello, world!\" )) <class 'int'> <class 'str'> 5. Typecasting A bit earlier, we tried to evalute the expression 3 + '5' . This failed because we were trying to add an int and a str , which isn't natively supported operation with the + operand. If we wanted to actually get the sum of 3 + 5 in this case, but '5' is given as a str , we can actually force '5' to be an int instead using typecasting, as below: 3 + int ( \"5\" ) 8 You can see that now, instead of throwing a TypeError , the expression returns the expected value. We typecast in Python by calling our data type as a function, e.g. str() , int() , float() , etc. In some cases, you can't cast a value into another type; for example, running the code below throws a ValueError . int ( \"banana\" ) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) Cell In[24], line 1 ----> 1 int(\"banana\") ValueError: invalid literal for int() with base 10: 'banana' This example is probably pretty intuitive, as it's hard to imagine how you would turn the string 'banana' into an integer. But there are plenty of cases where typecasting won't work, so be aware that this isn't a perfect solution. P1. Practice Let's try some things with basic Python variables! Practice 1 What values do x and y have after each of the following statements? Practice 1 Answer x = 25, y does not yet exist x = 25, y = 6 x = 100.0, y = 6 x = 100.0, y = 10 x = 25 y = 6 x = x * 4.0 y = y + 4 Practice 2 What is the expected value of m at the end of the cell below? Practice 2 Answer This is a trick question! You might have expected the cell below to throw a TypeError because we're multiplying an int with a str . But `4 * '15'` is actually a special operation in Python; it's telling Python to repeat the string '15' four times. Therefore, the correct answer is '15151515' i = 3 j = \"5\" k = i * int ( j ) l = 4 m = l * str ( k ) 6. Defining a custom function In addition to working with built-in functions, you can also define your own functions in Python. This is one of the most powerful ways to use the language, as it allows you to write one block of code which you can use repeatedly in your scripts. Let's write our first function, called hello_world() . To define a custom function, you can start by writing def , followed by the desired name of your function, and parentheses () . Finish the line with a colon : . def hello_world (): print ( \"Hello world!\" ) When you execute the cell above, nothing prints out. Why? The answer is that using the code above, you've only told Python that you're defining a function called hello_world() , which itself calls print() . To run the function, you have to call it using the parentheses operator, as below. hello_world () Hello world! 7. Indentation and code blocks You've defined a function above, but how does Python know when the function ends? Python uses indentation to signify the scope of a function. In the code block below, we define a function, then define a variable, and finally pass the variable to the function. The line break and lack of indentation after the end of the function's scope tells Python that the function is completed. Note: This indentation syntax is uncommon among programming languages \u2013 often, languages require specific characters to set the scope of the code. For example, in C (another programming language), you end every code line with a semicolon ; . def hello_name ( name ): return \"Hello \" + name + \"!\" my_name = \"Guido\" hello_name ( my_name ) 'Hello Guido!' You might notice some differences in the code above to the hello_world function we defined previously. Let's go over these differences. There is now a variable name in the parentheses of the hello_name definition. This is how we define arguments of functions, or their input parameters. For example, the print() function accepts a comma-separated list of values as its arguments. Arguments act as placeholders \u2013 in the above example, name has no explicit value. But any value or variable placed into hello_name gets treated as the name variable, which is then kept through the rest of the function. The function ends with a return statement instead of a print() call. Most Python functions should end with a return statement. This is the value that the function spits out when it finishes running. If a function lacks a return statement, it will instead return None , a special value of the type NoneType . An important thing to know is that the output of functions can themselves be assigned to variables, as exemplified below. Combining function calls with variable assignment is a powerful way of using Python functions to modify or generate new variables. sentence_1 = hello_name ( my_name ) sentence_2 = \"My name is Python!\" print ( sentence_1 , sentence_2 ) Hello Guido! My name is Python! 8. Comments A crucial part of writing good code is being able to understand what it does later. For example, consider how mystifying it can be to try to read the code block below. Note: the ** operand is the exponential. def factor ( a , b , c ): step_one = - 1 * b step_q = 4 * a * c step_two_a = step_one + ( b ** 2 - step_q ) step_two_b = step_one - ( b ** 2 - step_q ) step_p = 2 * a step_three_a = step_two_a / step_p step_three_b = step_two_b / step_p result = step_three_a , step_three_b return result a = 1 b = 7 c = 3 factor ( a , b , c ) (15.0, -22.0) The function above is definitely an example of inefficiently written code, but let's put that aside for now. If you ended up writing the function above, it might be helpful to be able to know what the overall goal of the code is, and perhaps what each step is doing. This is where comments come to the rescue. Comments in Python begin on the left with a hash mark ( # ) and are ignored by the Python interpreter. Comments can also be written in blocks demarcated with triple-double-quotes ( \"\"\" ). You can even include a comment in the same line as functional code! Consider the code below with comments - how does the code feel with comments and additional line breaks included? # Applies the quadratic formula to find the intercepts of a quadratic equation of the format y = ax**2 + bx + c def factor ( a , b , c ): \"\"\" This is a block comment. The formula is: -b +/- sqrt(b**2 - 4ac) / 2a \"\"\" step_one = - 1 * b # evaluates -b step_q = 4 * a * c # evaluates 4ac step_two_a = step_one + ( b ** 2 - step_q ) # evaluates the numerator with addition step_two_b = step_one - ( b ** 2 - step_q ) # evaluates the numerator with subtraction step_p = 2 * a # evaluates the denominator step_three_a = step_two_a / step_p # gets the right intercept step_three_b = step_two_b / step_p # gets the left intercept result = step_three_a , step_three_b # combines the intercept into a tuple return result # Defines a, b, and c a = 1 b = 7 c = 3 # Run the actual function, returning result factor ( a , b , c ) (15.0, -22.0) P2. Practice Let's try writing some basic Python functions. Practice 3 Write a function called greetings() that takes two arguments: greeting and name and returns the string 'Greeting, Name!' Practice 3 Sample Answer def greetings(greeting, name): return greeting + ', ' + name + '!' ########################################### # Write your function in the space below. # ## Then run the cell to check your work. ## ########################################### greeting = \"Bonjour\" name = \"Jacques\" greetings ( greeting , name ) Bonjour, Jacques! Practice 4 What happens if you pass the variable robot_name below to your greetings() function? How could you modify the function to avoid this error? Practice 4 Sample Answer def greetings(greeting, name): return greeting + ', ' + str(name) + '!' ############################################### # Write your new function in the space below. # #### Then run the cell to check your work. #### ############################################### robot_name = 120 greetings ( greeting , robot_name ) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) Cell In[34], line 12 1 ############################################### 2 # Write your new function in the space below. # 3 #### Then run the cell to check your work. #### (...) 7 8 ############################################### 10 robot_name = 120 ---> 12 greetings(greeting, robot_name) Cell In[40], line 6, in greetings(greeting, name) 5 def greetings(greeting, name): ----> 6 print(greeting + \", \" + name + \"!\") TypeError: can only concatenate str (not \"int\") to str 9. Running Python from the command line In this lesson, we've taken advantage of the interactive computing experience offered by Jupyter notebooks. This computing environment is nice because it allows you to link input code to output data more easily. However, there are other ways to execute Python code. For example, you can write Python code into a text file ending with .py . We've included a script in the lesson folder called hello_world.py . Its contents are as follows: print ( \"Hello world!\" ) You can execute that script using the command line as follows: ! python hello_world . py Hello world! The cell above starts with a special Jupyter operator, the exclamation point ! , which passes code directly to a shell. This is the same as using python hello_world.py in your command line terminal. Code executed through a Python script works just the same as code executed in Jupyter, and comes with some advantages: - it's the most common way to use Python - it's easier to compartmentalize code that runs different functions - it doesn't requires setting up a Jupyter environment, which can be confusing In general, Jupyter notebooks are a helpful way to write exploratory code, while functions you've finalized and plan to use repeatedly are better off being stored in scripts. 9.1 Receiving user input with input() The built-in function input() prompts the user for input and accepts the response. input() takes a str argument which can be used to describe what the function is trying to get from the user as input. This is one way to be able to interact with a Python script interactively. In the example below, the user is prompted to input a day of the week. weekday = input ( \"What day of the week is it?\" ) print ( \"It sounds like it's \" + weekday ) What day of the week is it? Wednesday It sounds like it's Wednesday You can use this function to prompt a user for input as part of a Python script. Try running the script hello_name.py from a Terminal to see how the input function could be used. The contents of the script are shown below. name = input ( \"What's your name?\" ) print ( \"Hello, \" + name + \"!\" ) P3. Practice Let's try writing a Python script. Practice 5. Write a Python script in a file called greetings.py . The script should prompt the user for a greeting and a name as interactive input, and print {Greeting}, {Name}! You'll need to test the script using the terminal, in part because Jupyter doesn't reliably allow user input in output blocks. 10. Problem Set To really get familiar with coding in Python (or any other language), the best way is to write a lot of code . We recommend working through a problem set to practice some more with coding in Python. The problem set will also cover some of the basics we covered in a bit more detail. We expect the problem set won't take longer than ~1-2 hrs to complete. You can find the problem set at this Google CoLab Document . CoLab works just like a Jupyter notebook, but it automatically saves your changes and keeps them as a cloud document. To get started on the problem set, make sure to first click \"File\" > \"Save a copy in Drive\" to make sure you don't lose your progress! We'll have AUG office hours next week for you to stop by and work on your problem set with other Arcadians as well as to work through any bugs in your code. If you're stumped, we also have an Answer Key that you can check. Feel free to ping the #software-questions Slack channel for anything that comes up in the meantime!","title":"Introduction to Python, Part 1"},{"location":"arcadia-users-group/20230228-intro-to-python-1/lesson/#introduction-to-python-part-1","text":"Follow along in Binder by clicking the badge below:","title":"Introduction to Python, Part 1"},{"location":"arcadia-users-group/20230228-intro-to-python-1/lesson/#0-fundamentals-of-python","text":"Python is a popular programming language. It was designed to be easy to read in comparison to other programming languages, so it's a great first programming language to learn. This lesson covers the following basics of Python, such as: - variables - data types - built-in functions - creating custom functions - running Python from the command line Over the following lessons, we'll cover more basics of Python, including: - data structures - loops - conditional expressions - packages - methods - debugging - ... and more! At the end of this workshop series, you'll have enough experience with Python to be able to write your own basic Python scripts. You'll also have enough knowledge of Python to be able to use the OpenTrons API (Application-Program Interface) to control our in-house liquid handling robot, covered in the upcoming Intro to OpenTrons series.","title":"0. Fundamentals of Python"},{"location":"arcadia-users-group/20230228-intro-to-python-1/lesson/#01-jupyter-notebooks","text":"For the first workshop, we'll work in an interactive computing environment called a Jupyter notebook. To run a \"cell\" of code in Jupyter, you'll press Shift + Return from inside the cell. The result of the cell - the output - will display just below the cell. You can tell if a cell has successfully executed based on the [ ] text to the left of the cell. A cell that is in progress has an asterisk [*] , and a cell that has completed has a number [1] . For more on Jupyter notebooks, you can check out this AUG Lesson . Fun fact! The Python programming language is not named for snakes, but instead for Monty Python, a signifier of the idea that the language should be fun to use . Despite this, many Python tools and packages make reference to snakes, such as packages you may have heard us talk about before: Anaconda , conda , mamba , snakemake , etc.","title":"0.1 Jupyter Notebooks"},{"location":"arcadia-users-group/20230228-intro-to-python-1/lesson/#02-python-version","text":"This lesson is written for Python version 3 and above. You can figure out what version of Python you're running using the cell below. (We'll explain how the cell below works at the end of the lesson.) ! python -- version Python 3.11.0","title":"0.2 Python Version"},{"location":"arcadia-users-group/20230228-intro-to-python-1/lesson/#1-variables","text":"Variables in programming languages are containers for assigning values. Below, we assign the value 1 to the variable x and the value 4 to the variable y . x = 1 y = 4 When we ask Python to evalute x + y , it substitutes the value 1 for x and the value 4 for y . Running the cell below returns the output value of the expression x + y . x + y 5","title":"1. Variables"},{"location":"arcadia-users-group/20230228-intro-to-python-1/lesson/#2-data-types","text":"Variables in Python come in a variety of types, such as: - int or integer : whole numbers such as 1 , 2 , 3 , etc. - float or floating-point number : numbers including decimals, e.g. 1.03 . - str or string : chains of alphanumeric characters flanked by quotation marks, e.g. \"apple\" . Double-quotes are stylistically perferred, but single-quotes also work. If you need to include quotation marks in a string, you can use the other option to \"escape\" the quotation mark. Python is able to natively perform certain operations using these datatypes, such as basic addition + , subtraction - , multiplication * , division / , etc. 1 + 2 * 3.4 / 5 2.36 Python also has some helpful intuitive operations \u2013 for example, you can join two strings together with a + operator as follows: \"pine\" + \"apple\" 'pineapple' Certain operations are not supported by default in Python - for example, you can't add a str with an int . Running the code below will cause a TypeError . Python is quite helpful in explaining errors you might encounter. Read the error message below and see if it makes sense to you. In general, you should expect to encounter lots of these errors, or \"bugs\", when you start getting into programming. This is totally normal! If you're ever confused about the source of an error, websites such as StackOverflow are great places to look for answers. One skill in learning to programming is learning what part of an error message to Google. 3 + \"5\" --------------------------------------------------------------------------- TypeError Traceback (most recent call last) Cell In[17], line 1 ----> 1 3 + \"5\" TypeError: unsupported operand type(s) for +: 'int' and 'str' Fun Fact! The term \"bug\" predates modern computers, but there are real examples of actual insects interfering with the function of a computer, such as this \"bug\" found at Harvard in 1947 .","title":"2. Data types"},{"location":"arcadia-users-group/20230228-intro-to-python-1/lesson/#3-using-variables","text":"You assign variables in Python using the variable = value syntax. You can perform operations on a mix of hard-coded values and variables interchangeably. This is very useful when you expect to use a variable in many different places in your script - Python keeps track of its value so you don't have to. If you use variable names that correspond to the value the variable holds (think wind_mph for wind speed), this can also help make your code more readable for yourself and others. a = 1 b = 6 + a c = a * b a + b + c 15 You can assign a different value to a variable later in your code, which replaces the original value. This can be useful, but it's also important to be careful not to overwrite variables you want to keep! a = 4 a + b 11","title":"3. Using variables"},{"location":"arcadia-users-group/20230228-intro-to-python-1/lesson/#4-built-in-python-functions","text":"In addition to managing variables and values, Python also provides a variety of built-in functions . A function is a block of organized, reusable code that accomplishes a specific task. Functions are \"called\" by chaining the function name with parentheses () . Parameters of that function can be typed between the parentheses to tell the function what to do. For example, the print() function takes any value and prints it as output. print ( \"Hello, world!\" ) print ( a + b ) Hello, world! 11 We can call a function from within another function call. For example, the type() function gives the type of a Python value. print ( type ( 4 )) print ( type ( \"Hello, world!\" )) <class 'int'> <class 'str'>","title":"4. Built-in Python functions"},{"location":"arcadia-users-group/20230228-intro-to-python-1/lesson/#5-typecasting","text":"A bit earlier, we tried to evalute the expression 3 + '5' . This failed because we were trying to add an int and a str , which isn't natively supported operation with the + operand. If we wanted to actually get the sum of 3 + 5 in this case, but '5' is given as a str , we can actually force '5' to be an int instead using typecasting, as below: 3 + int ( \"5\" ) 8 You can see that now, instead of throwing a TypeError , the expression returns the expected value. We typecast in Python by calling our data type as a function, e.g. str() , int() , float() , etc. In some cases, you can't cast a value into another type; for example, running the code below throws a ValueError . int ( \"banana\" ) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) Cell In[24], line 1 ----> 1 int(\"banana\") ValueError: invalid literal for int() with base 10: 'banana' This example is probably pretty intuitive, as it's hard to imagine how you would turn the string 'banana' into an integer. But there are plenty of cases where typecasting won't work, so be aware that this isn't a perfect solution.","title":"5. Typecasting"},{"location":"arcadia-users-group/20230228-intro-to-python-1/lesson/#p1-practice","text":"Let's try some things with basic Python variables!","title":"P1. Practice"},{"location":"arcadia-users-group/20230228-intro-to-python-1/lesson/#practice-1","text":"What values do x and y have after each of the following statements? Practice 1 Answer x = 25, y does not yet exist x = 25, y = 6 x = 100.0, y = 6 x = 100.0, y = 10 x = 25 y = 6 x = x * 4.0 y = y + 4","title":"Practice 1"},{"location":"arcadia-users-group/20230228-intro-to-python-1/lesson/#practice-2","text":"What is the expected value of m at the end of the cell below? Practice 2 Answer This is a trick question! You might have expected the cell below to throw a TypeError because we're multiplying an int with a str . But `4 * '15'` is actually a special operation in Python; it's telling Python to repeat the string '15' four times. Therefore, the correct answer is '15151515' i = 3 j = \"5\" k = i * int ( j ) l = 4 m = l * str ( k )","title":"Practice 2"},{"location":"arcadia-users-group/20230228-intro-to-python-1/lesson/#6-defining-a-custom-function","text":"In addition to working with built-in functions, you can also define your own functions in Python. This is one of the most powerful ways to use the language, as it allows you to write one block of code which you can use repeatedly in your scripts. Let's write our first function, called hello_world() . To define a custom function, you can start by writing def , followed by the desired name of your function, and parentheses () . Finish the line with a colon : . def hello_world (): print ( \"Hello world!\" ) When you execute the cell above, nothing prints out. Why? The answer is that using the code above, you've only told Python that you're defining a function called hello_world() , which itself calls print() . To run the function, you have to call it using the parentheses operator, as below. hello_world () Hello world!","title":"6. Defining a custom function"},{"location":"arcadia-users-group/20230228-intro-to-python-1/lesson/#7-indentation-and-code-blocks","text":"You've defined a function above, but how does Python know when the function ends? Python uses indentation to signify the scope of a function. In the code block below, we define a function, then define a variable, and finally pass the variable to the function. The line break and lack of indentation after the end of the function's scope tells Python that the function is completed. Note: This indentation syntax is uncommon among programming languages \u2013 often, languages require specific characters to set the scope of the code. For example, in C (another programming language), you end every code line with a semicolon ; . def hello_name ( name ): return \"Hello \" + name + \"!\" my_name = \"Guido\" hello_name ( my_name ) 'Hello Guido!' You might notice some differences in the code above to the hello_world function we defined previously. Let's go over these differences. There is now a variable name in the parentheses of the hello_name definition. This is how we define arguments of functions, or their input parameters. For example, the print() function accepts a comma-separated list of values as its arguments. Arguments act as placeholders \u2013 in the above example, name has no explicit value. But any value or variable placed into hello_name gets treated as the name variable, which is then kept through the rest of the function. The function ends with a return statement instead of a print() call. Most Python functions should end with a return statement. This is the value that the function spits out when it finishes running. If a function lacks a return statement, it will instead return None , a special value of the type NoneType . An important thing to know is that the output of functions can themselves be assigned to variables, as exemplified below. Combining function calls with variable assignment is a powerful way of using Python functions to modify or generate new variables. sentence_1 = hello_name ( my_name ) sentence_2 = \"My name is Python!\" print ( sentence_1 , sentence_2 ) Hello Guido! My name is Python!","title":"7. Indentation and code blocks"},{"location":"arcadia-users-group/20230228-intro-to-python-1/lesson/#8-comments","text":"A crucial part of writing good code is being able to understand what it does later. For example, consider how mystifying it can be to try to read the code block below. Note: the ** operand is the exponential. def factor ( a , b , c ): step_one = - 1 * b step_q = 4 * a * c step_two_a = step_one + ( b ** 2 - step_q ) step_two_b = step_one - ( b ** 2 - step_q ) step_p = 2 * a step_three_a = step_two_a / step_p step_three_b = step_two_b / step_p result = step_three_a , step_three_b return result a = 1 b = 7 c = 3 factor ( a , b , c ) (15.0, -22.0) The function above is definitely an example of inefficiently written code, but let's put that aside for now. If you ended up writing the function above, it might be helpful to be able to know what the overall goal of the code is, and perhaps what each step is doing. This is where comments come to the rescue. Comments in Python begin on the left with a hash mark ( # ) and are ignored by the Python interpreter. Comments can also be written in blocks demarcated with triple-double-quotes ( \"\"\" ). You can even include a comment in the same line as functional code! Consider the code below with comments - how does the code feel with comments and additional line breaks included? # Applies the quadratic formula to find the intercepts of a quadratic equation of the format y = ax**2 + bx + c def factor ( a , b , c ): \"\"\" This is a block comment. The formula is: -b +/- sqrt(b**2 - 4ac) / 2a \"\"\" step_one = - 1 * b # evaluates -b step_q = 4 * a * c # evaluates 4ac step_two_a = step_one + ( b ** 2 - step_q ) # evaluates the numerator with addition step_two_b = step_one - ( b ** 2 - step_q ) # evaluates the numerator with subtraction step_p = 2 * a # evaluates the denominator step_three_a = step_two_a / step_p # gets the right intercept step_three_b = step_two_b / step_p # gets the left intercept result = step_three_a , step_three_b # combines the intercept into a tuple return result # Defines a, b, and c a = 1 b = 7 c = 3 # Run the actual function, returning result factor ( a , b , c ) (15.0, -22.0)","title":"8. Comments"},{"location":"arcadia-users-group/20230228-intro-to-python-1/lesson/#p2-practice","text":"Let's try writing some basic Python functions.","title":"P2. Practice"},{"location":"arcadia-users-group/20230228-intro-to-python-1/lesson/#practice-3","text":"Write a function called greetings() that takes two arguments: greeting and name and returns the string 'Greeting, Name!' Practice 3 Sample Answer def greetings(greeting, name): return greeting + ', ' + name + '!' ########################################### # Write your function in the space below. # ## Then run the cell to check your work. ## ########################################### greeting = \"Bonjour\" name = \"Jacques\" greetings ( greeting , name ) Bonjour, Jacques!","title":"Practice 3"},{"location":"arcadia-users-group/20230228-intro-to-python-1/lesson/#practice-4","text":"What happens if you pass the variable robot_name below to your greetings() function? How could you modify the function to avoid this error? Practice 4 Sample Answer def greetings(greeting, name): return greeting + ', ' + str(name) + '!' ############################################### # Write your new function in the space below. # #### Then run the cell to check your work. #### ############################################### robot_name = 120 greetings ( greeting , robot_name ) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) Cell In[34], line 12 1 ############################################### 2 # Write your new function in the space below. # 3 #### Then run the cell to check your work. #### (...) 7 8 ############################################### 10 robot_name = 120 ---> 12 greetings(greeting, robot_name) Cell In[40], line 6, in greetings(greeting, name) 5 def greetings(greeting, name): ----> 6 print(greeting + \", \" + name + \"!\") TypeError: can only concatenate str (not \"int\") to str","title":"Practice 4"},{"location":"arcadia-users-group/20230228-intro-to-python-1/lesson/#9-running-python-from-the-command-line","text":"In this lesson, we've taken advantage of the interactive computing experience offered by Jupyter notebooks. This computing environment is nice because it allows you to link input code to output data more easily. However, there are other ways to execute Python code. For example, you can write Python code into a text file ending with .py . We've included a script in the lesson folder called hello_world.py . Its contents are as follows: print ( \"Hello world!\" ) You can execute that script using the command line as follows: ! python hello_world . py Hello world! The cell above starts with a special Jupyter operator, the exclamation point ! , which passes code directly to a shell. This is the same as using python hello_world.py in your command line terminal. Code executed through a Python script works just the same as code executed in Jupyter, and comes with some advantages: - it's the most common way to use Python - it's easier to compartmentalize code that runs different functions - it doesn't requires setting up a Jupyter environment, which can be confusing In general, Jupyter notebooks are a helpful way to write exploratory code, while functions you've finalized and plan to use repeatedly are better off being stored in scripts.","title":"9. Running Python from the command line"},{"location":"arcadia-users-group/20230228-intro-to-python-1/lesson/#91-receiving-user-input-with-input","text":"The built-in function input() prompts the user for input and accepts the response. input() takes a str argument which can be used to describe what the function is trying to get from the user as input. This is one way to be able to interact with a Python script interactively. In the example below, the user is prompted to input a day of the week. weekday = input ( \"What day of the week is it?\" ) print ( \"It sounds like it's \" + weekday ) What day of the week is it? Wednesday It sounds like it's Wednesday You can use this function to prompt a user for input as part of a Python script. Try running the script hello_name.py from a Terminal to see how the input function could be used. The contents of the script are shown below. name = input ( \"What's your name?\" ) print ( \"Hello, \" + name + \"!\" )","title":"9.1 Receiving user input with input()"},{"location":"arcadia-users-group/20230228-intro-to-python-1/lesson/#p3-practice","text":"Let's try writing a Python script.","title":"P3. Practice"},{"location":"arcadia-users-group/20230228-intro-to-python-1/lesson/#practice-5","text":"Write a Python script in a file called greetings.py . The script should prompt the user for a greeting and a name as interactive input, and print {Greeting}, {Name}! You'll need to test the script using the terminal, in part because Jupyter doesn't reliably allow user input in output blocks.","title":"Practice 5."},{"location":"arcadia-users-group/20230228-intro-to-python-1/lesson/#10-problem-set","text":"To really get familiar with coding in Python (or any other language), the best way is to write a lot of code . We recommend working through a problem set to practice some more with coding in Python. The problem set will also cover some of the basics we covered in a bit more detail. We expect the problem set won't take longer than ~1-2 hrs to complete. You can find the problem set at this Google CoLab Document . CoLab works just like a Jupyter notebook, but it automatically saves your changes and keeps them as a cloud document. To get started on the problem set, make sure to first click \"File\" > \"Save a copy in Drive\" to make sure you don't lose your progress! We'll have AUG office hours next week for you to stop by and work on your problem set with other Arcadians as well as to work through any bugs in your code. If you're stumped, we also have an Answer Key that you can check. Feel free to ping the #software-questions Slack channel for anything that comes up in the meantime!","title":"10. Problem Set"},{"location":"arcadia-users-group/20230314-intro-to-python-2/lesson/","text":"Introduction to Python, Part 2 Follow along in Binder by clicking the badge below: 0. Loops and Conditionals This lesson is the second in a series of workshops that provide an introduction to programming using Python. This lesson builds on the foundation of lesson 1 and covers: - lists and dictionaries - methods - loops - booleans - conditional expressions 1. Data structures Often, when you're working with values in science, you're not just working with x = 1 and y = 2 , but instead with large amounts of data. Python offers a variety of data structures that aggregate data, including: list s: ordered arrays of values. dict or dictionaries: values ordered with key-value pairs. set s: like lists, but each value can only occur once. tuple s: like lists, but you can't modify the values. We'll first talk about list and dict structures, as they're the most common data structures. 1.1 Lists You can create a list in Python using bracket notation, such as below. lst = [ 1 , 2 , 3 , 4 , 5 ] To access a specific value in the list, such as 2 , you use the index , or the position of the value in the list, in bracket notation, as below. A key thing to remember in Python is that lists are indexed starting with the index [0] . So, if you wanted to get the value 2 from the list above, you would use the index [1] . lst [ 1 ] 2 Lists are powerful because you can do many different operations with them using built-in functionality. For example, you can slice lists to get only specific elements using the colon operator : . To get the first through third elements of a list, you would use the following notation: lst [ 0 : 3 ] [1, 2, 3] Note that, in the above example, the elements are extracted with a 'left-included, right-excluded' notation - the fourth element in the list, 4 , at index 3 , is not included. You can slice and index lists in some pretty interesting ways, such as in the examples below: lst = [ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 ] # The below expression is equivalent to lst[0:3]. # It gives you everything from the start of the list up to the ending address. print ( lst [: 3 ]) # The below expression gives you the last element in the list. print ( lst [ - 1 ]) # The below expression gives you the last two elements in the list. print ( lst [ - 2 :]) [0, 1, 2] 8 [7, 8] Note that accessing a slice of a list returns a list, whereas accessing a single element returns the value itself. 1.2 List Methods Many data types in Python have special functions built-in to their data type. These special functions are called methods and are accessed using a dot operator, or period ( . ). For example, you can add new elements to a list using the .append() method. fruit_trees = [ \"apple\" , \"orange\" , \"peach\" , \"pear\" , \"cherry\" ] print ( fruit_trees ) new_fruit = \"coconut\" fruit_trees . append ( new_fruit ) print ( fruit_trees ) ['apple', 'orange', 'peach', 'pear', 'cherry'] ['apple', 'orange', 'peach', 'pear', 'cherry', 'coconut'] There are many different list methods. You can take a look at some of them at the Python documentation page . 1.3 List indexing tricks You can actually apply list indexing to certain other data types, such as str . str objects can be treated like lists, where each character is an element of the list. For example: fruit = \"pineapples\" print ( fruit [ 0 : 4 ]) print ( fruit [ - 6 :]) pine apples 1.4 Dictionaries Dictionaries are another data structure in Python, which allows for assigning a key to each value in the structure. Dictionaries are created using specific curly brace {} notation as below: # Dictionaries are instantiated using curly braces. # Each key-value pair in the dictionary is assigned using the notation: ## 'key': 'value' # Entries are separated using commas. favorite_fruits = { \"Anabelle\" : \"peach\" , \"Becky\" : \"pear\" , \"Cole\" : \"coconut\" } print ( favorite_fruits ) {'Anabelle': 'peach', 'Becky': 'pear', 'Cole': 'coconut'} To access the value in a dictionary, you index by the key . This returns the value assigned to that key . favorite_fruits [ \"Anabelle\" ] 'peach' Sometimes you might want just the keys or the values in your dictionary. You can access those with the .keys() and .values() methods. villagers = favorite_fruits . keys () print ( villagers ) fruits = favorite_fruits . values () print ( fruits ) dict_keys(['Anabelle', 'Becky', 'Cole']) dict_values(['peach', 'pear', 'coconut']) These methods return special types of data structures: dict_keys and dict_values . It's important to note that these are not list s, and operations or methods that you can normally perform on lists might not work on these data structures. However, you can always use typecasting to coerce dict_keys and dict_values structures into list format. list ( villagers ) ['Anabelle', 'Becky', 'Cole'] P1. Practice Let's explore some of the methods available for different data types. Let's also practice using search engines to look for our functionality of interest. Practice 1 What do each of the methods below do? a) Try each method individually and determine what it does. b) Write down the type and expected value for each output variable at the end of this script. Practice 1 Answer a) What each method does: .split() : breaks a string, returning a list using a separator, in this case a space ( ' ' ) .upper() : capitalizes all alphabetic characters in the string .find() : returns the int address where the first instance of the provided pattern ( 'dog' ) appears .partition() : like split, only returns a tuple with the string before, the query ( 'fox' ), and the string after .remove() : removes the given element from a list in-place, returning None .copy() : creates a copy of the list, returning the copy .reverse() : reverses the order of the list in-place, returning None b) Expected values at the end of the script: output_1 = ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'dog.'] ; type: list output_2 = 'THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG.' ; type: str output_3 = 40 ; type: int output_4 = ('The quick brown ', 'fox', ' jumps over the lazy dog.') ; type: tuple output_5 = None ; type: NoneType output_6 = ['dog.', 'the', 'over', 'jumps', 'fox', 'brown', 'quick', 'The'] ; type: list output_7 = None ; type: NoneType my_sentence = \"The quick brown fox jumps over the lazy dog.\" output_1 = my_sentence . split ( ' ' ) output_2 = my_sentence . upper () output_3 = my_sentence . find ( 'dog' ) output_4 = my_sentence . partition ( 'fox' ) output_5 = output_1 . remove ( 'lazy' ) output_6 = output_1 . copy () output_7 = output_6 . reverse () .format() A very helpful method for str datatypes is the .format() method. This method allows you to replace a bracketed space or variable name into a string, as below. example_1 = \"The quick {} fox jumps over the {} dog.\" . format ( \"red\" , \"happy\" ) example_2 = \"The quick {0} fox jumps over the {1} dog.\" . format ( \"arctic\" , \"sleepy\" ) example_3 = \"The quick {foxtype} fox jumps over the {dogtype} dog.\" . format ( foxtype = \"silver\" , dogtype = \"lucky\" ) print ( example_1 , example_2 , example_3 , sep = ' \\n ' ) The quick red fox jumps over the happy dog. The quick arctic fox jumps over the sleepy dog. The quick silver fox jumps over the lucky dog. Practice 2 a) How can you use .format() to adjust the following text so that it displays the price in dollar notation (\\$1.00)?\" b) How could you convert this to instead display things in yen format (\u00a5100) where one dollar = 100 yen? Practice 2 Answer a) print(\"Apples cost ${price:.2f} each.\".format(price = cost)) b) print(\"Apples cost \u00a5{price} each.\".format(price = cost * 100)) cost = 1 print ( \"Apples cost {price} each.\" . format ( price = cost )) Apples cost 1 each. 2. Loops Often when working with scientific data, you might want to apply the same set of transformations to many different samples. You can repeat a process in Python and other programming languages using loops . 2.1 for loops A basic example of this approach is the for loop. Let's say you had a bunch of different samples with different ID numbers, and you wanted to create a sample name as a string for each. You could take the following approach and manually do that transformation: sample_ids = [ 100 , 231 , 572 ] sample_1 = \"sample_\" + str ( sample_ids [ 0 ]) sample_2 = \"sample_\" + str ( sample_ids [ 1 ]) sample_3 = \"sample_\" + str ( sample_ids [ 2 ]) print ( sample_1 , sample_2 , sample_3 ) sample_100 sample_231 sample_572 A faster way to do this would be to use a for loop. The for loop will repeat for every element in a list, as below: for i in [ 0 , 1 , 2 ]: print ( \"Ha\" ) Ha Ha Ha You can combine a for loop with your data structure to be able to iterate through your list, as follows: sample_ids = [ 100 , 231 , 572 ] sample_names = [] for i in [ 0 , 1 , 2 ]: sample_name = \"sample_\" + str ( sample_ids [ i ]) sample_names . append ( sample_name ) print ( sample_names ) ['sample_100', 'sample_231', 'sample_572'] The example code above differs from our first attempt in several ways. Let's walk through what each of these changes is doing: - We created an empty sample_names list. Rather than creating a new variable for each sample name we're creating, we can store those values in a list that mirrors our sample_ids list. We create this empty list so that we have somewhere to put the new strings we're generating. We add those new values to the empty list using the .append() method. We used a for loop to iterate through the original list. In the original example, we manually added the string 'sample_' and converted the int from sample_ids to a str . We accessed the values in sample_ids using the explicit index at each position, e.g. [0] , [1] , [2] . In the revised code, we instead use the for loop to do that tedious work for us. The loop assigns a variable i to a list of values, [0, 1, 2] . Every time the loop runs, it replaces the variable i in our code with one of the values in the list, in that exact order. Hopefully you can see from this example how loops can help us automate repetitive processes. This also helps our code become much more readable. 2.2 Implicit iteration In a lot of programming languages, looping requires you to create a separate list of elements that are used for iteration, as in the previous example. Python actually has a more helpful way of looping over elements - rather than creating a separate list, you can simply iterate over the list you care about. See the example below, where we want to print out the elements in the list vegetables : vegetables = [ \"pumpkin\" , \"potato\" , \"tomato\" ] for veg in vegetables : print ( veg ) pumpkin potato tomato In the case above, instead of an arbitrary list [0, 1, 2] , we use the specific vegetables list we'd like to iterate across. Instead of the variable i , we create a different variable, veg , which takes on the value of each element of vegetables in each iteration of the loop. When iterating in this way, it's usually helpful to have a more explicit name such as veg than an unintuitive name such as i , as it helps contextualize what you're doing. 2.3 Loops using dict s In addition to iterating through loops using list data structures, we can also use dict s. You can understand some of the behavior of how this iteration happens using the example below: favorite_vegetables = { \"Marko\" : \"pumpkin\" , \"Charlene\" : \"potato\" , \"Stu\" : \"tomato\" } for entry in favorite_vegetables : print ( entry ) Marko Charlene Stu You'll notice that iterating using a for loop across a dictionary iterates over the key s of that dictionary. If you wanted to access both the key and the value in the dictionary, you can use the .items() method of a dict data structure. for villager , veg in favorite_vegetables . items (): print ( villager + \"'s favorite vegetable is\" , veg + \".\" ) Marko's favorite vegetable is pumpkin. Charlene's favorite vegetable is potato. Stu's favorite vegetable is tomato. Rather than using a single variable such as entry , because .items() returns two values for every entry in the dictionary, we can unpack those paired values into two separate variables: villager , and veg . 2.4 while loops Another type of helpful loop in Python is the while loop. Unlike the for loop, which requires you to explicitly state what you're iterating over, a while loop continues as long as a conditional statement is True . For example: a = [ 0 , 1 , 2 , 3 , 4 ] i = 0 while a [ i ] < 3 : print ( a [ i ]) i += 1 # The += operator adds the value on the right and updates the original variable. 0 1 2 In the above example, there are a number of different things happening. Prior to the loop, we specify a variable i . At the end of each entry in the loop, we increment i by 1 in order to step through the list a . How do we know when to stop? Take a look at the while statement: while a [ i ] < 3 : This means that as long as the entry at index [i] in list [a] is less than 3, we continue through the loop, printing the value of a[i] . As soon as we arrive at i = 3 , at the top of the while loop, we get the value a[i] = 3 , which is NOT less than 3 . Therefore, the while loop terminates, and the value 3 is never printed. while loops can be very powerful for a class of Python functions that rely on recursion, but you can also easily end up writing functions that \"hang\", or run forever without stopping. P2. Practice Let's explore using loops. Practice 3 Write a function called fasta_renamer() which takes a list of filenames and renames them with the following specifications: - Converts all characters to lowercase. - Converts files ending with .fasta to .fa . - Optionally, adds a prefix of the user's choice to the start of every file name. - Returns a list of the new file names. Note: In this case, we're performing this operation on a list of file name strings, not actual files. In the next lesson, we'll talk about actually reading and writing files. Practice 3 Sample Answer def fasta_renamer(lst, prefix = ''): new_names = [] for name in lst: new_name = name.lower().replace( '.fasta', '.fa') new_name = prefix + new_name new_names.append(new_name) return new_names ########################################### # Write your function in the space below. # ## Then run the cell to check your work. ## ########################################### starting_names = [ 'Chlamy.fa' , 'ENSARG005.FASTA' , 'Homo_sapiens.FA' ] prefix = 'genome_' fasta_renamer ( starting_names , prefix ) ['genome_chlamy.fa', 'genome_ensarg005.fa', 'genome_homo_sapiens.fa'] 3. Booleans In the previous lesson, we covered some basic data types such as int , float , and str . In this lesson, we've also covered data structures such as list and dict . One major data type that we haven't yet discussed is the bool , or Boolean. bool values are True or False and are crucial to a lot of what gives programming languages power and flexibility to manage different types of scenarios. bool values also have their own specific operands which can be used to build logic into our code. You can see some examples in the code below. 3.1 Equal To check if two values are equivalent, you can use the == operand. This evaluates to True if the two values are equal and False if they are not. print ( 1 == 1 ) print ( 1 == '1' ) x = 'apple' print ( x == 'apple' ) True False True 3.2 Not Equal The opposite of the == operand is the != operand, which returns True if the values are NOT equal. print ( 1 != 1 ) print ( 1 != '1' ) False True 3.3. Greater Than or Less Than You can also perform comparison between two values using expressions such as > , < , >= , and <= . print ( 1 < 2 ) print ( 1 > 2 ) print ( 2 <= 2.0 ) True False True 3.4 and and or You can also chain together boolean expressions using the operands and and or . - The and operator only returns True if BOTH of the elements are True . - the or operator returns True if EITHER of the elements are True . print ( 1 == 1 and 2 == 2 ) print ( 1 == 1 or 2 == 3 ) True True 3.5 not The not operand gives the inverse of a boolean expression. print ( not ( 1 == 1 and 2 == 2 )) False 3.6 Checking membership with in The in operand searches for values in data structures such as list or dict . It can also be used to search for substrings within str . pasture = [ 'sheep' , 'sheep' , 'sheep' , 'wolf' , 'sheep' , 'sheep' ] print ( 'wolf' in pasture ) sentence = 'The quick brown fox jumped over the lazy dog.' print ( 'dog' in sentence ) print ( 'monkey' in sentence ) True True False These Boolean operations are crucial to the logic that underpins programming. They are also the backbone of conditional expressions, which allow us to have a Python script make decisions. 4. Conditional expressions When working with data, you don't always want to perform the same transformations to every feature of the data. Sometimes, you want to select only specific aspects of the data and manipulate those. Using boolean expressions, we can generate conditional code. For example, we could determine whether a value falls within a specific range using an if statement. 4.1. if this, else that We can use if statements to choose how Python will proceed, as in the example below. value = 10 if value <= 7 : print ( 'Value is less than or equal to 7.' ) elif value > 7 and value < 12 : print ( 'Value is between 7 and 12.' ) else : print ( 'Value is greater than 12.' ) Value is between 7 and 12. In the example above, you can see the three major types of conditional expressions associated with an if statement: - if : If this expression evaluates as True, do the thing in the first indented block. - elif : If the if statement is False, and this is True, do the thing in the second indented block. - else : If all other if and elif statements are false, do the thing in the last indented block. 4.2 Bringing it all together The above example might seem a bit silly, because we can already tell in advance where 10 is relative to 7 and 12 . But you could also write this code in a different way to be able to set the values of the high and low bar. value = 10 upper = 18 lower = 10 if value <= lower : print ( value , 'is less than or equal to' , str ( lower ) + '.' ) elif value > lower and value < upper : print ( value , 'is between' , str ( lower ), 'and' , str ( upper ) + '.' ) else : print ( value , 'is greater than' , str ( upper ) + '.' ) 10 is less than or equal to 10. You could make this code more reusable by turning it into a function. def goldilocks ( upper , lower , value ): if value <= lower : print ( value , 'is less than or equal to' , str ( lower ) + '.' ) elif value > lower and value < upper : print ( value , 'is between' , str ( lower ), 'and' , str ( upper ) + '.' ) else : print ( value , 'is greater than' , str ( upper ) + '.' ) return None value = 10 upper = 30 lower = 5 goldilocks ( upper , lower , value ) 10 is between 5 and 30. And, if you combined this function call with a for loop, you could easily apply the same function to a variety of different numbers. values = [ 5 , 6 , 10 , 30 , 60 , 1 ] upper = 20 lower = 7 for value in values : goldilocks ( upper , lower , value ) 5 is less than or equal to 7. 6 is less than or equal to 7. 10 is between 7 and 20. 30 is greater than 20. 60 is greater than 20. 1 is less than or equal to 7. 4.3 Doing nothing Sometimes, when combining a conditional and a loop, you want the loop to do nothing at all. In this specific situation, you can force a loop to continue . For example, let's say you had a list of names and wanted to add a number to only the ones beginning with 'C' and return those. names = [ 'Celine' , 'Britney' , 'Stefani' , 'Rina' , 'Charli' , 'Billie' ] output = [] for name in names : if name [ 0 ] == 'C' : output . append ( name + '_01' ) else : continue print ( output ) ['Celine_01', 'Charli_01'] 4.4 Other conditional statements There are other types of conditional statements in Python which you'll encounter less frequently, but can still be very helpful: try except finally : tries to do something, unless it returns an error. If an error occurs, runs the except statement. You can specify different outcomes for different error types. finally at the end of each attempt, do a thing. values = [ 'a' , 1 , 3 , '5' , 'q' , 3.0 ] for value in values : try : print ( value / 3 ) except TypeError : print ( \"Cannot divide - wrong value type.\" ) finally : print ( \"---\" ) Cannot divide - wrong value type. --- 0.3333333333333333 --- 1.0 --- Cannot divide - wrong value type. --- Cannot divide - wrong value type. --- 1.0 --- match case : works very similarly to an if - elif - else statement, but can be much easier to read when you have specific expected outcomes. See example here . NOTE: match - case statements are still very new to Python 3.10, so Jupyter doesn't have syntax highlighting for them yet. with open() : used for opening files. We'll cover this in lesson 3! P3. Practice Let's consider how we might use conditional expressions to help us navigate biological data. A common case would be grouping data starting with similar IDs into a dictionary structure. Practice 4 You have a list of filenames that you want to aggregate into a dictionary based on the IDs contained within them. Write a function called aggregate_files() with the following behavior: - The function accepts a list and groups elements of the list into a dictionary based on a prefix. - You can assume that each file starts with a prefix delimited by a _ . - The function should return a dictionary. - Each entry in the dictionary should be a list containing elements from the input list. - Hint: You could try using conditional expressions or the set datatype . Practice 4 Sample Answer def aggregate_files(file_list): output_dict = {} for file in file_list: prefix = file.split('_')[0] if prefix not in output_dict.keys(): output_dict[prefix] = [file] else: output_dict[prefix] = output_dict[prefix] + [file] return output_dict ########################################### # Write your function in the space below. # ## Then run the cell to check your work. ## ########################################### file_names = [ 'Chlamy_img001.tiff' , 'Chlamy_img002.tiff' , 'Chlamy_img003.tiff' , 'Chlamy_img004.tiff' , 'Chlamy_img005.tiff' , 'Colpoda_img001.tiff' , 'Colpoda_img002.tiff' , 'Colpoda_img003.tiff' , 'Bigelow_img001.tiff' , 'Bigelow_img002.tiff' , 'Bigelow_img003.tiff' , 'LEX_img001.tiff' ] aggregate_files ( file_names ) {'Chlamy': ['Chlamy_img001.tiff', 'Chlamy_img002.tiff', 'Chlamy_img003.tiff', 'Chlamy_img004.tiff', 'Chlamy_img005.tiff'], 'Colpoda': ['Colpoda_img001.tiff', 'Colpoda_img002.tiff', 'Colpoda_img003.tiff'], 'Bigelow': ['Bigelow_img001.tiff', 'Bigelow_img002.tiff', 'Bigelow_img003.tiff'], 'LEX': ['LEX_img001.tiff']} 5. Problem Set With a wealth of data types, functions, loops, and conditional expressions at your disposal, you can accomplish many different computations using Python. This week's problem set features several open-ended problems that you can solve in many different ways. Some will revisit and revise the code you wrote in Problem Set 1. Hopefully you'll be able to write some fun and interesting code and feel empowered by Python programming! You can find the problem set at this Google CoLab Document . CoLab works just like a Jupyter notebook, but it automatically saves your changes and keeps them as a cloud document. To get started on the problem set, make sure to first click \"File\" > \"Save a copy in Drive\" to make sure you don't lose your progress! We'll have AUG office hours next week for you to stop by and work on your problem set with other Arcadians as well as to work through any bugs in your code. If you're stumped, we also have an Answer Key that you can check. Feel free to ping the #software-questions Slack channel for anything that comes up in the meantime!","title":"Introduction to Python, Part 2"},{"location":"arcadia-users-group/20230314-intro-to-python-2/lesson/#introduction-to-python-part-2","text":"Follow along in Binder by clicking the badge below:","title":"Introduction to Python, Part 2"},{"location":"arcadia-users-group/20230314-intro-to-python-2/lesson/#0-loops-and-conditionals","text":"This lesson is the second in a series of workshops that provide an introduction to programming using Python. This lesson builds on the foundation of lesson 1 and covers: - lists and dictionaries - methods - loops - booleans - conditional expressions","title":"0. Loops and Conditionals"},{"location":"arcadia-users-group/20230314-intro-to-python-2/lesson/#1-data-structures","text":"Often, when you're working with values in science, you're not just working with x = 1 and y = 2 , but instead with large amounts of data. Python offers a variety of data structures that aggregate data, including: list s: ordered arrays of values. dict or dictionaries: values ordered with key-value pairs. set s: like lists, but each value can only occur once. tuple s: like lists, but you can't modify the values. We'll first talk about list and dict structures, as they're the most common data structures.","title":"1. Data structures"},{"location":"arcadia-users-group/20230314-intro-to-python-2/lesson/#11-lists","text":"You can create a list in Python using bracket notation, such as below. lst = [ 1 , 2 , 3 , 4 , 5 ] To access a specific value in the list, such as 2 , you use the index , or the position of the value in the list, in bracket notation, as below. A key thing to remember in Python is that lists are indexed starting with the index [0] . So, if you wanted to get the value 2 from the list above, you would use the index [1] . lst [ 1 ] 2 Lists are powerful because you can do many different operations with them using built-in functionality. For example, you can slice lists to get only specific elements using the colon operator : . To get the first through third elements of a list, you would use the following notation: lst [ 0 : 3 ] [1, 2, 3] Note that, in the above example, the elements are extracted with a 'left-included, right-excluded' notation - the fourth element in the list, 4 , at index 3 , is not included. You can slice and index lists in some pretty interesting ways, such as in the examples below: lst = [ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 ] # The below expression is equivalent to lst[0:3]. # It gives you everything from the start of the list up to the ending address. print ( lst [: 3 ]) # The below expression gives you the last element in the list. print ( lst [ - 1 ]) # The below expression gives you the last two elements in the list. print ( lst [ - 2 :]) [0, 1, 2] 8 [7, 8] Note that accessing a slice of a list returns a list, whereas accessing a single element returns the value itself.","title":"1.1 Lists"},{"location":"arcadia-users-group/20230314-intro-to-python-2/lesson/#12-list-methods","text":"Many data types in Python have special functions built-in to their data type. These special functions are called methods and are accessed using a dot operator, or period ( . ). For example, you can add new elements to a list using the .append() method. fruit_trees = [ \"apple\" , \"orange\" , \"peach\" , \"pear\" , \"cherry\" ] print ( fruit_trees ) new_fruit = \"coconut\" fruit_trees . append ( new_fruit ) print ( fruit_trees ) ['apple', 'orange', 'peach', 'pear', 'cherry'] ['apple', 'orange', 'peach', 'pear', 'cherry', 'coconut'] There are many different list methods. You can take a look at some of them at the Python documentation page .","title":"1.2 List Methods"},{"location":"arcadia-users-group/20230314-intro-to-python-2/lesson/#13-list-indexing-tricks","text":"You can actually apply list indexing to certain other data types, such as str . str objects can be treated like lists, where each character is an element of the list. For example: fruit = \"pineapples\" print ( fruit [ 0 : 4 ]) print ( fruit [ - 6 :]) pine apples","title":"1.3 List indexing tricks"},{"location":"arcadia-users-group/20230314-intro-to-python-2/lesson/#14-dictionaries","text":"Dictionaries are another data structure in Python, which allows for assigning a key to each value in the structure. Dictionaries are created using specific curly brace {} notation as below: # Dictionaries are instantiated using curly braces. # Each key-value pair in the dictionary is assigned using the notation: ## 'key': 'value' # Entries are separated using commas. favorite_fruits = { \"Anabelle\" : \"peach\" , \"Becky\" : \"pear\" , \"Cole\" : \"coconut\" } print ( favorite_fruits ) {'Anabelle': 'peach', 'Becky': 'pear', 'Cole': 'coconut'} To access the value in a dictionary, you index by the key . This returns the value assigned to that key . favorite_fruits [ \"Anabelle\" ] 'peach' Sometimes you might want just the keys or the values in your dictionary. You can access those with the .keys() and .values() methods. villagers = favorite_fruits . keys () print ( villagers ) fruits = favorite_fruits . values () print ( fruits ) dict_keys(['Anabelle', 'Becky', 'Cole']) dict_values(['peach', 'pear', 'coconut']) These methods return special types of data structures: dict_keys and dict_values . It's important to note that these are not list s, and operations or methods that you can normally perform on lists might not work on these data structures. However, you can always use typecasting to coerce dict_keys and dict_values structures into list format. list ( villagers ) ['Anabelle', 'Becky', 'Cole']","title":"1.4 Dictionaries"},{"location":"arcadia-users-group/20230314-intro-to-python-2/lesson/#p1-practice","text":"Let's explore some of the methods available for different data types. Let's also practice using search engines to look for our functionality of interest.","title":"P1. Practice"},{"location":"arcadia-users-group/20230314-intro-to-python-2/lesson/#practice-1","text":"What do each of the methods below do? a) Try each method individually and determine what it does. b) Write down the type and expected value for each output variable at the end of this script. Practice 1 Answer a) What each method does: .split() : breaks a string, returning a list using a separator, in this case a space ( ' ' ) .upper() : capitalizes all alphabetic characters in the string .find() : returns the int address where the first instance of the provided pattern ( 'dog' ) appears .partition() : like split, only returns a tuple with the string before, the query ( 'fox' ), and the string after .remove() : removes the given element from a list in-place, returning None .copy() : creates a copy of the list, returning the copy .reverse() : reverses the order of the list in-place, returning None b) Expected values at the end of the script: output_1 = ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'dog.'] ; type: list output_2 = 'THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG.' ; type: str output_3 = 40 ; type: int output_4 = ('The quick brown ', 'fox', ' jumps over the lazy dog.') ; type: tuple output_5 = None ; type: NoneType output_6 = ['dog.', 'the', 'over', 'jumps', 'fox', 'brown', 'quick', 'The'] ; type: list output_7 = None ; type: NoneType my_sentence = \"The quick brown fox jumps over the lazy dog.\" output_1 = my_sentence . split ( ' ' ) output_2 = my_sentence . upper () output_3 = my_sentence . find ( 'dog' ) output_4 = my_sentence . partition ( 'fox' ) output_5 = output_1 . remove ( 'lazy' ) output_6 = output_1 . copy () output_7 = output_6 . reverse ()","title":"Practice 1"},{"location":"arcadia-users-group/20230314-intro-to-python-2/lesson/#format","text":"A very helpful method for str datatypes is the .format() method. This method allows you to replace a bracketed space or variable name into a string, as below. example_1 = \"The quick {} fox jumps over the {} dog.\" . format ( \"red\" , \"happy\" ) example_2 = \"The quick {0} fox jumps over the {1} dog.\" . format ( \"arctic\" , \"sleepy\" ) example_3 = \"The quick {foxtype} fox jumps over the {dogtype} dog.\" . format ( foxtype = \"silver\" , dogtype = \"lucky\" ) print ( example_1 , example_2 , example_3 , sep = ' \\n ' ) The quick red fox jumps over the happy dog. The quick arctic fox jumps over the sleepy dog. The quick silver fox jumps over the lucky dog.","title":".format()"},{"location":"arcadia-users-group/20230314-intro-to-python-2/lesson/#practice-2","text":"a) How can you use .format() to adjust the following text so that it displays the price in dollar notation (\\$1.00)?\" b) How could you convert this to instead display things in yen format (\u00a5100) where one dollar = 100 yen? Practice 2 Answer a) print(\"Apples cost ${price:.2f} each.\".format(price = cost)) b) print(\"Apples cost \u00a5{price} each.\".format(price = cost * 100)) cost = 1 print ( \"Apples cost {price} each.\" . format ( price = cost )) Apples cost 1 each.","title":"Practice 2"},{"location":"arcadia-users-group/20230314-intro-to-python-2/lesson/#2-loops","text":"Often when working with scientific data, you might want to apply the same set of transformations to many different samples. You can repeat a process in Python and other programming languages using loops .","title":"2. Loops"},{"location":"arcadia-users-group/20230314-intro-to-python-2/lesson/#21-for-loops","text":"A basic example of this approach is the for loop. Let's say you had a bunch of different samples with different ID numbers, and you wanted to create a sample name as a string for each. You could take the following approach and manually do that transformation: sample_ids = [ 100 , 231 , 572 ] sample_1 = \"sample_\" + str ( sample_ids [ 0 ]) sample_2 = \"sample_\" + str ( sample_ids [ 1 ]) sample_3 = \"sample_\" + str ( sample_ids [ 2 ]) print ( sample_1 , sample_2 , sample_3 ) sample_100 sample_231 sample_572 A faster way to do this would be to use a for loop. The for loop will repeat for every element in a list, as below: for i in [ 0 , 1 , 2 ]: print ( \"Ha\" ) Ha Ha Ha You can combine a for loop with your data structure to be able to iterate through your list, as follows: sample_ids = [ 100 , 231 , 572 ] sample_names = [] for i in [ 0 , 1 , 2 ]: sample_name = \"sample_\" + str ( sample_ids [ i ]) sample_names . append ( sample_name ) print ( sample_names ) ['sample_100', 'sample_231', 'sample_572'] The example code above differs from our first attempt in several ways. Let's walk through what each of these changes is doing: - We created an empty sample_names list. Rather than creating a new variable for each sample name we're creating, we can store those values in a list that mirrors our sample_ids list. We create this empty list so that we have somewhere to put the new strings we're generating. We add those new values to the empty list using the .append() method. We used a for loop to iterate through the original list. In the original example, we manually added the string 'sample_' and converted the int from sample_ids to a str . We accessed the values in sample_ids using the explicit index at each position, e.g. [0] , [1] , [2] . In the revised code, we instead use the for loop to do that tedious work for us. The loop assigns a variable i to a list of values, [0, 1, 2] . Every time the loop runs, it replaces the variable i in our code with one of the values in the list, in that exact order. Hopefully you can see from this example how loops can help us automate repetitive processes. This also helps our code become much more readable.","title":"2.1 for loops"},{"location":"arcadia-users-group/20230314-intro-to-python-2/lesson/#22-implicit-iteration","text":"In a lot of programming languages, looping requires you to create a separate list of elements that are used for iteration, as in the previous example. Python actually has a more helpful way of looping over elements - rather than creating a separate list, you can simply iterate over the list you care about. See the example below, where we want to print out the elements in the list vegetables : vegetables = [ \"pumpkin\" , \"potato\" , \"tomato\" ] for veg in vegetables : print ( veg ) pumpkin potato tomato In the case above, instead of an arbitrary list [0, 1, 2] , we use the specific vegetables list we'd like to iterate across. Instead of the variable i , we create a different variable, veg , which takes on the value of each element of vegetables in each iteration of the loop. When iterating in this way, it's usually helpful to have a more explicit name such as veg than an unintuitive name such as i , as it helps contextualize what you're doing.","title":"2.2 Implicit iteration"},{"location":"arcadia-users-group/20230314-intro-to-python-2/lesson/#23-loops-using-dicts","text":"In addition to iterating through loops using list data structures, we can also use dict s. You can understand some of the behavior of how this iteration happens using the example below: favorite_vegetables = { \"Marko\" : \"pumpkin\" , \"Charlene\" : \"potato\" , \"Stu\" : \"tomato\" } for entry in favorite_vegetables : print ( entry ) Marko Charlene Stu You'll notice that iterating using a for loop across a dictionary iterates over the key s of that dictionary. If you wanted to access both the key and the value in the dictionary, you can use the .items() method of a dict data structure. for villager , veg in favorite_vegetables . items (): print ( villager + \"'s favorite vegetable is\" , veg + \".\" ) Marko's favorite vegetable is pumpkin. Charlene's favorite vegetable is potato. Stu's favorite vegetable is tomato. Rather than using a single variable such as entry , because .items() returns two values for every entry in the dictionary, we can unpack those paired values into two separate variables: villager , and veg .","title":"2.3 Loops using dicts"},{"location":"arcadia-users-group/20230314-intro-to-python-2/lesson/#24-while-loops","text":"Another type of helpful loop in Python is the while loop. Unlike the for loop, which requires you to explicitly state what you're iterating over, a while loop continues as long as a conditional statement is True . For example: a = [ 0 , 1 , 2 , 3 , 4 ] i = 0 while a [ i ] < 3 : print ( a [ i ]) i += 1 # The += operator adds the value on the right and updates the original variable. 0 1 2 In the above example, there are a number of different things happening. Prior to the loop, we specify a variable i . At the end of each entry in the loop, we increment i by 1 in order to step through the list a . How do we know when to stop? Take a look at the while statement: while a [ i ] < 3 : This means that as long as the entry at index [i] in list [a] is less than 3, we continue through the loop, printing the value of a[i] . As soon as we arrive at i = 3 , at the top of the while loop, we get the value a[i] = 3 , which is NOT less than 3 . Therefore, the while loop terminates, and the value 3 is never printed. while loops can be very powerful for a class of Python functions that rely on recursion, but you can also easily end up writing functions that \"hang\", or run forever without stopping.","title":"2.4 while loops"},{"location":"arcadia-users-group/20230314-intro-to-python-2/lesson/#p2-practice","text":"Let's explore using loops.","title":"P2. Practice"},{"location":"arcadia-users-group/20230314-intro-to-python-2/lesson/#practice-3","text":"Write a function called fasta_renamer() which takes a list of filenames and renames them with the following specifications: - Converts all characters to lowercase. - Converts files ending with .fasta to .fa . - Optionally, adds a prefix of the user's choice to the start of every file name. - Returns a list of the new file names. Note: In this case, we're performing this operation on a list of file name strings, not actual files. In the next lesson, we'll talk about actually reading and writing files. Practice 3 Sample Answer def fasta_renamer(lst, prefix = ''): new_names = [] for name in lst: new_name = name.lower().replace( '.fasta', '.fa') new_name = prefix + new_name new_names.append(new_name) return new_names ########################################### # Write your function in the space below. # ## Then run the cell to check your work. ## ########################################### starting_names = [ 'Chlamy.fa' , 'ENSARG005.FASTA' , 'Homo_sapiens.FA' ] prefix = 'genome_' fasta_renamer ( starting_names , prefix ) ['genome_chlamy.fa', 'genome_ensarg005.fa', 'genome_homo_sapiens.fa']","title":"Practice 3"},{"location":"arcadia-users-group/20230314-intro-to-python-2/lesson/#3-booleans","text":"In the previous lesson, we covered some basic data types such as int , float , and str . In this lesson, we've also covered data structures such as list and dict . One major data type that we haven't yet discussed is the bool , or Boolean. bool values are True or False and are crucial to a lot of what gives programming languages power and flexibility to manage different types of scenarios. bool values also have their own specific operands which can be used to build logic into our code. You can see some examples in the code below.","title":"3. Booleans"},{"location":"arcadia-users-group/20230314-intro-to-python-2/lesson/#31-equal","text":"To check if two values are equivalent, you can use the == operand. This evaluates to True if the two values are equal and False if they are not. print ( 1 == 1 ) print ( 1 == '1' ) x = 'apple' print ( x == 'apple' ) True False True","title":"3.1 Equal"},{"location":"arcadia-users-group/20230314-intro-to-python-2/lesson/#32-not-equal","text":"The opposite of the == operand is the != operand, which returns True if the values are NOT equal. print ( 1 != 1 ) print ( 1 != '1' ) False True","title":"3.2 Not Equal"},{"location":"arcadia-users-group/20230314-intro-to-python-2/lesson/#33-greater-than-or-less-than","text":"You can also perform comparison between two values using expressions such as > , < , >= , and <= . print ( 1 < 2 ) print ( 1 > 2 ) print ( 2 <= 2.0 ) True False True","title":"3.3. Greater Than or Less Than"},{"location":"arcadia-users-group/20230314-intro-to-python-2/lesson/#34-and-and-or","text":"You can also chain together boolean expressions using the operands and and or . - The and operator only returns True if BOTH of the elements are True . - the or operator returns True if EITHER of the elements are True . print ( 1 == 1 and 2 == 2 ) print ( 1 == 1 or 2 == 3 ) True True","title":"3.4 and and or"},{"location":"arcadia-users-group/20230314-intro-to-python-2/lesson/#35-not","text":"The not operand gives the inverse of a boolean expression. print ( not ( 1 == 1 and 2 == 2 )) False","title":"3.5 not"},{"location":"arcadia-users-group/20230314-intro-to-python-2/lesson/#36-checking-membership-with-in","text":"The in operand searches for values in data structures such as list or dict . It can also be used to search for substrings within str . pasture = [ 'sheep' , 'sheep' , 'sheep' , 'wolf' , 'sheep' , 'sheep' ] print ( 'wolf' in pasture ) sentence = 'The quick brown fox jumped over the lazy dog.' print ( 'dog' in sentence ) print ( 'monkey' in sentence ) True True False These Boolean operations are crucial to the logic that underpins programming. They are also the backbone of conditional expressions, which allow us to have a Python script make decisions.","title":"3.6 Checking membership with in"},{"location":"arcadia-users-group/20230314-intro-to-python-2/lesson/#4-conditional-expressions","text":"When working with data, you don't always want to perform the same transformations to every feature of the data. Sometimes, you want to select only specific aspects of the data and manipulate those. Using boolean expressions, we can generate conditional code. For example, we could determine whether a value falls within a specific range using an if statement.","title":"4. Conditional expressions"},{"location":"arcadia-users-group/20230314-intro-to-python-2/lesson/#41-if-this-else-that","text":"We can use if statements to choose how Python will proceed, as in the example below. value = 10 if value <= 7 : print ( 'Value is less than or equal to 7.' ) elif value > 7 and value < 12 : print ( 'Value is between 7 and 12.' ) else : print ( 'Value is greater than 12.' ) Value is between 7 and 12. In the example above, you can see the three major types of conditional expressions associated with an if statement: - if : If this expression evaluates as True, do the thing in the first indented block. - elif : If the if statement is False, and this is True, do the thing in the second indented block. - else : If all other if and elif statements are false, do the thing in the last indented block.","title":"4.1. if this, else that"},{"location":"arcadia-users-group/20230314-intro-to-python-2/lesson/#42-bringing-it-all-together","text":"The above example might seem a bit silly, because we can already tell in advance where 10 is relative to 7 and 12 . But you could also write this code in a different way to be able to set the values of the high and low bar. value = 10 upper = 18 lower = 10 if value <= lower : print ( value , 'is less than or equal to' , str ( lower ) + '.' ) elif value > lower and value < upper : print ( value , 'is between' , str ( lower ), 'and' , str ( upper ) + '.' ) else : print ( value , 'is greater than' , str ( upper ) + '.' ) 10 is less than or equal to 10. You could make this code more reusable by turning it into a function. def goldilocks ( upper , lower , value ): if value <= lower : print ( value , 'is less than or equal to' , str ( lower ) + '.' ) elif value > lower and value < upper : print ( value , 'is between' , str ( lower ), 'and' , str ( upper ) + '.' ) else : print ( value , 'is greater than' , str ( upper ) + '.' ) return None value = 10 upper = 30 lower = 5 goldilocks ( upper , lower , value ) 10 is between 5 and 30. And, if you combined this function call with a for loop, you could easily apply the same function to a variety of different numbers. values = [ 5 , 6 , 10 , 30 , 60 , 1 ] upper = 20 lower = 7 for value in values : goldilocks ( upper , lower , value ) 5 is less than or equal to 7. 6 is less than or equal to 7. 10 is between 7 and 20. 30 is greater than 20. 60 is greater than 20. 1 is less than or equal to 7.","title":"4.2 Bringing it all together"},{"location":"arcadia-users-group/20230314-intro-to-python-2/lesson/#43-doing-nothing","text":"Sometimes, when combining a conditional and a loop, you want the loop to do nothing at all. In this specific situation, you can force a loop to continue . For example, let's say you had a list of names and wanted to add a number to only the ones beginning with 'C' and return those. names = [ 'Celine' , 'Britney' , 'Stefani' , 'Rina' , 'Charli' , 'Billie' ] output = [] for name in names : if name [ 0 ] == 'C' : output . append ( name + '_01' ) else : continue print ( output ) ['Celine_01', 'Charli_01']","title":"4.3 Doing nothing"},{"location":"arcadia-users-group/20230314-intro-to-python-2/lesson/#44-other-conditional-statements","text":"There are other types of conditional statements in Python which you'll encounter less frequently, but can still be very helpful: try except finally : tries to do something, unless it returns an error. If an error occurs, runs the except statement. You can specify different outcomes for different error types. finally at the end of each attempt, do a thing. values = [ 'a' , 1 , 3 , '5' , 'q' , 3.0 ] for value in values : try : print ( value / 3 ) except TypeError : print ( \"Cannot divide - wrong value type.\" ) finally : print ( \"---\" ) Cannot divide - wrong value type. --- 0.3333333333333333 --- 1.0 --- Cannot divide - wrong value type. --- Cannot divide - wrong value type. --- 1.0 --- match case : works very similarly to an if - elif - else statement, but can be much easier to read when you have specific expected outcomes. See example here . NOTE: match - case statements are still very new to Python 3.10, so Jupyter doesn't have syntax highlighting for them yet. with open() : used for opening files. We'll cover this in lesson 3!","title":"4.4 Other conditional statements"},{"location":"arcadia-users-group/20230314-intro-to-python-2/lesson/#p3-practice","text":"Let's consider how we might use conditional expressions to help us navigate biological data. A common case would be grouping data starting with similar IDs into a dictionary structure.","title":"P3. Practice"},{"location":"arcadia-users-group/20230314-intro-to-python-2/lesson/#practice-4","text":"You have a list of filenames that you want to aggregate into a dictionary based on the IDs contained within them. Write a function called aggregate_files() with the following behavior: - The function accepts a list and groups elements of the list into a dictionary based on a prefix. - You can assume that each file starts with a prefix delimited by a _ . - The function should return a dictionary. - Each entry in the dictionary should be a list containing elements from the input list. - Hint: You could try using conditional expressions or the set datatype . Practice 4 Sample Answer def aggregate_files(file_list): output_dict = {} for file in file_list: prefix = file.split('_')[0] if prefix not in output_dict.keys(): output_dict[prefix] = [file] else: output_dict[prefix] = output_dict[prefix] + [file] return output_dict ########################################### # Write your function in the space below. # ## Then run the cell to check your work. ## ########################################### file_names = [ 'Chlamy_img001.tiff' , 'Chlamy_img002.tiff' , 'Chlamy_img003.tiff' , 'Chlamy_img004.tiff' , 'Chlamy_img005.tiff' , 'Colpoda_img001.tiff' , 'Colpoda_img002.tiff' , 'Colpoda_img003.tiff' , 'Bigelow_img001.tiff' , 'Bigelow_img002.tiff' , 'Bigelow_img003.tiff' , 'LEX_img001.tiff' ] aggregate_files ( file_names ) {'Chlamy': ['Chlamy_img001.tiff', 'Chlamy_img002.tiff', 'Chlamy_img003.tiff', 'Chlamy_img004.tiff', 'Chlamy_img005.tiff'], 'Colpoda': ['Colpoda_img001.tiff', 'Colpoda_img002.tiff', 'Colpoda_img003.tiff'], 'Bigelow': ['Bigelow_img001.tiff', 'Bigelow_img002.tiff', 'Bigelow_img003.tiff'], 'LEX': ['LEX_img001.tiff']}","title":"Practice 4"},{"location":"arcadia-users-group/20230314-intro-to-python-2/lesson/#5-problem-set","text":"With a wealth of data types, functions, loops, and conditional expressions at your disposal, you can accomplish many different computations using Python. This week's problem set features several open-ended problems that you can solve in many different ways. Some will revisit and revise the code you wrote in Problem Set 1. Hopefully you'll be able to write some fun and interesting code and feel empowered by Python programming! You can find the problem set at this Google CoLab Document . CoLab works just like a Jupyter notebook, but it automatically saves your changes and keeps them as a cloud document. To get started on the problem set, make sure to first click \"File\" > \"Save a copy in Drive\" to make sure you don't lose your progress! We'll have AUG office hours next week for you to stop by and work on your problem set with other Arcadians as well as to work through any bugs in your code. If you're stumped, we also have an Answer Key that you can check. Feel free to ping the #software-questions Slack channel for anything that comes up in the meantime!","title":"5. Problem Set"},{"location":"arcadia-users-group/20230328-intro-to-python-3/lesson/","text":"Introduction to Python, Part 3 Follow along in Binder by clicking the badge below: 0. Packages This lesson is the third in a series of workshops that provide an introduction to programming using Python. This lesson builds on the foundation of lessons 1 and 2 and covers: - packages (also known as libraries) - interfacing with files using os - running from the command line with subprocess - mathematical operations with math - file I/O (input/output) using open() - plotting using matplotlib - DataFrame operations using pandas 1. Built-in Packages Using the basics of Python we've learned so far, it's possible to perform many operations you might want to deal with in Python. In theory, you could probably write a script or function to do just about anything you wanted using just base Python language. However, that would probably not be an efficient use of your time! It turns out that there are many built-in packages that Python provides to do more complex things, built upon the Python language. 1.1 File handling with os Importing a package in Python uses the following syntax. Below, we import the os package. import os Let's explore some of the functions of the os package. The function os.getcwd() returns the current working directory your script or Jupyter notebook is located in. You might notice that we access this function using a dot . operator, much like we use when accessing the method of a data type, such as list.append() . os . getcwd () '/home/ec2-user/arcadia-computational-training/docs/arcadia-users-group/20230328-intro-to-python-3' Another helpful function is os.listdir() , which returns a list of all files in the current directory. os . listdir () ['.ipynb_checkpoints', 'newfile.txt', 'newfile2.txt', 'practice2.txt', 'test.csv', 'lesson_files', 'testfiles', 'cities.csv', 'hello.txt', 'lesson.ipynb', 'lesson.md', 'playground.ipynb'] You can find more information about the functions in os using Python's web documentation . Most of the functions in os are not things we'd need to use for biological data analysis. The most useful part of os for us is actually one of its modules . A module is a sub-collection of functions within a package. You can access modules in the same way you access the methods of a data type, using a dot . operator. A particularly useful module of os is os.path . You can find its web documentation here . For example, you can use the os.path.exists() function to see if a file exists at a specific location. os . path . exists ( 'hello.txt' ) True You can also use os.path to get the path to a specific file in both absolute and relative formats. # Prints the relative path to the file print ( os . path . relpath ( 'hello.txt' )) # Prints the absolute path to the file print ( os . path . abspath ( 'hello.txt' )) hello.txt /home/ec2-user/arcadia-computational-training/docs/arcadia-users-group/20230328-intro-to-python-3/hello.txt These functions can be particularly helpful when using Python to create or modify files used for bioinformatic analysis. 1.2 Interfacing with the command line using subprocess Many bioinformatic software packages are executed through the command line. For example, tblastn can be run using your terminal if you have it installed. Jupyter notebooks have certain \"magic\" functions of cells that can allow you to run specific command line functions directly through your notebook. For example, basic commands such as ls or pwd can be run when typed directly into a cell. ls cities.csv lesson.ipynb newfile.txt test.csv hello.txt lesson.md playground.ipynb \u001b[0m\u001b[01;34mtestfiles\u001b[0m/ \u001b[01;34mlesson_files\u001b[0m/ newfile2.txt practice2.txt pwd '/home/ec2-user/arcadia-computational-training/docs/arcadia-users-group/20230328-intro-to-python-3' However, not all command line functions can be accessed in this way. For example, you can't use the which utility using Jupyter magic - you'll get an error instead. which python Cell In[8], line 1 which python ^ SyntaxError: invalid syntax For commands that aren't compatible with this approach, you can use a different type of Jupyter magic syntax using the exclamation point operand ! . ! which python /home/ec2-user/miniconda3/envs/umap/bin/python This can be really nice when you plan on programming interactively. However, using the exclamation point Jupyter magic doesn't translate well if you were to convert your Jupyter notebook into a Python script. If you were to try to use the exclamation point operator in a script, you'd have to rewrite some of your code, which is never fun! Instead of relying Jupyter magic, we suggest using the package subprocess , which passes commands from Python into the shell. import subprocess subprocess . run ([ 'which' , 'python' ]) /home/ec2-user/miniconda3/envs/umap/bin/python CompletedProcess(args=['which', 'python'], returncode=0) The code above runs the same as !which python . subprocess.run passes each element of a list as an argument to the shell. It's unfortunately not as tidy looking, but it does allow for more robust code that can be converted to Python scripts without rewriting! 1.3 math ematics The built-in math package can perform some useful mathematical operations. import math # Take the natural logarithm of a number print ( math . log ( 2 )) # Take the square root of a number print ( math . sqrt ( 4 )) # Get the factorial of a number print ( math . factorial ( 3 )) 0.6931471805599453 2.0 6 In addition to its many functions, math also includes hard-coded values such as: print ( math . pi ) print ( math . e ) 3.141592653589793 2.718281828459045 Packages can thus be used to distribute specific values in addition to helpful functions! P1. Practice Let's try using some Python packages to do useful things. Practice 1 Using a for loop, create a directory called testfiles/ in the current directory and populate it with empty files numbered file1.txt to file5.txt Hint: The command-line tool touch creates an empty file. Its syntax is touch filename . Hint: You also might want to check out this list of os functions . Practice 1 Sample Answer import subprocess import os numbers = [1, 2, 3, 4, 5] destination = 'testfiles/' os.mkdir(destination) for number in numbers: filename = 'file' + str(number) + '.txt' subprocess.run(['touch', destination + filename]) *Example using os.path.join()* import subprocess import os numbers = [1, 2, 3, 4, 5] destination = 'testfiles/' os.mkdir(destination) for number in numbers: filename = 'file' + str(number) + '.txt' subprocess.run(['touch', os.path.join(destination, filename)]) ############################################### ##### Write your code in the space below. ##### #### Then run the cell to check your work. #### ############################################### # Prints 'True' if all files exist, else False. # This checker uses a special syntax called a List Comprehension. # You don't need to understand this yet, but if you're curious, feel free to ask! filenames = [ 'testfiles/file' + str ( num ) + '.txt' for num in [ 1 , 2 , 3 , 4 , 5 ]] print ( all ([ os . path . exists ( file ) for file in filenames ])) True 2. Opening files with open() You've created some empty files in the exercise above, but what if you actually wanted to open files? Python has a built-in function, open() , which allows for direct file handling. file = open ( 'hello.txt' ) The open() function doesn't print anything - it actually is a data type called _io.TextIOWrapper . type ( file ) _io.TextIOWrapper This particular data type has certain methods, such as .read() , which reads each element of the file character by character. file . read () 'Hello\\nworld\\nmy\\nname\\nis\\nGuido' You can see that calling .read() returns a str which is every character in the file in order. This would probably not be a practical way of dealing with a very large file, and is certainly quite a clunky interface. You can improve this slightly by using the .readlines() method. This splits the file into a list, using a linebreak. file = open ( 'hello.txt' ) file . readlines () ['Hello\\n', 'world\\n', 'my\\n', 'name\\n', 'is\\n', 'Guido'] Another somewhat annoying aspect of this approach is that you also always have to .close() a file when you're done with it, or you can run into all sorts of weird errors. For example, if we try to read the file again without closing it, we get an empty list. This is because we already read through all of the lines in the file, and Python has kept track of that and picked back up reading at the end of the file. file . readlines () [] To read the file in a different way, we have to .close() it and then open() it again. file . close () file = open ( 'hello.txt' ) print ( file . readlines ()) file . close () ['Hello\\n', 'world\\n', 'my\\n', 'name\\n', 'is\\n', 'Guido'] 2.1 Opening files more easily using with Because of these quirks of Python's file handling using open() , the preferred way of loading files is using a with statement. This statement works similarly to try - except - finally but is mostly used for file streaming. with open ( 'hello.txt' ) as file : output = file . readlines () print ( output ) ['Hello\\n', 'world\\n', 'my\\n', 'name\\n', 'is\\n', 'Guido'] The with statement is a special kind of statement that does some stuff in the background to automatically .close() your file when the code block completes. If you're curious, this tutorial gives some more explanation, but we won't go into the details here. 2.2 Writing to files We've seen how you can pull information out of an existing file and pass it as a string or list. But what if you have data that you've generated or loaded in Python and want to actually save that information to a file? You can also use with open() for this as well. my_text = \"Let's see if this works...\" with open ( 'newfile.txt' , 'w' ) as file : file . write ( my_text ) The difference between read() ing a file and write() ing a file comes from the 'w' argument we passed to the open() function. The open() function actually accepts two arguments: - file : the path to the file you're opening - mode : how you're opening the file. This is 'r' or read by default - if you want to be able to write to file, you use 'w' . To be able to both read and write , you use 'w+' . Let's check whether our file contains the expected text. less newfile . txt Let's see if this works... Using the Jupyter magic command less , we can preview the file we just created. This seems to have worked! What about if we wanted to write not just a single string, but a list of strings, separated each by a line break? my_text_list = [ 'Lorem' , 'ipsum' , 'dolor' , 'sit' , 'amet' ] with open ( 'newfile2.txt' , 'w' ) as file : file . write ( my_text_list ) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) Cell In[25], line 4 1 my_text_list = ['Lorem', 'ipsum', 'dolor', 'sit', 'amet'] 3 with open('newfile2.txt', 'w') as file: ----> 4 file.write(my_text_list) TypeError: write() argument must be str, not list You'll see that the code above returns a TypeError: write() argument must be str, not list . We can't pass the variable my_text_list to write() because it's not a list! Rather than explaining exactly how to solve this problem, why don't we practice figuring out how to solve the problem? P2. Practice Let's practice figuring out how to do something in Python! Practice 2 You want to be able to write a list to a file, with each element of the list on a different line in the file. Figure out how to write the list practice_list to a file called practice2.txt . You can already solve this problem with what we've learned, but there are many possible ways to deal with this issue. Try to figure out on your own how you might overcome this problem. Tip: Search engines such as Google are your friend in this situation! Try looking for help on websites like StackOverflow , W3Schools , or Real Python . A huge part of learning how to program is also learning how to search for the solutions to your own programming problems. Hint: Try simply searching for the error above ( TypeError: write() argument must be str, not list ) and see where that gets you. How might you modify this query to get closer to solving your problem? Practice 2 Sample Answer A Using file.writelines() : new_list = [] for word in practice_list: new_list.append(word + '\\n') with open('practice2.txt', 'w+') as file: file.writelines(new_list) Practice 2 Sample Answer B Using a for loop with file.write() : with open('practice2.txt', 'w+') as file: for word in practice_list: file.write(word) file.write('\\n') practice_list = [ 'We' , 'can' , 'solve' , 'Python' , 'problems' , 'ourselves' ] ############################################### ##### Write your code in the space below. ##### #### Then run the cell to check your work. #### ############################################### with open ( 'practice2.txt' , 'r' ) as file : print ( file . read ()) We can solve Python problems ourselves 3. Installed Packages One of the awesome things about Python is the large community of other programmers \u2013 software engineers, scientists, hobbyists - who are also building functions using Python. Many of these folks share their work through Python packages (also sometimes referred to as libraries ). Today we'll have a whirlwind tour about how to import and use some of the more commonly-used packages. We won't cover how to install these packages - many can be installed using mamba , which is our preferred way of managing software installation. Check out this AUG training session on conda and mamba for more. We'll be blazing through these examples, so don't worry if you don't understand all of the code in each cell - this is mostly to show you how many different kinds of things you can do with Python packages. 3.1 Plotting with matplotlib Presenting data in a visual format often helps us make decisions or communicate our results. A popular Python package for making plots is matplotlib . Below, we use the module matplotlib.pyplot to plot some data using x and y coordinates. import matplotlib.pyplot as plt x = [ 1 , 2 , 3.5 , 4.4 , 6 ] y = [ 5 , 4 , 4.1 , 3 , 3.8 ] plt . plot ( x , y ) plt . show () You'll notice that rather than simply using import matplotlib.pyplot , we added an as plt expression to the end of our import statement. Python allows you to import packages using aliases. This saves you from having to type out matplotlib.pyplot.plot() and shortens the expression to plt.plot() . Package documentation examples often give you a suggestion for how you might abbreviate packages for others to be able to understand. The base matplotlib package, for example, is often abbreviated as mpl . You'll see additional examples of this in the other examples we'll look at. 3.2 Modifying a matplotlib plot So, we've plotted a basic line plot. But what do the axes mean? And why is it so large - can we make it smaller? matplotlib is a popular package for plotting because you can customize essentially every single aspect of any plot you make. See the comments below for an example of a more customized plot. import matplotlib.pyplot as plt x = [ 1 , 2 , 3.5 , 4.4 , 6 ] y = [ 5 , 4 , 4.1 , 3 , 3.8 ] # You can establish a figure and set the figure size prior to plotting. plt . figure ( figsize = ( 3 , 2 )) # You can change the background color of your plot ax = plt . axes () ax . set_facecolor ( \"xkcd:midnight\" ) # matplotlib recognizes a variety of color names plt . plot ( x , y , c = 'c' , linestyle = '-' ) # You can change the color (c) to cyan using 'c'. You can also set a dashed ('-') linestyle. # If you call another plotting function without creating a new figure, you can plot again on the same plot. plt . scatter ( x , y , c = 'w' , marker = '*' , s = 50 ) # For a scatter plot, you can choose color (c), marker style, and marker size (s). # You can set the specific x and y limits of your plot plt . xlim ( 0.5 , 6.5 ) plt . ylim ( 2.5 , 5.5 ) # You can have matplotlib show a grid, and choose its color plt . grid ( c = 'b' ) # You can label the x and y axes of the plot plt . xlabel ( 'longitude' ) plt . ylabel ( 'latitude' ) # You can add a title to the plot plt . title ( 'Casseiopeia' ) plt . show () This example only scratches the barest surface of what you can do with plt . If people want to learn more, we can have future workshops that dive into deeper detail about how matplotlib works and what you can do with it. 3.3 Working with DataFrame s using pandas A popular package for dealing with tabular data like .tsv and .csv files is pandas . You can use pandas to load files and perform all manner of database manipulations using DataFrame data structures. import pandas as pd df = pd . read_csv ( 'cities.csv' ) # The display() function allows us to nicely view tables and other data in Jupyter display ( df ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } LatD LatM LatS NS LonD LonM LonS EW City State 0 41 5 59 N 80 39 0 W Youngstown OH 1 42 52 48 N 97 23 23 W Yankton SD 2 46 35 59 N 120 30 36 W Yakima WA 3 42 16 12 N 71 48 0 W Worcester MA 4 43 37 48 N 89 46 11 W WisconsinDells WI ... ... ... ... ... ... ... ... ... ... ... 123 39 31 12 N 119 48 35 W Reno NV 124 50 25 11 N 104 39 0 W Regina SA 125 40 10 48 N 122 14 23 W RedBluff CA 126 40 19 48 N 75 55 48 W Reading PA 127 41 9 35 N 81 14 23 W Ravenna OH 128 rows \u00d7 10 columns You can use pandas to do things like: - filter your data to get specific subsets - calculate summaries across specific axes - perform complex manipulations using aggregation # Select only rows where the column 'State' is equal to 'CA' ca_cities = df [ df [ 'State' ] == 'CA' ] display ( ca_cities ) # Summarize the number of cities per state cities_per_state = df . value_counts ( 'State' ) display ( cities_per_state [ 0 : 10 ]) # Display counts for the top 10 states # Get a table that lists the name of each city in each state cities_listed = df [[ 'State' , 'City' ]] . groupby ( 'State' ) . agg ( lambda x : [ i for i in x ]) display ( cities_listed . head ( 10 )) # Display the first 10 states .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } LatD LatM LatS NS LonD LonM LonS EW City State 18 41 25 11 N 122 23 23 W Weed CA 59 37 57 35 N 121 17 24 W Stockton CA 85 38 26 23 N 122 43 12 W SantaRosa CA 87 34 25 11 N 119 41 59 W SantaBarbara CA 88 33 45 35 N 117 52 12 W SantaAna CA 89 37 20 24 N 121 52 47 W SanJose CA 90 37 46 47 N 122 25 11 W SanFrancisco CA 92 32 42 35 N 117 9 0 W SanDiego CA 93 34 6 36 N 117 18 35 W SanBernardino CA 98 36 40 11 N 121 39 0 W Salinas CA 110 38 35 24 N 121 29 23 W Sacramento CA 125 40 10 48 N 122 14 23 W RedBluff CA State CA 12 TX 8 OH 6 WA 6 PA 6 NY 5 FL 5 WI 4 MO 4 GA 4 dtype: int64 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } City State AL [Tuscaloosa, Selma] AZ [Tucson] BC [Vancouver] CA [Weed, Stockton, SantaRosa, SantaBarbara, Sant... CO [Trinidad, Sterling, Salida] CT [Waterbury] DC [Washington] DE [Wilmington] FL [WestPalmBeach, Tampa, Tallahassee, Sarasota, ... GA [Waycross, Valdosta, Swainsboro, Savannah] As you might notice, pandas can do rather complex operations using relatively few commands. If you tried to manually build some of these functions, it could be quite challenging! Hopefully these examples give you a sense of what you can do using Python if you build your skills and become fluent in the language. 4. Next Steps We've gone through a whirlwind tour of Python over the last few weeks, but there are many more things to learn! If there's interest, we can cover additional topics in future Python workshops.","title":"Introduction to Python, Part 3"},{"location":"arcadia-users-group/20230328-intro-to-python-3/lesson/#introduction-to-python-part-3","text":"Follow along in Binder by clicking the badge below:","title":"Introduction to Python, Part 3"},{"location":"arcadia-users-group/20230328-intro-to-python-3/lesson/#0-packages","text":"This lesson is the third in a series of workshops that provide an introduction to programming using Python. This lesson builds on the foundation of lessons 1 and 2 and covers: - packages (also known as libraries) - interfacing with files using os - running from the command line with subprocess - mathematical operations with math - file I/O (input/output) using open() - plotting using matplotlib - DataFrame operations using pandas","title":"0. Packages"},{"location":"arcadia-users-group/20230328-intro-to-python-3/lesson/#1-built-in-packages","text":"Using the basics of Python we've learned so far, it's possible to perform many operations you might want to deal with in Python. In theory, you could probably write a script or function to do just about anything you wanted using just base Python language. However, that would probably not be an efficient use of your time! It turns out that there are many built-in packages that Python provides to do more complex things, built upon the Python language.","title":"1. Built-in Packages"},{"location":"arcadia-users-group/20230328-intro-to-python-3/lesson/#11-file-handling-with-os","text":"Importing a package in Python uses the following syntax. Below, we import the os package. import os Let's explore some of the functions of the os package. The function os.getcwd() returns the current working directory your script or Jupyter notebook is located in. You might notice that we access this function using a dot . operator, much like we use when accessing the method of a data type, such as list.append() . os . getcwd () '/home/ec2-user/arcadia-computational-training/docs/arcadia-users-group/20230328-intro-to-python-3' Another helpful function is os.listdir() , which returns a list of all files in the current directory. os . listdir () ['.ipynb_checkpoints', 'newfile.txt', 'newfile2.txt', 'practice2.txt', 'test.csv', 'lesson_files', 'testfiles', 'cities.csv', 'hello.txt', 'lesson.ipynb', 'lesson.md', 'playground.ipynb'] You can find more information about the functions in os using Python's web documentation . Most of the functions in os are not things we'd need to use for biological data analysis. The most useful part of os for us is actually one of its modules . A module is a sub-collection of functions within a package. You can access modules in the same way you access the methods of a data type, using a dot . operator. A particularly useful module of os is os.path . You can find its web documentation here . For example, you can use the os.path.exists() function to see if a file exists at a specific location. os . path . exists ( 'hello.txt' ) True You can also use os.path to get the path to a specific file in both absolute and relative formats. # Prints the relative path to the file print ( os . path . relpath ( 'hello.txt' )) # Prints the absolute path to the file print ( os . path . abspath ( 'hello.txt' )) hello.txt /home/ec2-user/arcadia-computational-training/docs/arcadia-users-group/20230328-intro-to-python-3/hello.txt These functions can be particularly helpful when using Python to create or modify files used for bioinformatic analysis.","title":"1.1 File handling with os"},{"location":"arcadia-users-group/20230328-intro-to-python-3/lesson/#12-interfacing-with-the-command-line-using-subprocess","text":"Many bioinformatic software packages are executed through the command line. For example, tblastn can be run using your terminal if you have it installed. Jupyter notebooks have certain \"magic\" functions of cells that can allow you to run specific command line functions directly through your notebook. For example, basic commands such as ls or pwd can be run when typed directly into a cell. ls cities.csv lesson.ipynb newfile.txt test.csv hello.txt lesson.md playground.ipynb \u001b[0m\u001b[01;34mtestfiles\u001b[0m/ \u001b[01;34mlesson_files\u001b[0m/ newfile2.txt practice2.txt pwd '/home/ec2-user/arcadia-computational-training/docs/arcadia-users-group/20230328-intro-to-python-3' However, not all command line functions can be accessed in this way. For example, you can't use the which utility using Jupyter magic - you'll get an error instead. which python Cell In[8], line 1 which python ^ SyntaxError: invalid syntax For commands that aren't compatible with this approach, you can use a different type of Jupyter magic syntax using the exclamation point operand ! . ! which python /home/ec2-user/miniconda3/envs/umap/bin/python This can be really nice when you plan on programming interactively. However, using the exclamation point Jupyter magic doesn't translate well if you were to convert your Jupyter notebook into a Python script. If you were to try to use the exclamation point operator in a script, you'd have to rewrite some of your code, which is never fun! Instead of relying Jupyter magic, we suggest using the package subprocess , which passes commands from Python into the shell. import subprocess subprocess . run ([ 'which' , 'python' ]) /home/ec2-user/miniconda3/envs/umap/bin/python CompletedProcess(args=['which', 'python'], returncode=0) The code above runs the same as !which python . subprocess.run passes each element of a list as an argument to the shell. It's unfortunately not as tidy looking, but it does allow for more robust code that can be converted to Python scripts without rewriting!","title":"1.2 Interfacing with the command line using subprocess"},{"location":"arcadia-users-group/20230328-intro-to-python-3/lesson/#13-math-ematics","text":"The built-in math package can perform some useful mathematical operations. import math # Take the natural logarithm of a number print ( math . log ( 2 )) # Take the square root of a number print ( math . sqrt ( 4 )) # Get the factorial of a number print ( math . factorial ( 3 )) 0.6931471805599453 2.0 6 In addition to its many functions, math also includes hard-coded values such as: print ( math . pi ) print ( math . e ) 3.141592653589793 2.718281828459045 Packages can thus be used to distribute specific values in addition to helpful functions!","title":"1.3 math ematics"},{"location":"arcadia-users-group/20230328-intro-to-python-3/lesson/#p1-practice","text":"Let's try using some Python packages to do useful things.","title":"P1. Practice"},{"location":"arcadia-users-group/20230328-intro-to-python-3/lesson/#practice-1","text":"Using a for loop, create a directory called testfiles/ in the current directory and populate it with empty files numbered file1.txt to file5.txt Hint: The command-line tool touch creates an empty file. Its syntax is touch filename . Hint: You also might want to check out this list of os functions . Practice 1 Sample Answer import subprocess import os numbers = [1, 2, 3, 4, 5] destination = 'testfiles/' os.mkdir(destination) for number in numbers: filename = 'file' + str(number) + '.txt' subprocess.run(['touch', destination + filename]) *Example using os.path.join()* import subprocess import os numbers = [1, 2, 3, 4, 5] destination = 'testfiles/' os.mkdir(destination) for number in numbers: filename = 'file' + str(number) + '.txt' subprocess.run(['touch', os.path.join(destination, filename)]) ############################################### ##### Write your code in the space below. ##### #### Then run the cell to check your work. #### ############################################### # Prints 'True' if all files exist, else False. # This checker uses a special syntax called a List Comprehension. # You don't need to understand this yet, but if you're curious, feel free to ask! filenames = [ 'testfiles/file' + str ( num ) + '.txt' for num in [ 1 , 2 , 3 , 4 , 5 ]] print ( all ([ os . path . exists ( file ) for file in filenames ])) True","title":"Practice 1"},{"location":"arcadia-users-group/20230328-intro-to-python-3/lesson/#2-opening-files-with-open","text":"You've created some empty files in the exercise above, but what if you actually wanted to open files? Python has a built-in function, open() , which allows for direct file handling. file = open ( 'hello.txt' ) The open() function doesn't print anything - it actually is a data type called _io.TextIOWrapper . type ( file ) _io.TextIOWrapper This particular data type has certain methods, such as .read() , which reads each element of the file character by character. file . read () 'Hello\\nworld\\nmy\\nname\\nis\\nGuido' You can see that calling .read() returns a str which is every character in the file in order. This would probably not be a practical way of dealing with a very large file, and is certainly quite a clunky interface. You can improve this slightly by using the .readlines() method. This splits the file into a list, using a linebreak. file = open ( 'hello.txt' ) file . readlines () ['Hello\\n', 'world\\n', 'my\\n', 'name\\n', 'is\\n', 'Guido'] Another somewhat annoying aspect of this approach is that you also always have to .close() a file when you're done with it, or you can run into all sorts of weird errors. For example, if we try to read the file again without closing it, we get an empty list. This is because we already read through all of the lines in the file, and Python has kept track of that and picked back up reading at the end of the file. file . readlines () [] To read the file in a different way, we have to .close() it and then open() it again. file . close () file = open ( 'hello.txt' ) print ( file . readlines ()) file . close () ['Hello\\n', 'world\\n', 'my\\n', 'name\\n', 'is\\n', 'Guido']","title":"2. Opening files with open()"},{"location":"arcadia-users-group/20230328-intro-to-python-3/lesson/#21-opening-files-more-easily-using-with","text":"Because of these quirks of Python's file handling using open() , the preferred way of loading files is using a with statement. This statement works similarly to try - except - finally but is mostly used for file streaming. with open ( 'hello.txt' ) as file : output = file . readlines () print ( output ) ['Hello\\n', 'world\\n', 'my\\n', 'name\\n', 'is\\n', 'Guido'] The with statement is a special kind of statement that does some stuff in the background to automatically .close() your file when the code block completes. If you're curious, this tutorial gives some more explanation, but we won't go into the details here.","title":"2.1 Opening files more easily using with"},{"location":"arcadia-users-group/20230328-intro-to-python-3/lesson/#22-writing-to-files","text":"We've seen how you can pull information out of an existing file and pass it as a string or list. But what if you have data that you've generated or loaded in Python and want to actually save that information to a file? You can also use with open() for this as well. my_text = \"Let's see if this works...\" with open ( 'newfile.txt' , 'w' ) as file : file . write ( my_text ) The difference between read() ing a file and write() ing a file comes from the 'w' argument we passed to the open() function. The open() function actually accepts two arguments: - file : the path to the file you're opening - mode : how you're opening the file. This is 'r' or read by default - if you want to be able to write to file, you use 'w' . To be able to both read and write , you use 'w+' . Let's check whether our file contains the expected text. less newfile . txt Let's see if this works... Using the Jupyter magic command less , we can preview the file we just created. This seems to have worked! What about if we wanted to write not just a single string, but a list of strings, separated each by a line break? my_text_list = [ 'Lorem' , 'ipsum' , 'dolor' , 'sit' , 'amet' ] with open ( 'newfile2.txt' , 'w' ) as file : file . write ( my_text_list ) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) Cell In[25], line 4 1 my_text_list = ['Lorem', 'ipsum', 'dolor', 'sit', 'amet'] 3 with open('newfile2.txt', 'w') as file: ----> 4 file.write(my_text_list) TypeError: write() argument must be str, not list You'll see that the code above returns a TypeError: write() argument must be str, not list . We can't pass the variable my_text_list to write() because it's not a list! Rather than explaining exactly how to solve this problem, why don't we practice figuring out how to solve the problem?","title":"2.2 Writing to files"},{"location":"arcadia-users-group/20230328-intro-to-python-3/lesson/#p2-practice","text":"Let's practice figuring out how to do something in Python!","title":"P2. Practice"},{"location":"arcadia-users-group/20230328-intro-to-python-3/lesson/#practice-2","text":"You want to be able to write a list to a file, with each element of the list on a different line in the file. Figure out how to write the list practice_list to a file called practice2.txt . You can already solve this problem with what we've learned, but there are many possible ways to deal with this issue. Try to figure out on your own how you might overcome this problem. Tip: Search engines such as Google are your friend in this situation! Try looking for help on websites like StackOverflow , W3Schools , or Real Python . A huge part of learning how to program is also learning how to search for the solutions to your own programming problems. Hint: Try simply searching for the error above ( TypeError: write() argument must be str, not list ) and see where that gets you. How might you modify this query to get closer to solving your problem? Practice 2 Sample Answer A Using file.writelines() : new_list = [] for word in practice_list: new_list.append(word + '\\n') with open('practice2.txt', 'w+') as file: file.writelines(new_list) Practice 2 Sample Answer B Using a for loop with file.write() : with open('practice2.txt', 'w+') as file: for word in practice_list: file.write(word) file.write('\\n') practice_list = [ 'We' , 'can' , 'solve' , 'Python' , 'problems' , 'ourselves' ] ############################################### ##### Write your code in the space below. ##### #### Then run the cell to check your work. #### ############################################### with open ( 'practice2.txt' , 'r' ) as file : print ( file . read ()) We can solve Python problems ourselves","title":"Practice 2"},{"location":"arcadia-users-group/20230328-intro-to-python-3/lesson/#3-installed-packages","text":"One of the awesome things about Python is the large community of other programmers \u2013 software engineers, scientists, hobbyists - who are also building functions using Python. Many of these folks share their work through Python packages (also sometimes referred to as libraries ). Today we'll have a whirlwind tour about how to import and use some of the more commonly-used packages. We won't cover how to install these packages - many can be installed using mamba , which is our preferred way of managing software installation. Check out this AUG training session on conda and mamba for more. We'll be blazing through these examples, so don't worry if you don't understand all of the code in each cell - this is mostly to show you how many different kinds of things you can do with Python packages.","title":"3. Installed Packages"},{"location":"arcadia-users-group/20230328-intro-to-python-3/lesson/#31-plotting-with-matplotlib","text":"Presenting data in a visual format often helps us make decisions or communicate our results. A popular Python package for making plots is matplotlib . Below, we use the module matplotlib.pyplot to plot some data using x and y coordinates. import matplotlib.pyplot as plt x = [ 1 , 2 , 3.5 , 4.4 , 6 ] y = [ 5 , 4 , 4.1 , 3 , 3.8 ] plt . plot ( x , y ) plt . show () You'll notice that rather than simply using import matplotlib.pyplot , we added an as plt expression to the end of our import statement. Python allows you to import packages using aliases. This saves you from having to type out matplotlib.pyplot.plot() and shortens the expression to plt.plot() . Package documentation examples often give you a suggestion for how you might abbreviate packages for others to be able to understand. The base matplotlib package, for example, is often abbreviated as mpl . You'll see additional examples of this in the other examples we'll look at.","title":"3.1 Plotting with matplotlib"},{"location":"arcadia-users-group/20230328-intro-to-python-3/lesson/#32-modifying-a-matplotlib-plot","text":"So, we've plotted a basic line plot. But what do the axes mean? And why is it so large - can we make it smaller? matplotlib is a popular package for plotting because you can customize essentially every single aspect of any plot you make. See the comments below for an example of a more customized plot. import matplotlib.pyplot as plt x = [ 1 , 2 , 3.5 , 4.4 , 6 ] y = [ 5 , 4 , 4.1 , 3 , 3.8 ] # You can establish a figure and set the figure size prior to plotting. plt . figure ( figsize = ( 3 , 2 )) # You can change the background color of your plot ax = plt . axes () ax . set_facecolor ( \"xkcd:midnight\" ) # matplotlib recognizes a variety of color names plt . plot ( x , y , c = 'c' , linestyle = '-' ) # You can change the color (c) to cyan using 'c'. You can also set a dashed ('-') linestyle. # If you call another plotting function without creating a new figure, you can plot again on the same plot. plt . scatter ( x , y , c = 'w' , marker = '*' , s = 50 ) # For a scatter plot, you can choose color (c), marker style, and marker size (s). # You can set the specific x and y limits of your plot plt . xlim ( 0.5 , 6.5 ) plt . ylim ( 2.5 , 5.5 ) # You can have matplotlib show a grid, and choose its color plt . grid ( c = 'b' ) # You can label the x and y axes of the plot plt . xlabel ( 'longitude' ) plt . ylabel ( 'latitude' ) # You can add a title to the plot plt . title ( 'Casseiopeia' ) plt . show () This example only scratches the barest surface of what you can do with plt . If people want to learn more, we can have future workshops that dive into deeper detail about how matplotlib works and what you can do with it.","title":"3.2 Modifying a matplotlib plot"},{"location":"arcadia-users-group/20230328-intro-to-python-3/lesson/#33-working-with-dataframes-using-pandas","text":"A popular package for dealing with tabular data like .tsv and .csv files is pandas . You can use pandas to load files and perform all manner of database manipulations using DataFrame data structures. import pandas as pd df = pd . read_csv ( 'cities.csv' ) # The display() function allows us to nicely view tables and other data in Jupyter display ( df ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } LatD LatM LatS NS LonD LonM LonS EW City State 0 41 5 59 N 80 39 0 W Youngstown OH 1 42 52 48 N 97 23 23 W Yankton SD 2 46 35 59 N 120 30 36 W Yakima WA 3 42 16 12 N 71 48 0 W Worcester MA 4 43 37 48 N 89 46 11 W WisconsinDells WI ... ... ... ... ... ... ... ... ... ... ... 123 39 31 12 N 119 48 35 W Reno NV 124 50 25 11 N 104 39 0 W Regina SA 125 40 10 48 N 122 14 23 W RedBluff CA 126 40 19 48 N 75 55 48 W Reading PA 127 41 9 35 N 81 14 23 W Ravenna OH 128 rows \u00d7 10 columns You can use pandas to do things like: - filter your data to get specific subsets - calculate summaries across specific axes - perform complex manipulations using aggregation # Select only rows where the column 'State' is equal to 'CA' ca_cities = df [ df [ 'State' ] == 'CA' ] display ( ca_cities ) # Summarize the number of cities per state cities_per_state = df . value_counts ( 'State' ) display ( cities_per_state [ 0 : 10 ]) # Display counts for the top 10 states # Get a table that lists the name of each city in each state cities_listed = df [[ 'State' , 'City' ]] . groupby ( 'State' ) . agg ( lambda x : [ i for i in x ]) display ( cities_listed . head ( 10 )) # Display the first 10 states .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } LatD LatM LatS NS LonD LonM LonS EW City State 18 41 25 11 N 122 23 23 W Weed CA 59 37 57 35 N 121 17 24 W Stockton CA 85 38 26 23 N 122 43 12 W SantaRosa CA 87 34 25 11 N 119 41 59 W SantaBarbara CA 88 33 45 35 N 117 52 12 W SantaAna CA 89 37 20 24 N 121 52 47 W SanJose CA 90 37 46 47 N 122 25 11 W SanFrancisco CA 92 32 42 35 N 117 9 0 W SanDiego CA 93 34 6 36 N 117 18 35 W SanBernardino CA 98 36 40 11 N 121 39 0 W Salinas CA 110 38 35 24 N 121 29 23 W Sacramento CA 125 40 10 48 N 122 14 23 W RedBluff CA State CA 12 TX 8 OH 6 WA 6 PA 6 NY 5 FL 5 WI 4 MO 4 GA 4 dtype: int64 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } City State AL [Tuscaloosa, Selma] AZ [Tucson] BC [Vancouver] CA [Weed, Stockton, SantaRosa, SantaBarbara, Sant... CO [Trinidad, Sterling, Salida] CT [Waterbury] DC [Washington] DE [Wilmington] FL [WestPalmBeach, Tampa, Tallahassee, Sarasota, ... GA [Waycross, Valdosta, Swainsboro, Savannah] As you might notice, pandas can do rather complex operations using relatively few commands. If you tried to manually build some of these functions, it could be quite challenging! Hopefully these examples give you a sense of what you can do using Python if you build your skills and become fluent in the language.","title":"3.3 Working with DataFrames using pandas"},{"location":"arcadia-users-group/20230328-intro-to-python-3/lesson/#4-next-steps","text":"We've gone through a whirlwind tour of Python over the last few weeks, but there are many more things to learn! If there's interest, we can cover additional topics in future Python workshops.","title":"4. Next Steps"},{"location":"arcadia-users-group/20230711-3d-printing/lesson/","text":"3D printing with stereolithography (SLA) Description: This 2-hour training session will introduce learners to the basics of 3D printing, specifically focusing on designing stamps to create microchambers for cells using Autodesk Fusion 360 software and utilizing the Form2 SLA printer from Formlabs for actual 3D printing. Learning objectives: By the end of this training session, learners will be able to: Use Autodesk Fusion 360 software to create designs for microchamber stamps. Understand how to translate their digital designs into physical 3D printed models. Operate the Form2 SLA printer and understand the basics of SLA resins. Access resources for further learning and exploration in the field of 3D printing. Preparation: Ensure all learners have Autodesk Fusion 360 software installed on their laptops. A trial version of Fusion 360 can be downloaded here . (8 minutes to download and setup) Ensure all students have PreForm software installed on their laptops. PreForm from Formlabs can be downloaded here. (Takes 1 minute to download. No login required) Students need a mouse with a middle-click button to use Autodesk Fusion 360 software. Set up the Form2 SLA printer and have it ready for demonstration. (Galo will do this) Arrange for various types of SLA resins for demonstration purposes. (Galo will do this) Prepare basic safety equipment (goggles, gloves) for handling SLA resins. (General lab PPE) Outline: Part 1: Design the object to print (60 minutes) Introduction to Autodesk Fusion 360 and its relevance to 3D printing (15 minutes) Demonstrating the process of designing microchamber stamps for confining cells in Fusion 360 (30 minutes) Guided practice: Learners work on their own designs under instructor supervision (15 minutes) Part 2: Print the design (60 minutes) Introduction to Form2 SLA printer and SLA printing technology (15 minutes) Demonstrating the process of preparing a design for print and starting the print process (20 minutes) Guided practice: Students prepare their designs for print and start the print process under instructor supervision (20 minutes) Q&A and wrap-up (5 minutes) Other resources: Software: Autodesk Fusion 360 : Main software for 3D design. Create and edit 3D models (.stl files) PreForm : Formlabs' software for preparing models for 3D printing. .stl file to Formlabs formatted file PrusaSlicer : Prusa\u2019s software for preparing models from 3D printing .stl file to G-code formatted file SLA Resins: Standard Grey Resin : Good for general prototyping. Clear resin: Good for transparent objects. Many others\u2026 Repositories: There are several repositories that host 3D printing models and designs relevant to biology and research. Some of these include: NIH 3D Print Exchange : A public website that enables users to share, download and edit 3D print files related to health and science. The repository hosts a wide range of files, from laboratory equipment to anatomical models and molecular structures. Thingiverse : While Thingiverse is a general 3D model repository, it contains many designs related to biology and research, including models of biological structures, lab equipment, and educational aids. Sketchfab : Sketchfab's science section contains many biological and scientific 3D models. Models can be downloaded and printed. GrabCAD : GrabCAD has a community section where users share their designs. Many users share designs related to science and biology. Embodi3D : This repository focuses on medical and anatomical models. While the focus is more clinical, it may be of interest to researchers working with human biology. Yeggi : Yeggi is a search engine for 3D printable models. By using specific search terms, you can find models related to biology and research. Other types of printers: FDM Printers : More common, uses a plastic filament. DLP Printers : Similar to SLA, but uses a digital light projector screen. SLS Printers : Uses lasers to sinter powdered material. FDM vs SLA printing FDM printers, like the Prusa printer at Arcadia, are based on 3-axis CNC machines common in the manufacturing industry. These printers extrude a plastic filament through a hot end, layer by layer building up material and printing your part. FDM printers are easy and inexpensive to use and have gained huge popularity. SLA printing, in contrast, uses light to polymerize a growing resin structure from a solution of monomers. This method is capable of printing intricate and complex designs, including high-resolution parts and shapes that cannot be achieved easily on an FDM print bed. For most users, resolution requirements will determine which printer is most appropriate. FDM printers are great for rapid prototyping, medium-sized parts, and basic 3D objects, but resolutions lower than 1 mm are difficult to achieve. SLA printers can print all the same parts as FDM printers, but with significantly increased resolution (down to 100 um) and freedom in the design process. FDM printers are somewhat limited in the resin properties, with most resins being a sort of hard, chemically weak plastic. Meanwhile, SLA printers can print a wide range of resins and elastomers with different physical and chemical properties. Finally, FDM printers are simpler to operate and do not involve reactive resins or solvents, while SLA printing is a more involved procedure with clear, albeit mild, chemical hazards. This lesson plan was constructed with assistance by GPT-4.","title":"3D printing with stereolithography (SLA)"},{"location":"arcadia-users-group/20230711-3d-printing/lesson/#3d-printing-with-stereolithography-sla","text":"","title":"3D printing with stereolithography (SLA)"},{"location":"arcadia-users-group/20230711-3d-printing/lesson/#description","text":"This 2-hour training session will introduce learners to the basics of 3D printing, specifically focusing on designing stamps to create microchambers for cells using Autodesk Fusion 360 software and utilizing the Form2 SLA printer from Formlabs for actual 3D printing.","title":"Description:"},{"location":"arcadia-users-group/20230711-3d-printing/lesson/#learning-objectives","text":"By the end of this training session, learners will be able to: Use Autodesk Fusion 360 software to create designs for microchamber stamps. Understand how to translate their digital designs into physical 3D printed models. Operate the Form2 SLA printer and understand the basics of SLA resins. Access resources for further learning and exploration in the field of 3D printing.","title":"Learning objectives:"},{"location":"arcadia-users-group/20230711-3d-printing/lesson/#preparation","text":"Ensure all learners have Autodesk Fusion 360 software installed on their laptops. A trial version of Fusion 360 can be downloaded here . (8 minutes to download and setup) Ensure all students have PreForm software installed on their laptops. PreForm from Formlabs can be downloaded here. (Takes 1 minute to download. No login required) Students need a mouse with a middle-click button to use Autodesk Fusion 360 software. Set up the Form2 SLA printer and have it ready for demonstration. (Galo will do this) Arrange for various types of SLA resins for demonstration purposes. (Galo will do this) Prepare basic safety equipment (goggles, gloves) for handling SLA resins. (General lab PPE)","title":"Preparation:"},{"location":"arcadia-users-group/20230711-3d-printing/lesson/#outline","text":"","title":"Outline:"},{"location":"arcadia-users-group/20230711-3d-printing/lesson/#part-1-design-the-object-to-print-60-minutes","text":"Introduction to Autodesk Fusion 360 and its relevance to 3D printing (15 minutes) Demonstrating the process of designing microchamber stamps for confining cells in Fusion 360 (30 minutes) Guided practice: Learners work on their own designs under instructor supervision (15 minutes)","title":"Part 1: Design the object to print (60 minutes)"},{"location":"arcadia-users-group/20230711-3d-printing/lesson/#part-2-print-the-design-60-minutes","text":"Introduction to Form2 SLA printer and SLA printing technology (15 minutes) Demonstrating the process of preparing a design for print and starting the print process (20 minutes) Guided practice: Students prepare their designs for print and start the print process under instructor supervision (20 minutes) Q&A and wrap-up (5 minutes)","title":"Part 2: Print the design (60 minutes)"},{"location":"arcadia-users-group/20230711-3d-printing/lesson/#other-resources","text":"","title":"Other resources:"},{"location":"arcadia-users-group/20230711-3d-printing/lesson/#software","text":"Autodesk Fusion 360 : Main software for 3D design. Create and edit 3D models (.stl files) PreForm : Formlabs' software for preparing models for 3D printing. .stl file to Formlabs formatted file PrusaSlicer : Prusa\u2019s software for preparing models from 3D printing .stl file to G-code formatted file","title":"Software:"},{"location":"arcadia-users-group/20230711-3d-printing/lesson/#sla-resins","text":"Standard Grey Resin : Good for general prototyping. Clear resin: Good for transparent objects. Many others\u2026","title":"SLA Resins:"},{"location":"arcadia-users-group/20230711-3d-printing/lesson/#repositories","text":"There are several repositories that host 3D printing models and designs relevant to biology and research. Some of these include: NIH 3D Print Exchange : A public website that enables users to share, download and edit 3D print files related to health and science. The repository hosts a wide range of files, from laboratory equipment to anatomical models and molecular structures. Thingiverse : While Thingiverse is a general 3D model repository, it contains many designs related to biology and research, including models of biological structures, lab equipment, and educational aids. Sketchfab : Sketchfab's science section contains many biological and scientific 3D models. Models can be downloaded and printed. GrabCAD : GrabCAD has a community section where users share their designs. Many users share designs related to science and biology. Embodi3D : This repository focuses on medical and anatomical models. While the focus is more clinical, it may be of interest to researchers working with human biology. Yeggi : Yeggi is a search engine for 3D printable models. By using specific search terms, you can find models related to biology and research.","title":"Repositories:"},{"location":"arcadia-users-group/20230711-3d-printing/lesson/#other-types-of-printers","text":"FDM Printers : More common, uses a plastic filament. DLP Printers : Similar to SLA, but uses a digital light projector screen. SLS Printers : Uses lasers to sinter powdered material.","title":"Other types of printers:"},{"location":"arcadia-users-group/20230711-3d-printing/lesson/#fdm-vs-sla-printing","text":"FDM printers, like the Prusa printer at Arcadia, are based on 3-axis CNC machines common in the manufacturing industry. These printers extrude a plastic filament through a hot end, layer by layer building up material and printing your part. FDM printers are easy and inexpensive to use and have gained huge popularity. SLA printing, in contrast, uses light to polymerize a growing resin structure from a solution of monomers. This method is capable of printing intricate and complex designs, including high-resolution parts and shapes that cannot be achieved easily on an FDM print bed. For most users, resolution requirements will determine which printer is most appropriate. FDM printers are great for rapid prototyping, medium-sized parts, and basic 3D objects, but resolutions lower than 1 mm are difficult to achieve. SLA printers can print all the same parts as FDM printers, but with significantly increased resolution (down to 100 um) and freedom in the design process. FDM printers are somewhat limited in the resin properties, with most resins being a sort of hard, chemically weak plastic. Meanwhile, SLA printers can print a wide range of resins and elastomers with different physical and chemical properties. Finally, FDM printers are simpler to operate and do not involve reactive resins or solvents, while SLA printing is a more involved procedure with clear, albeit mild, chemical hazards. This lesson plan was constructed with assistance by GPT-4.","title":"FDM vs SLA printing"},{"location":"arcadia-users-group/20231017-intro-to-ggplot2/","text":"The lesson.Rmd file is a valid R markdown file. Knitting it produces lesson.md and the aux files and plots.","title":"Index"},{"location":"arcadia-users-group/20231017-intro-to-ggplot2/lesson/","text":"Intro to R via ggplot2 Note that this lesson has been modified from The Carpentries alternative version of the R Ecology lesson . Parts are reproduced in full, but the major changes were included to shorten the lesson to 60 minutes. Introduction to R and RStudio What are R and RStudio? R refers to a programming language as well as the software that runs R code. RStudio is a software interface that can make it easier to write R scripts and interact with the R software. It's a very popular platform, and RStudio also maintains the tidyverse series of packages we will use in this lesson. Navigating RStudio We will use the RStudio integrated development environment (IDE) to write code into scripts, run code in R, navigate files on our computer, inspect objects we create in R, and look at the plots we make. RStudio has many other features that can help with things like version control, developing R packages, and writing Shiny apps, but we won't cover those in the workshop. In the above screenshot, we can see 4 \"panes\" in the default layout: Top-Left: the Source pane that displays scripts and other files. If you only have 3 panes, and the Console pane is in the top left, press Shift+Cmd+N (Mac) or Shift+Ctrl+N (Windows) to open a blank R script, which should make the Source pane appear. Top-Right: the Environment/History pane, which shows all the objects in your current R session (Environment) and your command history (History) there are some other tabs here, including Connections, Build, Tutorial, and possibly Git we won't cover any of the other tabs, but RStudio has lots of other useful features Bottom-Left: the Console pane, where you can interact directly with an R console, which interprets R commands and prints the results There are also tabs for Terminal and Jobs Bottom-Right: the Files/Plots/Help/Viewer pane to navigate files or view plots and help pages You can customize the layout of these panes, as well as many settings such as RStudio color scheme, font, and even keyboard shortcuts. You can access these settings by going to the menu bar, then clicking on Tools \u2192 Global Options. RStudio puts most of the things you need to work in R into a single window, and also includes features like keyboard shortcuts, autocompletion of code, and syntax highlighting (different types of code are colored differently, making it easier to navigate your code). Console vs. script You can run commands directly in the R console, or you can write them into an R script. It may help to think of working in the console vs. working in a script as something like cooking. The console is like making up a new recipe, but not writing anything down. You can carry out a series of steps and produce a nice, tasty dish at the end. However, because you didn't write anything down, it's harder to figure out exactly what you did, and in what order. Writing a script is like taking nice notes while cooking- you can tweak and edit the recipe all you want, you can come back in 6 months and try it again, and you don't have to try to remember what went well and what didn't. It's actually even easier than cooking, since you can hit one button and the computer \"cooks\" the whole recipe for you! An additional benefit of scripts is that you can leave comments for yourself or others to read. Lines that start with # are considered comments and will not be interpreted as R code. Working in R and RStudio The basis of programming is that we write down instructions for the computer to follow, and then we tell the computer to follow those instructions. We write these instructions in the form of code , which is a common language that is understood by the computer and humans (after some practice). We call these instructions commands , and we tell the computer to follow the instructions by running (also called executing ) the commands. Console The R console is where code is run/executed The prompt , which is the > symbol, is where you can type commands By pressing Enter , R will execute those commands and print the result. You can work here, and your history is saved in the History pane, but you can't access it in the future Script A script is a record of commands to send to R, preserved in a plain text file with a .R extension You can make a new R script by clicking File \u2192 New File \u2192 R Script , clicking the green + button in the top left corner of RStudio, or pressing Shift+Cmd+N (Mac) or Shift+Ctrl+N (Windows). It will be unsaved, and called \"Untitled1\" If you type out lines of R code in a script, you can send them to the R console to be evaluated Cmd+Enter (Mac) or Ctrl+Enter (Windows) will run the line of code that your cursor is on If you highlight multiple lines of code, you can run all of them by pressing Cmd+Enter (Mac) or Ctrl+Enter (Windows) By preserving commands in a script, you can edit and rerun them quickly, save them for later, and share them with others You can leave comments for yourself by starting a line with a # Data visualization with ggplot2 Getting started with ggplot2 and data We are going to be using functions from the ggplot2 package to create visualizations of data. Functions are predefined bits of code that automate sets of actions. R itself has many built-in functions, but we can access many more by loading other packages of functions and data into R. If you don't have a blank, untitled script open yet, go ahead and open one with Shift+Cmd+N (Mac) or Shift+Ctrl+N (Windows). First, we'll use the install.packages() function to install the ggplot2 package. install.packages(\"ggplot2\") This function installs the ggplot2 package onto your computer so that R can access it. To use it in our current session, we load the package using the library() function. library ( ggplot2 ) Next lesson we will learn how to read data from external files into R, but for now we are going to use a clean and ready-to-use dataset that is provided by the ratdat data package. While most packages exist to bring new functionality into R, others package data, and some do both. To make our dataset available, we need to install and load the ratdat package too. install.packages ( 'ratdat' ) library ( ratdat ) The ratdat package contains data from the Portal Project which is a long-term data set from Portal, Arizona, in the Chihuahuan desert. We will be using a data set called complete_old , which contains older years of survey data. Let's try to learn a little bit about the data. We can use the View() function to open an interactive viewer which behaves like a simplified version of a spreadsheet program. It's a handy function, but somewhat limited when trying to view large data sets. View(complete_old) If you hover over the tab for the interactive View() , you can click the \"x\" that appears, which will close the tab. We can find out more about the dataset by using the str() function to examine the str ucture of the data. str ( complete_old ) ## 'data.frame': 16878 obs. of 13 variables: ## $ record_id : int 1 2 3 4 5 6 7 8 9 10 ... ## $ month : int 7 7 7 7 7 7 7 7 7 7 ... ## $ day : int 16 16 16 16 16 16 16 16 16 16 ... ## $ year : int 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977 ... ## $ plot_id : int 2 3 2 7 3 1 2 1 1 6 ... ## $ species_id : chr \"NL\" \"NL\" \"DM\" \"DM\" ... ## $ sex : chr \"M\" \"M\" \"F\" \"M\" ... ## $ hindfoot_length: int 32 33 37 36 35 14 NA 37 34 20 ... ## $ weight : int NA NA NA NA NA NA NA NA NA NA ... ## $ genus : chr \"Neotoma\" \"Neotoma\" \"Dipodomys\" \"Dipodomys\" ... ## $ species : chr \"albigula\" \"albigula\" \"merriami\" \"merriami\" ... ## $ taxa : chr \"Rodent\" \"Rodent\" \"Rodent\" \"Rodent\" ... ## $ plot_type : chr \"Control\" \"Long-term Krat Exclosure\" \"Control\" \"Rodent Exclosure\" ... str() will tell us how many observations/rows (obs) and variables/columns we have, as well as some information about each of the variables. We see the name of a variable (such as year ), followed by the kind of variable ( int for integer, chr for character), and the first 10 entries in that variable. We will talk more about different data types and structures later on. Plotting with ggplot2 ggplot2 is a powerful package that allows you to create plots from tabular data (data in a table format with rows and columns). The gg in ggplot2 stands for \"grammar of graphics\", and the package uses consistent vocabulary to create plots of widely varying types. Therefore, we only need small changes to our code if the underlying data changes or we decide to make a box plot instead of a scatter plot. This approach helps you create publication-quality plots with minimal adjusting and tweaking. ggplot2 is part of the tidyverse series of packages, which tend to like data in the \"long\" or \"tidy\" format, which means each column represents a single variable, and each row represents a single observation. Well-structured data will save you lots of time making figures with ggplot2 . For now, we will use data that are already in this format. Later, we'll learn about other packages in tidyverse and how to use them to get data into this format. We start learning R by using ggplot2 because it relies on concepts that we will need when we talk about data transformation in the next lessons. ggplot plots are built step by step by adding new layers, which allows for extensive flexibility and customization of plots. To build a plot, we will use a basic template that can be used for different types of plots: ggplot ( data = < DATA > , mapping = aes ( < MAPPINGS > )) + < GEOM_FUNCTION > () We use the ggplot() function to create a plot. To tell it what data to use, we need to specify the data argument . An argument is an input that a function takes. You set arguments using the = sign. ggplot ( data = complete_old ) We get a blank plot because we haven't told ggplot() which variables we want to correspond to parts of the plot. We can specify the \"mapping\" of variables to plot elements, such as x/y coordinates, size, or shape, by using the aes() function. We'll also add a comment, which is any line starting with a # . It's a good idea to use comments to organize your code or clarify what you are doing. # adding a mapping to x and y axes ggplot ( data = complete_old , mapping = aes ( x = weight , y = hindfoot_length )) Now we've got a plot with x and y axes corresponding to variables from complete_old . However, we haven't specified how we want the data to be displayed. We do this using geom_ functions, which specify the type of geom etry we want, such as points, lines, or bars. We can add a geom_point() layer to our plot by using the + sign. We indent onto a new line to make it easier to read. The plus sign + at the end of the first line signifies that we are adding another layer to the plot. ggplot ( data = complete_old , mapping = aes ( x = weight , y = hindfoot_length )) + geom_point () ## Warning: Removed 3081 rows containing missing values (`geom_point()`). You may notice a warning that missing values were removed. If a variable necessary to make the plot is missing from a given row of data (in this case, hindfoot_length or weight ), it can't be plotted. ggplot2 uses a warning message to let us know that some rows couldn't be plotted. Warning messages are one of a few ways R will communicate with you. Warnings can be thought of as a \"heads up\". Nothing necessarily went wrong , but the author of that function wanted to draw your attention to something. In the above case, it's worth knowing that some of the rows of your data were not plotted because they had missing data. A more serious type of message is an error . Here's an example: ggplot ( data = complete_old , mapping = aes ( x = weight , y = hindfoot_length )) + geom_poit () As you can see, we only get the error message, with no plot, because something has actually gone wrong. This particular error message is fairly common, and it happened because we misspelled point as poit . Because there is no function named geom_poit() , R tells us it can't find a function with that name. Changing aesthetics ggplot plots are made layer by layer. We'll keep adding new layers to the plot we just made to make it look different. You may have noticed that parts of our scatter plot have many overlapping points, making it difficult to see all the data. We can adjust the transparency of the points using the alpha argument, which takes a value between 0 and 1: ggplot ( data = complete_old , mapping = aes ( x = weight , y = hindfoot_length )) + geom_point ( alpha = 0.2 ) We can also change the color of the points: ggplot ( data = complete_old , mapping = aes ( x = weight , y = hindfoot_length )) + geom_point ( alpha = 0.2 , color = \"blue\" ) When we add more information into the aes() function, we're changing the aesthetic of the plot. This information is linked directly to the data in the underlying data frame that we're plotting. Adding another variable Let's try coloring our points according to the sampling plot type (plot here refers to the physical area where rodents were sampled and has nothing to do with making graphs). Since we're now mapping a variable ( plot_type ) to a component of the ggplot2 plot ( color ), we need to put the argument inside aes() : ggplot ( data = complete_old , mapping = aes ( x = weight , y = hindfoot_length , color = plot_type )) + geom_point ( alpha = 0.2 ) Challenge 1: Modifying plots Part 1: Try modifying the plot so that the shape of the point varies by sex . You will set the shape the same way you set the color . Do you think this is a good way to represent sex with these data? Challenge solution ggplot ( data = complete_old , mapping = aes ( x = weight , y = hindfoot_length , shape = sex )) + geom_point ( alpha = 0.2 ) Part 2: Now try changing the plot so that the color of the points vary by year . Do you notice a difference in the color scale compared to changing color by plot type? Why do you think this happened? Challenge solution ggplot ( data = complete_old , mapping = aes ( x = weight , y = hindfoot_length , color = year )) + geom_point ( alpha = 0.2 ) - For Part 2, the color scale is different compared to using color=plot_type because plot_type and year are different variable types. plot_type is a categorical variable, so ggplot2 defaults to use a discrete color scale, whereas year is a numeric variable, so ggplot2 uses a continuous color scale. Changing scales The default discrete color scale isn't always ideal: it isn't friendly to viewers with colorblindness and it doesn't translate well to grayscale. However, ggplot2 comes with quite a few other color scales, including the fantastic viridis scales, which are designed to be colorblind and grayscale friendly. We can change scales by adding scale_ functions to our plots: ggplot ( data = complete_old , mapping = aes ( x = weight , y = hindfoot_length , color = plot_type )) + geom_point ( alpha = 0.2 ) + scale_color_viridis_d () Scales don't just apply to colors - any plot component that you put inside aes() can be modified with scale_ functions. Just as we modified the scale used to map plot_type to color , we can modify the way that weight is mapped to the x axis by using the scale_x_log10() function: ggplot ( data = complete_old , mapping = aes ( x = weight , y = hindfoot_length , color = plot_type )) + geom_point ( alpha = 0.2 ) + scale_x_log10 () Boxplot Let's try making a different type of plot altogether. We'll start off with our same basic building blocks using ggplot() and aes() . ggplot ( data = complete_old , mapping = aes ( x = plot_type , y = hindfoot_length )) This time, let's try making a boxplot, which will have plot_type on the x axis and hindfoot_length on the y axis. We can do this by adding geom_boxplot() to our ggplot() : ggplot ( data = complete_old , mapping = aes ( x = plot_type , y = hindfoot_length )) + geom_boxplot () ## Warning: Removed 2733 rows containing non-finite values (`stat_boxplot()`). Just as we colored the points before, we can color our boxplot by plot_type as well: ggplot ( data = complete_old , mapping = aes ( x = plot_type , y = hindfoot_length , color = plot_type )) + geom_boxplot () It looks like color has only affected the outlines of the boxplot, not the rectangular portions. This is because the color only impacts 1-dimensional parts of a ggplot : points and lines. To change the color of 2-dimensional parts of a plot, we use fill : ggplot ( data = complete_old , mapping = aes ( x = plot_type , y = hindfoot_length , fill = plot_type )) + geom_boxplot () Adding geoms One of the most powerful aspects of ggplot is the way we can add components to a plot in successive layers. While boxplots can be very useful for summarizing data, it is often helpful to show the raw data as well. With ggplot , we can easily add another geom_ to our plot to show the raw data. Let's add geom_point() to visualize the raw data. We will modify the alpha argument to help with overplotting. ggplot ( data = complete_old , mapping = aes ( x = plot_type , y = hindfoot_length )) + geom_boxplot () + geom_point ( alpha = 0.2 ) Uh oh... all our points for a given x axis category fall exactly on a line, which isn't very useful. We can shift to using geom_jitter() , which will add points with a bit of random noise added to the positions to prevent this from happening. ggplot ( data = complete_old , mapping = aes ( x = plot_type , y = hindfoot_length )) + geom_boxplot () + geom_jitter ( alpha = 0.2 ) You may have noticed that some of our data points are now appearing on our plot twice: the outliers are plotted as black points from geom_boxplot() , but they are also plotted with geom_jitter() . Since we don't want to represent these data multiple times in the same form (points), we can stop geom_boxplot() from plotting them. We do this by setting the outlier.shape argument to NA , which means the outliers don't have a shape to be plotted. ggplot ( data = complete_old , mapping = aes ( x = plot_type , y = hindfoot_length )) + geom_boxplot ( outlier.shape = NA ) + geom_jitter ( alpha = 0.2 ) Just as before, we can map plot_type to color by putting it inside aes() . ggplot ( data = complete_old , mapping = aes ( x = plot_type , y = hindfoot_length , color = plot_type )) + geom_boxplot ( outlier.shape = NA ) + geom_jitter ( alpha = 0.2 ) Notice that both the color of the points and the color of the boxplot lines changed. Any time we specify an aes() mapping inside our initial ggplot() function, that mapping will apply to all our geom s. If we want to limit the mapping to a single geom , we can put the mapping into the specific geom_ function, like this: ggplot ( data = complete_old , mapping = aes ( x = plot_type , y = hindfoot_length )) + geom_boxplot ( outlier.shape = NA ) + geom_jitter ( aes ( color = plot_type ), alpha = 0.2 ) Now our points are colored according to plot_type , but the boxplots are all the same color. One thing you might notice is that even with alpha = 0.2 , the points obscure parts of the boxplot. This is because the geom_point() layer comes after the geom_boxplot() layer, which means the points are plotted on top of the boxes. To put the boxplots on top, we switch the order of the layers: ggplot ( data = complete_old , mapping = aes ( x = plot_type , y = hindfoot_length )) + geom_jitter ( aes ( color = plot_type ), alpha = 0.2 ) + geom_boxplot ( outlier.shape = NA ) Now we have the opposite problem! The white fill of the boxplots completely obscures some of the points. To address this problem, we can remove the fill from the boxplots altogether, leaving only the black lines. To do this, we set fill to NA : ggplot ( data = complete_old , mapping = aes ( x = plot_type , y = hindfoot_length )) + geom_jitter ( aes ( color = plot_type ), alpha = 0.2 ) + geom_boxplot ( outlier.shape = NA , fill = NA ) Now we can see all the raw data and our boxplots on top. Challenge 2: Change geom s Violin plots are similar to boxplots- try making one using plot_type and hindfoot_length as the x and y variables. Remember that all geom functions start with geom_ , followed by the type of geom. This might also be a place to test your search engine skills. It is often useful to search for R package_name stuff you want to search . So for this example we might search for R ggplot2 violin plot . Challenge solution ggplot ( data = complete_old , mapping = aes ( x = plot_type , y = hindfoot_length , color = plot_type )) + geom_jitter ( alpha = 0.2 ) + geom_violin ( fill = \"white\" ) For an extra challenge , make the color of the points and outlines of the violins vary by plot_type , and set the fill of the violins to white. Try playing with the order of the layers to see what looks best. Challenge solution ggplot ( data = complete_old , mapping = aes ( x = plot_type , y = hindfoot_length , color = plot_type )) + geom_jitter ( alpha = 0.2 ) + geom_violin ( fill = \"white\" ) Changing themes So far we've been changing the appearance of parts of our plot related to our data and the geom_ functions, but we can also change many of the non-data components of our plot. At this point, we are pretty happy with the basic layout of our plot, so we can assign it to a plot to a named object . We do this using the assignment arrow <- . What we are doing here is taking the result of the code on the right side of the arrow, and assigning it to an object whose name is on the left side of the arrow. We will create an object called myplot . If you run the name of the ggplot2 object, it will show the plot, just like if you ran the code itself. myplot <- ggplot ( data = complete_old , mapping = aes ( x = plot_type , y = hindfoot_length )) + geom_jitter ( aes ( color = plot_type ), alpha = 0.2 ) + geom_boxplot ( outlier.shape = NA , fill = NA ) myplot ## Warning: Removed 2733 rows containing non-finite values (`stat_boxplot()`). ## Warning: Removed 2733 rows containing missing values (`geom_point()`). This process of assigning something to an object is not specific to ggplot2 , but rather a general feature of R. We will be using it a lot in the rest of this lesson. We can now work with the myplot object as if it was a block of ggplot2 code, which means we can use + to add new components to it. We can change the overall appearance using theme_ functions. Let's try a black-and-white theme by adding theme_bw() to our plot: myplot + theme_bw () As you can see, a number of parts of the plot have changed. theme_ functions usually control many aspects of a plot's appearance all at once, for the sake of convenience. To individually change parts of a plot, we can use the theme() function, which can take many different arguments to change things about the text, grid lines, background color, and more. Let's try changing the size of the text on our axis titles. We can do this by specifying that the axis.title should be an element_text() with size set to 14. myplot + theme_bw () + theme ( axis.title = element_text ( size = 14 )) Another change we might want to make is to remove the vertical grid lines. Since our x axis is categorical, those grid lines aren't useful. To do this, inside theme() , we will change the panel.grid.major.x to an element_blank() . myplot + theme_bw () + theme ( axis.title = element_text ( size = 14 ), panel.grid.major.x = element_blank ()) Another useful change might be to remove the color legend, since that information is already on our x axis. For this one, we will set legend.position to \"none\". myplot + theme_bw () + theme ( axis.title = element_text ( size = 14 ), panel.grid.major.x = element_blank (), legend.position = \"none\" ) Changing labels We probably want to make our axis titles nicer, and perhaps add a main title to the plot. We can do this using the labs() function: myplot + theme_bw () + theme ( axis.title = element_text ( size = 14 ), legend.position = \"none\" ) + labs ( title = \"Rodent size by plot type\" , x = \"Plot type\" , y = \"Hindfoot length (mm)\" ) We removed our legend from this plot, but you can also change the titles of various legends using labs() . For example, labs(color = \"Plot type\") would change the title of a color scale legend to \"Plot type\". Challenge 3: Customizing a plot Modify the previous plot by adding a descriptive subtitle. Increase the font size of the plot title and make it bold. Hint : \"bold\" is referred to as a font \"face.\" Challenge solution myplot + theme_bw () + theme ( axis.title = element_text ( size = 14 ), legend.position = \"none\" , plot.title = element_text ( face = \"bold\" , size = 20 )) + labs ( title = \"Rodent size by plot type\" , subtitle = \"Long-term dataset from Portal, AZ\" , x = \"Plot type\" , y = \"Hindfoot length (mm)\" ) Faceting One of the most powerful features of ggplot is the ability to quickly split a plot into multiple smaller plots based on a categorical variable, which is called faceting . So far we've mapped variables to the x axis, the y axis, and color, but trying to add a 4th variable becomes difficult. Changing the shape of a point might work, but only for very few categories, and even then, it can be hard to tell the differences between the shapes of small points. Instead of cramming one more variable into a single plot, we will use the facet_wrap() function to generate a series of smaller plots, split out by sex . We also use ncol to specify that we want them arranged in a single column: myplot + theme_bw () + theme ( axis.title = element_text ( size = 14 ), legend.position = \"none\" , panel.grid.major.x = element_blank ()) + labs ( title = \"Rodent size by plot type\" , x = \"Plot type\" , y = \"Hindfoot length (mm)\" , color = \"Plot type\" ) + facet_wrap ( vars ( sex ), ncol = 1 ) Faceting comes in handy in many scenarios. It can be useful when: a categorical variable has too many levels to differentiate by color (such as a dataset with 20 countries) your data overlap heavily, obscuring categories you want to show more than 3 variables at once you want to see each category in isolation while allowing for general comparisons between categories Exporting plots Once we are happy with our final plot, we can assign the whole thing to a new object, which we can call finalplot . finalplot <- myplot + theme_bw () + theme ( axis.title = element_text ( size = 14 ), legend.position = \"none\" , panel.grid.major.x = element_blank ()) + labs ( title = \"Rodent size by plot type\" , x = \"Plot type\" , y = \"Hindfoot length (mm)\" , color = \"Plot type\" ) + facet_wrap ( vars ( sex ), ncol = 1 ) After this, we can run ggsave() to save our plot. The first argument we give is the path to the file we want to save, including the correct file extension. This code will make an image called rodent_size_plots.jpg in the images/ folder of our current project. We are making a .jpg , but you can save .pdf , .tiff , and other file formats. Next, we tell it the name of the plot object we want to save. We can also specify things like the width and height of the plot in inches. ggsave ( filename = \"images/rodent_size_plots.jpg\" , plot = finalplot , height = 6 , width = 8 ) The final plot should look like this: ## Warning: Removed 2733 rows containing non-finite values (`stat_boxplot()`). ## Warning: Removed 2733 rows containing missing values (`geom_point()`). Challenge 4: Make your own plot Try making your own plot! You can run str(complete_old) or ?complete_old to explore variables you might use in your new plot. Feel free to use variables we have already seen, or some we haven't explored yet. Here are a couple ideas to get you started: make a histogram of one of the numeric variables try using a different color scale_ try changing the size of points or thickness of lines in a geom","title":"Intro to R via `ggplot2`"},{"location":"arcadia-users-group/20231017-intro-to-ggplot2/lesson/#intro-to-r-via-ggplot2","text":"Note that this lesson has been modified from The Carpentries alternative version of the R Ecology lesson . Parts are reproduced in full, but the major changes were included to shorten the lesson to 60 minutes.","title":"Intro to R via ggplot2"},{"location":"arcadia-users-group/20231017-intro-to-ggplot2/lesson/#introduction-to-r-and-rstudio","text":"","title":"Introduction to R and RStudio"},{"location":"arcadia-users-group/20231017-intro-to-ggplot2/lesson/#what-are-r-and-rstudio","text":"R refers to a programming language as well as the software that runs R code. RStudio is a software interface that can make it easier to write R scripts and interact with the R software. It's a very popular platform, and RStudio also maintains the tidyverse series of packages we will use in this lesson.","title":"What are R and RStudio?"},{"location":"arcadia-users-group/20231017-intro-to-ggplot2/lesson/#navigating-rstudio","text":"We will use the RStudio integrated development environment (IDE) to write code into scripts, run code in R, navigate files on our computer, inspect objects we create in R, and look at the plots we make. RStudio has many other features that can help with things like version control, developing R packages, and writing Shiny apps, but we won't cover those in the workshop. In the above screenshot, we can see 4 \"panes\" in the default layout: Top-Left: the Source pane that displays scripts and other files. If you only have 3 panes, and the Console pane is in the top left, press Shift+Cmd+N (Mac) or Shift+Ctrl+N (Windows) to open a blank R script, which should make the Source pane appear. Top-Right: the Environment/History pane, which shows all the objects in your current R session (Environment) and your command history (History) there are some other tabs here, including Connections, Build, Tutorial, and possibly Git we won't cover any of the other tabs, but RStudio has lots of other useful features Bottom-Left: the Console pane, where you can interact directly with an R console, which interprets R commands and prints the results There are also tabs for Terminal and Jobs Bottom-Right: the Files/Plots/Help/Viewer pane to navigate files or view plots and help pages You can customize the layout of these panes, as well as many settings such as RStudio color scheme, font, and even keyboard shortcuts. You can access these settings by going to the menu bar, then clicking on Tools \u2192 Global Options. RStudio puts most of the things you need to work in R into a single window, and also includes features like keyboard shortcuts, autocompletion of code, and syntax highlighting (different types of code are colored differently, making it easier to navigate your code).","title":"Navigating RStudio"},{"location":"arcadia-users-group/20231017-intro-to-ggplot2/lesson/#console-vs-script","text":"You can run commands directly in the R console, or you can write them into an R script. It may help to think of working in the console vs. working in a script as something like cooking. The console is like making up a new recipe, but not writing anything down. You can carry out a series of steps and produce a nice, tasty dish at the end. However, because you didn't write anything down, it's harder to figure out exactly what you did, and in what order. Writing a script is like taking nice notes while cooking- you can tweak and edit the recipe all you want, you can come back in 6 months and try it again, and you don't have to try to remember what went well and what didn't. It's actually even easier than cooking, since you can hit one button and the computer \"cooks\" the whole recipe for you! An additional benefit of scripts is that you can leave comments for yourself or others to read. Lines that start with # are considered comments and will not be interpreted as R code.","title":"Console vs. script"},{"location":"arcadia-users-group/20231017-intro-to-ggplot2/lesson/#working-in-r-and-rstudio","text":"The basis of programming is that we write down instructions for the computer to follow, and then we tell the computer to follow those instructions. We write these instructions in the form of code , which is a common language that is understood by the computer and humans (after some practice). We call these instructions commands , and we tell the computer to follow the instructions by running (also called executing ) the commands.","title":"Working in R and RStudio"},{"location":"arcadia-users-group/20231017-intro-to-ggplot2/lesson/#console","text":"The R console is where code is run/executed The prompt , which is the > symbol, is where you can type commands By pressing Enter , R will execute those commands and print the result. You can work here, and your history is saved in the History pane, but you can't access it in the future","title":"Console"},{"location":"arcadia-users-group/20231017-intro-to-ggplot2/lesson/#script","text":"A script is a record of commands to send to R, preserved in a plain text file with a .R extension You can make a new R script by clicking File \u2192 New File \u2192 R Script , clicking the green + button in the top left corner of RStudio, or pressing Shift+Cmd+N (Mac) or Shift+Ctrl+N (Windows). It will be unsaved, and called \"Untitled1\" If you type out lines of R code in a script, you can send them to the R console to be evaluated Cmd+Enter (Mac) or Ctrl+Enter (Windows) will run the line of code that your cursor is on If you highlight multiple lines of code, you can run all of them by pressing Cmd+Enter (Mac) or Ctrl+Enter (Windows) By preserving commands in a script, you can edit and rerun them quickly, save them for later, and share them with others You can leave comments for yourself by starting a line with a #","title":"Script"},{"location":"arcadia-users-group/20231017-intro-to-ggplot2/lesson/#data-visualization-with-ggplot2","text":"","title":"Data visualization with ggplot2"},{"location":"arcadia-users-group/20231017-intro-to-ggplot2/lesson/#getting-started-with-ggplot2-and-data","text":"We are going to be using functions from the ggplot2 package to create visualizations of data. Functions are predefined bits of code that automate sets of actions. R itself has many built-in functions, but we can access many more by loading other packages of functions and data into R. If you don't have a blank, untitled script open yet, go ahead and open one with Shift+Cmd+N (Mac) or Shift+Ctrl+N (Windows). First, we'll use the install.packages() function to install the ggplot2 package. install.packages(\"ggplot2\") This function installs the ggplot2 package onto your computer so that R can access it. To use it in our current session, we load the package using the library() function. library ( ggplot2 ) Next lesson we will learn how to read data from external files into R, but for now we are going to use a clean and ready-to-use dataset that is provided by the ratdat data package. While most packages exist to bring new functionality into R, others package data, and some do both. To make our dataset available, we need to install and load the ratdat package too. install.packages ( 'ratdat' ) library ( ratdat ) The ratdat package contains data from the Portal Project which is a long-term data set from Portal, Arizona, in the Chihuahuan desert. We will be using a data set called complete_old , which contains older years of survey data. Let's try to learn a little bit about the data. We can use the View() function to open an interactive viewer which behaves like a simplified version of a spreadsheet program. It's a handy function, but somewhat limited when trying to view large data sets. View(complete_old) If you hover over the tab for the interactive View() , you can click the \"x\" that appears, which will close the tab. We can find out more about the dataset by using the str() function to examine the str ucture of the data. str ( complete_old ) ## 'data.frame': 16878 obs. of 13 variables: ## $ record_id : int 1 2 3 4 5 6 7 8 9 10 ... ## $ month : int 7 7 7 7 7 7 7 7 7 7 ... ## $ day : int 16 16 16 16 16 16 16 16 16 16 ... ## $ year : int 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977 ... ## $ plot_id : int 2 3 2 7 3 1 2 1 1 6 ... ## $ species_id : chr \"NL\" \"NL\" \"DM\" \"DM\" ... ## $ sex : chr \"M\" \"M\" \"F\" \"M\" ... ## $ hindfoot_length: int 32 33 37 36 35 14 NA 37 34 20 ... ## $ weight : int NA NA NA NA NA NA NA NA NA NA ... ## $ genus : chr \"Neotoma\" \"Neotoma\" \"Dipodomys\" \"Dipodomys\" ... ## $ species : chr \"albigula\" \"albigula\" \"merriami\" \"merriami\" ... ## $ taxa : chr \"Rodent\" \"Rodent\" \"Rodent\" \"Rodent\" ... ## $ plot_type : chr \"Control\" \"Long-term Krat Exclosure\" \"Control\" \"Rodent Exclosure\" ... str() will tell us how many observations/rows (obs) and variables/columns we have, as well as some information about each of the variables. We see the name of a variable (such as year ), followed by the kind of variable ( int for integer, chr for character), and the first 10 entries in that variable. We will talk more about different data types and structures later on.","title":"Getting started with ggplot2 and data"},{"location":"arcadia-users-group/20231017-intro-to-ggplot2/lesson/#plotting-with-ggplot2","text":"ggplot2 is a powerful package that allows you to create plots from tabular data (data in a table format with rows and columns). The gg in ggplot2 stands for \"grammar of graphics\", and the package uses consistent vocabulary to create plots of widely varying types. Therefore, we only need small changes to our code if the underlying data changes or we decide to make a box plot instead of a scatter plot. This approach helps you create publication-quality plots with minimal adjusting and tweaking. ggplot2 is part of the tidyverse series of packages, which tend to like data in the \"long\" or \"tidy\" format, which means each column represents a single variable, and each row represents a single observation. Well-structured data will save you lots of time making figures with ggplot2 . For now, we will use data that are already in this format. Later, we'll learn about other packages in tidyverse and how to use them to get data into this format. We start learning R by using ggplot2 because it relies on concepts that we will need when we talk about data transformation in the next lessons. ggplot plots are built step by step by adding new layers, which allows for extensive flexibility and customization of plots. To build a plot, we will use a basic template that can be used for different types of plots: ggplot ( data = < DATA > , mapping = aes ( < MAPPINGS > )) + < GEOM_FUNCTION > () We use the ggplot() function to create a plot. To tell it what data to use, we need to specify the data argument . An argument is an input that a function takes. You set arguments using the = sign. ggplot ( data = complete_old ) We get a blank plot because we haven't told ggplot() which variables we want to correspond to parts of the plot. We can specify the \"mapping\" of variables to plot elements, such as x/y coordinates, size, or shape, by using the aes() function. We'll also add a comment, which is any line starting with a # . It's a good idea to use comments to organize your code or clarify what you are doing. # adding a mapping to x and y axes ggplot ( data = complete_old , mapping = aes ( x = weight , y = hindfoot_length )) Now we've got a plot with x and y axes corresponding to variables from complete_old . However, we haven't specified how we want the data to be displayed. We do this using geom_ functions, which specify the type of geom etry we want, such as points, lines, or bars. We can add a geom_point() layer to our plot by using the + sign. We indent onto a new line to make it easier to read. The plus sign + at the end of the first line signifies that we are adding another layer to the plot. ggplot ( data = complete_old , mapping = aes ( x = weight , y = hindfoot_length )) + geom_point () ## Warning: Removed 3081 rows containing missing values (`geom_point()`). You may notice a warning that missing values were removed. If a variable necessary to make the plot is missing from a given row of data (in this case, hindfoot_length or weight ), it can't be plotted. ggplot2 uses a warning message to let us know that some rows couldn't be plotted. Warning messages are one of a few ways R will communicate with you. Warnings can be thought of as a \"heads up\". Nothing necessarily went wrong , but the author of that function wanted to draw your attention to something. In the above case, it's worth knowing that some of the rows of your data were not plotted because they had missing data. A more serious type of message is an error . Here's an example: ggplot ( data = complete_old , mapping = aes ( x = weight , y = hindfoot_length )) + geom_poit () As you can see, we only get the error message, with no plot, because something has actually gone wrong. This particular error message is fairly common, and it happened because we misspelled point as poit . Because there is no function named geom_poit() , R tells us it can't find a function with that name.","title":"Plotting with ggplot2"},{"location":"arcadia-users-group/20231017-intro-to-ggplot2/lesson/#changing-aesthetics","text":"ggplot plots are made layer by layer. We'll keep adding new layers to the plot we just made to make it look different. You may have noticed that parts of our scatter plot have many overlapping points, making it difficult to see all the data. We can adjust the transparency of the points using the alpha argument, which takes a value between 0 and 1: ggplot ( data = complete_old , mapping = aes ( x = weight , y = hindfoot_length )) + geom_point ( alpha = 0.2 ) We can also change the color of the points: ggplot ( data = complete_old , mapping = aes ( x = weight , y = hindfoot_length )) + geom_point ( alpha = 0.2 , color = \"blue\" ) When we add more information into the aes() function, we're changing the aesthetic of the plot. This information is linked directly to the data in the underlying data frame that we're plotting.","title":"Changing aesthetics"},{"location":"arcadia-users-group/20231017-intro-to-ggplot2/lesson/#adding-another-variable","text":"Let's try coloring our points according to the sampling plot type (plot here refers to the physical area where rodents were sampled and has nothing to do with making graphs). Since we're now mapping a variable ( plot_type ) to a component of the ggplot2 plot ( color ), we need to put the argument inside aes() : ggplot ( data = complete_old , mapping = aes ( x = weight , y = hindfoot_length , color = plot_type )) + geom_point ( alpha = 0.2 ) Challenge 1: Modifying plots Part 1: Try modifying the plot so that the shape of the point varies by sex . You will set the shape the same way you set the color . Do you think this is a good way to represent sex with these data? Challenge solution ggplot ( data = complete_old , mapping = aes ( x = weight , y = hindfoot_length , shape = sex )) + geom_point ( alpha = 0.2 ) Part 2: Now try changing the plot so that the color of the points vary by year . Do you notice a difference in the color scale compared to changing color by plot type? Why do you think this happened? Challenge solution ggplot ( data = complete_old , mapping = aes ( x = weight , y = hindfoot_length , color = year )) + geom_point ( alpha = 0.2 ) - For Part 2, the color scale is different compared to using color=plot_type because plot_type and year are different variable types. plot_type is a categorical variable, so ggplot2 defaults to use a discrete color scale, whereas year is a numeric variable, so ggplot2 uses a continuous color scale.","title":"Adding another variable"},{"location":"arcadia-users-group/20231017-intro-to-ggplot2/lesson/#changing-scales","text":"The default discrete color scale isn't always ideal: it isn't friendly to viewers with colorblindness and it doesn't translate well to grayscale. However, ggplot2 comes with quite a few other color scales, including the fantastic viridis scales, which are designed to be colorblind and grayscale friendly. We can change scales by adding scale_ functions to our plots: ggplot ( data = complete_old , mapping = aes ( x = weight , y = hindfoot_length , color = plot_type )) + geom_point ( alpha = 0.2 ) + scale_color_viridis_d () Scales don't just apply to colors - any plot component that you put inside aes() can be modified with scale_ functions. Just as we modified the scale used to map plot_type to color , we can modify the way that weight is mapped to the x axis by using the scale_x_log10() function: ggplot ( data = complete_old , mapping = aes ( x = weight , y = hindfoot_length , color = plot_type )) + geom_point ( alpha = 0.2 ) + scale_x_log10 ()","title":"Changing scales"},{"location":"arcadia-users-group/20231017-intro-to-ggplot2/lesson/#boxplot","text":"Let's try making a different type of plot altogether. We'll start off with our same basic building blocks using ggplot() and aes() . ggplot ( data = complete_old , mapping = aes ( x = plot_type , y = hindfoot_length )) This time, let's try making a boxplot, which will have plot_type on the x axis and hindfoot_length on the y axis. We can do this by adding geom_boxplot() to our ggplot() : ggplot ( data = complete_old , mapping = aes ( x = plot_type , y = hindfoot_length )) + geom_boxplot () ## Warning: Removed 2733 rows containing non-finite values (`stat_boxplot()`). Just as we colored the points before, we can color our boxplot by plot_type as well: ggplot ( data = complete_old , mapping = aes ( x = plot_type , y = hindfoot_length , color = plot_type )) + geom_boxplot () It looks like color has only affected the outlines of the boxplot, not the rectangular portions. This is because the color only impacts 1-dimensional parts of a ggplot : points and lines. To change the color of 2-dimensional parts of a plot, we use fill : ggplot ( data = complete_old , mapping = aes ( x = plot_type , y = hindfoot_length , fill = plot_type )) + geom_boxplot ()","title":"Boxplot"},{"location":"arcadia-users-group/20231017-intro-to-ggplot2/lesson/#adding-geoms","text":"One of the most powerful aspects of ggplot is the way we can add components to a plot in successive layers. While boxplots can be very useful for summarizing data, it is often helpful to show the raw data as well. With ggplot , we can easily add another geom_ to our plot to show the raw data. Let's add geom_point() to visualize the raw data. We will modify the alpha argument to help with overplotting. ggplot ( data = complete_old , mapping = aes ( x = plot_type , y = hindfoot_length )) + geom_boxplot () + geom_point ( alpha = 0.2 ) Uh oh... all our points for a given x axis category fall exactly on a line, which isn't very useful. We can shift to using geom_jitter() , which will add points with a bit of random noise added to the positions to prevent this from happening. ggplot ( data = complete_old , mapping = aes ( x = plot_type , y = hindfoot_length )) + geom_boxplot () + geom_jitter ( alpha = 0.2 ) You may have noticed that some of our data points are now appearing on our plot twice: the outliers are plotted as black points from geom_boxplot() , but they are also plotted with geom_jitter() . Since we don't want to represent these data multiple times in the same form (points), we can stop geom_boxplot() from plotting them. We do this by setting the outlier.shape argument to NA , which means the outliers don't have a shape to be plotted. ggplot ( data = complete_old , mapping = aes ( x = plot_type , y = hindfoot_length )) + geom_boxplot ( outlier.shape = NA ) + geom_jitter ( alpha = 0.2 ) Just as before, we can map plot_type to color by putting it inside aes() . ggplot ( data = complete_old , mapping = aes ( x = plot_type , y = hindfoot_length , color = plot_type )) + geom_boxplot ( outlier.shape = NA ) + geom_jitter ( alpha = 0.2 ) Notice that both the color of the points and the color of the boxplot lines changed. Any time we specify an aes() mapping inside our initial ggplot() function, that mapping will apply to all our geom s. If we want to limit the mapping to a single geom , we can put the mapping into the specific geom_ function, like this: ggplot ( data = complete_old , mapping = aes ( x = plot_type , y = hindfoot_length )) + geom_boxplot ( outlier.shape = NA ) + geom_jitter ( aes ( color = plot_type ), alpha = 0.2 ) Now our points are colored according to plot_type , but the boxplots are all the same color. One thing you might notice is that even with alpha = 0.2 , the points obscure parts of the boxplot. This is because the geom_point() layer comes after the geom_boxplot() layer, which means the points are plotted on top of the boxes. To put the boxplots on top, we switch the order of the layers: ggplot ( data = complete_old , mapping = aes ( x = plot_type , y = hindfoot_length )) + geom_jitter ( aes ( color = plot_type ), alpha = 0.2 ) + geom_boxplot ( outlier.shape = NA ) Now we have the opposite problem! The white fill of the boxplots completely obscures some of the points. To address this problem, we can remove the fill from the boxplots altogether, leaving only the black lines. To do this, we set fill to NA : ggplot ( data = complete_old , mapping = aes ( x = plot_type , y = hindfoot_length )) + geom_jitter ( aes ( color = plot_type ), alpha = 0.2 ) + geom_boxplot ( outlier.shape = NA , fill = NA ) Now we can see all the raw data and our boxplots on top. Challenge 2: Change geom s Violin plots are similar to boxplots- try making one using plot_type and hindfoot_length as the x and y variables. Remember that all geom functions start with geom_ , followed by the type of geom. This might also be a place to test your search engine skills. It is often useful to search for R package_name stuff you want to search . So for this example we might search for R ggplot2 violin plot . Challenge solution ggplot ( data = complete_old , mapping = aes ( x = plot_type , y = hindfoot_length , color = plot_type )) + geom_jitter ( alpha = 0.2 ) + geom_violin ( fill = \"white\" ) For an extra challenge , make the color of the points and outlines of the violins vary by plot_type , and set the fill of the violins to white. Try playing with the order of the layers to see what looks best. Challenge solution ggplot ( data = complete_old , mapping = aes ( x = plot_type , y = hindfoot_length , color = plot_type )) + geom_jitter ( alpha = 0.2 ) + geom_violin ( fill = \"white\" )","title":"Adding geoms"},{"location":"arcadia-users-group/20231017-intro-to-ggplot2/lesson/#changing-themes","text":"So far we've been changing the appearance of parts of our plot related to our data and the geom_ functions, but we can also change many of the non-data components of our plot. At this point, we are pretty happy with the basic layout of our plot, so we can assign it to a plot to a named object . We do this using the assignment arrow <- . What we are doing here is taking the result of the code on the right side of the arrow, and assigning it to an object whose name is on the left side of the arrow. We will create an object called myplot . If you run the name of the ggplot2 object, it will show the plot, just like if you ran the code itself. myplot <- ggplot ( data = complete_old , mapping = aes ( x = plot_type , y = hindfoot_length )) + geom_jitter ( aes ( color = plot_type ), alpha = 0.2 ) + geom_boxplot ( outlier.shape = NA , fill = NA ) myplot ## Warning: Removed 2733 rows containing non-finite values (`stat_boxplot()`). ## Warning: Removed 2733 rows containing missing values (`geom_point()`). This process of assigning something to an object is not specific to ggplot2 , but rather a general feature of R. We will be using it a lot in the rest of this lesson. We can now work with the myplot object as if it was a block of ggplot2 code, which means we can use + to add new components to it. We can change the overall appearance using theme_ functions. Let's try a black-and-white theme by adding theme_bw() to our plot: myplot + theme_bw () As you can see, a number of parts of the plot have changed. theme_ functions usually control many aspects of a plot's appearance all at once, for the sake of convenience. To individually change parts of a plot, we can use the theme() function, which can take many different arguments to change things about the text, grid lines, background color, and more. Let's try changing the size of the text on our axis titles. We can do this by specifying that the axis.title should be an element_text() with size set to 14. myplot + theme_bw () + theme ( axis.title = element_text ( size = 14 )) Another change we might want to make is to remove the vertical grid lines. Since our x axis is categorical, those grid lines aren't useful. To do this, inside theme() , we will change the panel.grid.major.x to an element_blank() . myplot + theme_bw () + theme ( axis.title = element_text ( size = 14 ), panel.grid.major.x = element_blank ()) Another useful change might be to remove the color legend, since that information is already on our x axis. For this one, we will set legend.position to \"none\". myplot + theme_bw () + theme ( axis.title = element_text ( size = 14 ), panel.grid.major.x = element_blank (), legend.position = \"none\" )","title":"Changing themes"},{"location":"arcadia-users-group/20231017-intro-to-ggplot2/lesson/#changing-labels","text":"We probably want to make our axis titles nicer, and perhaps add a main title to the plot. We can do this using the labs() function: myplot + theme_bw () + theme ( axis.title = element_text ( size = 14 ), legend.position = \"none\" ) + labs ( title = \"Rodent size by plot type\" , x = \"Plot type\" , y = \"Hindfoot length (mm)\" ) We removed our legend from this plot, but you can also change the titles of various legends using labs() . For example, labs(color = \"Plot type\") would change the title of a color scale legend to \"Plot type\". Challenge 3: Customizing a plot Modify the previous plot by adding a descriptive subtitle. Increase the font size of the plot title and make it bold. Hint : \"bold\" is referred to as a font \"face.\" Challenge solution myplot + theme_bw () + theme ( axis.title = element_text ( size = 14 ), legend.position = \"none\" , plot.title = element_text ( face = \"bold\" , size = 20 )) + labs ( title = \"Rodent size by plot type\" , subtitle = \"Long-term dataset from Portal, AZ\" , x = \"Plot type\" , y = \"Hindfoot length (mm)\" )","title":"Changing labels"},{"location":"arcadia-users-group/20231017-intro-to-ggplot2/lesson/#faceting","text":"One of the most powerful features of ggplot is the ability to quickly split a plot into multiple smaller plots based on a categorical variable, which is called faceting . So far we've mapped variables to the x axis, the y axis, and color, but trying to add a 4th variable becomes difficult. Changing the shape of a point might work, but only for very few categories, and even then, it can be hard to tell the differences between the shapes of small points. Instead of cramming one more variable into a single plot, we will use the facet_wrap() function to generate a series of smaller plots, split out by sex . We also use ncol to specify that we want them arranged in a single column: myplot + theme_bw () + theme ( axis.title = element_text ( size = 14 ), legend.position = \"none\" , panel.grid.major.x = element_blank ()) + labs ( title = \"Rodent size by plot type\" , x = \"Plot type\" , y = \"Hindfoot length (mm)\" , color = \"Plot type\" ) + facet_wrap ( vars ( sex ), ncol = 1 ) Faceting comes in handy in many scenarios. It can be useful when: a categorical variable has too many levels to differentiate by color (such as a dataset with 20 countries) your data overlap heavily, obscuring categories you want to show more than 3 variables at once you want to see each category in isolation while allowing for general comparisons between categories","title":"Faceting"},{"location":"arcadia-users-group/20231017-intro-to-ggplot2/lesson/#exporting-plots","text":"Once we are happy with our final plot, we can assign the whole thing to a new object, which we can call finalplot . finalplot <- myplot + theme_bw () + theme ( axis.title = element_text ( size = 14 ), legend.position = \"none\" , panel.grid.major.x = element_blank ()) + labs ( title = \"Rodent size by plot type\" , x = \"Plot type\" , y = \"Hindfoot length (mm)\" , color = \"Plot type\" ) + facet_wrap ( vars ( sex ), ncol = 1 ) After this, we can run ggsave() to save our plot. The first argument we give is the path to the file we want to save, including the correct file extension. This code will make an image called rodent_size_plots.jpg in the images/ folder of our current project. We are making a .jpg , but you can save .pdf , .tiff , and other file formats. Next, we tell it the name of the plot object we want to save. We can also specify things like the width and height of the plot in inches. ggsave ( filename = \"images/rodent_size_plots.jpg\" , plot = finalplot , height = 6 , width = 8 ) The final plot should look like this: ## Warning: Removed 2733 rows containing non-finite values (`stat_boxplot()`). ## Warning: Removed 2733 rows containing missing values (`geom_point()`). Challenge 4: Make your own plot Try making your own plot! You can run str(complete_old) or ?complete_old to explore variables you might use in your new plot. Feel free to use variables we have already seen, or some we haven't explored yet. Here are a couple ideas to get you started: make a histogram of one of the numeric variables try using a different color scale_ try changing the size of points or thickness of lines in a geom","title":"Exporting plots"},{"location":"arcadia-users-group/20231031-intro-to-r/lesson/","text":"Exploring & working with data in R and tidyverse Exploring data in R In this lesson, we'll spend time unpacking the basic data structures in R that we breezed past in our previous lesson . Then, we'll go over how to read in data and manipulate it in tidyverse. Note that this lesson has been modified from The Carpentries alternative version of the R Ecology lesson . Parts are reproduced in full, but the major changes were included to shorten the lesson to 60 minutes. The data.frame In the previous lesson , we created visualizations from the complete_old data, but we did not talk much about what this complete_old thing is. During this lesson, we'll cover how R represents, uses, and stores data. To start out, launch a fresh RStudio application window and load the two packages we'll work with during this lesson. library ( tidyverse ) library ( ratdat ) The complete_old data is stored in R as a data.frame , which is the most common way that R represents tabular data (data that can be stored in a table format, like a spreadsheet). We can check what complete_old is by using the class() function: class ( complete_old ) ## [1] \"data.frame\" We can view the first few rows with the head() function, and the last few rows with the tail() function: head ( complete_old ) ## record_id month day year plot_id species_id sex hindfoot_length weight ## 1 1 7 16 1977 2 NL M 32 NA ## 2 2 7 16 1977 3 NL M 33 NA ## 3 3 7 16 1977 2 DM F 37 NA ## 4 4 7 16 1977 7 DM M 36 NA ## 5 5 7 16 1977 3 DM M 35 NA ## 6 6 7 16 1977 1 PF M 14 NA ## genus species taxa plot_type ## 1 Neotoma albigula Rodent Control ## 2 Neotoma albigula Rodent Long-term Krat Exclosure ## 3 Dipodomys merriami Rodent Control ## 4 Dipodomys merriami Rodent Rodent Exclosure ## 5 Dipodomys merriami Rodent Long-term Krat Exclosure ## 6 Perognathus flavus Rodent Spectab exclosure tail ( complete_old ) ## record_id month day year plot_id species_id sex hindfoot_length weight ## 16873 16873 12 5 1989 8 DO M 37 51 ## 16874 16874 12 5 1989 16 RM F 18 15 ## 16875 16875 12 5 1989 5 RM M 17 9 ## 16876 16876 12 5 1989 4 DM M 37 31 ## 16877 16877 12 5 1989 11 DM M 37 50 ## 16878 16878 12 5 1989 8 DM F 37 42 ## genus species taxa plot_type ## 16873 Dipodomys ordii Rodent Control ## 16874 Reithrodontomys megalotis Rodent Rodent Exclosure ## 16875 Reithrodontomys megalotis Rodent Rodent Exclosure ## 16876 Dipodomys merriami Rodent Control ## 16877 Dipodomys merriami Rodent Control ## 16878 Dipodomys merriami Rodent Control Refresher on named arguments & argument order We used these functions with just one argument, the object complete_old , and we didn't give the argument a name, like we often did with ggplot2 . In R, a function's arguments come in a particular order, and if you put them in the correct order, you don't need to name them. In this case, the name of the argument is x , so we can name it if we want, but since we know it's the first argument, we don't need to. To learn more about a function, you can type a ? in front of the name of the function, which will bring up the official documentation for that function: ? head Some arguments are optional. For example, the n argument in head() specifies the number of rows to print. It defaults to 6, but we can override that by specifying a different number: head ( complete_old , n = 10 ) If we order them correctly, we don't have to name either: head ( complete_old , 10 ) Additionally, if we name them, we can put them in any order we want: head ( n = 10 , x = complete_old ) Generally, it's good practice to start with the required arguments, like the data.frame whose rows you want to see, and then to name the optional arguments. If you are ever unsure, it never hurts to explicitly name an argument. As we have already done, we can use str() to look at the structure of an object: str ( complete_old ) ## 'data.frame': 16878 obs. of 13 variables: ## $ record_id : int 1 2 3 4 5 6 7 8 9 10 ... ## $ month : int 7 7 7 7 7 7 7 7 7 7 ... ## $ day : int 16 16 16 16 16 16 16 16 16 16 ... ## $ year : int 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977 ... ## $ plot_id : int 2 3 2 7 3 1 2 1 1 6 ... ## $ species_id : chr \"NL\" \"NL\" \"DM\" \"DM\" ... ## $ sex : chr \"M\" \"M\" \"F\" \"M\" ... ## $ hindfoot_length: int 32 33 37 36 35 14 NA 37 34 20 ... ## $ weight : int NA NA NA NA NA NA NA NA NA NA ... ## $ genus : chr \"Neotoma\" \"Neotoma\" \"Dipodomys\" \"Dipodomys\" ... ## $ species : chr \"albigula\" \"albigula\" \"merriami\" \"merriami\" ... ## $ taxa : chr \"Rodent\" \"Rodent\" \"Rodent\" \"Rodent\" ... ## $ plot_type : chr \"Control\" \"Long-term Krat Exclosure\" \"Control\" \"Rodent Exclosure\" ... One thing we didn't cover previously when looking at the output of the str() function was the preponderance of $ -- each variable is preceded by a $ . The $ is an operator that allows us to select individual columns from a data.frame. Like functions and their arguments, we can use tab-completion after $ to select which variable you want from a given data.frame. For example, to get the year variable, we can type complete_old$ and then hit Tab . We get a list of the variables that we can move through with up and down arrow keys. Hit Enter when you reach year , which should finish this code: complete_old $ year This prints the values of the year column to the console. Vectors: the building block of data You might have noticed that our last result looked different from when we printed out the complete_old data.frame itself. That's because it is not a data.frame, it is a vector . A vector is a 1-dimensional series of values, in this case a vector of numbers representing years. Data.frames are made up of vectors; each column in a data.frame is a vector. Vectors are the basic building blocks of all data in R. Basically, everything in R is a vector, a bunch of vectors stitched together in some way, or a function. There are 4 main types of vectors (also known as atomic vectors ): \"character\" for strings of characters, like our genus or sex columns. Each entry in a character vector is wrapped in quotes. In other programming languages, this type of data may be referred to as \"strings\". \"integer\" for integers. All the numeric values in complete_old are integers. You may sometimes see integers represented like 2L or 20L . The L indicates to R that it is an integer, instead of the next data type, \"numeric\" . \"numeric\" , aka \"double\" , vectors can contain numbers including decimals. Other languages may refer to these as \"float\" or \"floating point\" numbers. \"logical\" for TRUE and FALSE , which can also be represented as T and F . In other contexts, these may be referred to as \"Boolean\" data. Vectors can only be of a single type . Since each column in a data.frame is a vector, this means an accidental character following a number, like 29, can change the type of the whole vector. To create a vector from scratch, we can use the c() function, putting values inside, separated by commas. c ( 1 , 2 , 5 , 12 , 4 ) ## [1] 1 2 5 12 4 As you can see, those values get printed out in the console, just like with complete_old$year . To store this vector so we can continue to work with it, we need to assign it to an object. num <- c ( 1 , 2 , 5 , 12 , 4 ) You can check what kind of object num is with the class() function. class ( num ) ## [1] \"numeric\" We see that num is a numeric vector. Let's try making a character vector: char <- c ( \"apple\" , \"pear\" , \"grape\" ) class ( char ) ## [1] \"character\" Remember that each entry, like \"apple\" , needs to be surrounded by quotes, and entries are separated with commas. If you do something like \"apple, pear, grape\" , you will have only a single entry containing that whole string. Finally, let's make a logical vector: logi <- c ( TRUE , FALSE , TRUE , TRUE ) class ( logi ) ## [1] \"logical\" Challenge 1: Coercion Since vectors can only hold one type of data, something has to be done when we try to combine different types of data into one vector. What type will each of these vectors be? Try to guess without running any code at first, then run the code and use class() to verify your answers. num_logi <- c ( 1 , 4 , 6 , TRUE ) num_char <- c ( 1 , 3 , \"10\" , 6 ) char_logi <- c ( \"a\" , \"b\" , TRUE ) tricky <- c ( \"a\" , \"b\" , \"1\" , FALSE ) Challenge solution class ( num_logi ) ## [1] \"numeric\" class ( num_char ) ## [1] \"character\" class ( char_logi ) ## [1] \"character\" class ( tricky ) ## [1] \"character\" R will automatically convert values in a vector so that they are all the same type, a process called coercion . How many values in combined_logical are \"TRUE\" (as a character)? combined_logical <- c ( num_logi , char_logi ) Challenge solution combined_logical ## [1] \"1\" \"4\" \"6\" \"1\" \"a\" \"b\" \"TRUE\" class ( combined_logical ) ## [1] \"character\" Only one value is \"TRUE\" . Coercion happens when each vector is created, so the TRUE in num_logi becomes a 1 , while the TRUE in char_logi becomes \"TRUE\" . When these two vectors are combined, R doesn't remember that the 1 in num_logi used to be a TRUE , it will just coerce the 1 to \"1\" . Now that you've seen a few examples of coercion, you might have started to see that there are some rules about how types get converted. There is a hierarchy to coercion. Can you draw a diagram that represents the hierarchy of what types get converted to other types? Challenge solution logical \u2192 integer \u2192 numeric \u2192 character Logical vectors can only take on two values: TRUE or FALSE . Integer vectors can only contain integers, so TRUE and FALSE can be coerced to 1 and 0 . Numeric vectors can contain numbers with decimals, so integers can be coerced from, say, 6 to 6.0 (though R will still display a numeric 6 as 6 .). Finally, any string of characters can be represented as a character vector, so any of the other types can be coerced to a character vector. Coercion most often happens when combining vectors when they contain different data types or reading data into R where a stray character may change an entire numeric vector into a character vector. Using the class() function can help confirm an object's class meets your expectations, particularly if you are running into confusing error messages. Missing data R represents missing data as NA , without quotes, in vectors of any type. The NA signals to R to treat that data differently than the rest of the entries in the vector. Let's make a numeric vector with an NA value: weights <- c ( 25 , 34 , 12 , NA , 42 ) By default, may R functions won't work when NA values are present; instead, if you use them they'll return NA themselves. min ( weights ) ## [1] NA This behavior protects the user from not considering missing data. If we decide to exclude our missing values, many basic math functions have an argument to r e m ove them: min ( weights , na.rm = TRUE ) ## [1] 12 Vectors as arguments A common reason to create a vector from scratch is to use in a function argument. For example, the quantile() function will calculate a quantile for a given vector of numeric values. We set the quantile using the probs argument. We also need to set na.rm = TRUE , since there are NA values in the weight column. quantile ( complete_old $ weight , probs = 0.25 , na.rm = TRUE ) ## 25% ## 24 Now we get back the 25% quantile value for weights. However, we often want to know more than one quantile. Luckily, the probs argument is vectorized , meaning it can take a whole vector of values. Let's try getting the 25%, 50% (median), and 75% quantiles all at once. quantile ( complete_old $ weight , probs = c ( 0.25 , 0.5 , 0.75 ), na.rm = TRUE ) ## 25% 50% 75% ## 24 42 53 Other data tyes in R We have now seen vectors in a few different forms: as columns in a data.frame and as single vectors. However, they can be manipulated into lots of other shapes and forms. Some other common forms are: matrices 2-dimensional numeric representations arrays many-dimensional numeric lists lists are very flexible ways to store vectors a list can contain vectors of many different types and lengths an entry in a list can be another list, so lists can get deeply nested a data.frame is a type of list where each column is an individual vector and each vector has to be the same length, since a data.frame has an entry in every column for each row factors a way to represent categorical data factors can be ordered or unordered they often look like character vectors, but behave differently under the hood, they are integers with character labels, called levels , for each integer We won't spend time on these data types in this lesson. If you want more information on these classes, you can read through this Software Carpentry lesson . Working with data in R Importing data Up until this point, we have been working with the complete_old data.frame contained in the ratdat package. However, you typically won't access data from an R package; it is much more common to access data files stored somewhere on your computer. We are going to download a CSV file containing the surveys data to our computer, which we will then read into R. Click this link to download the file: https://github.com/Arcadia-Science/arcadia-computational-training/blob/main/docs/arcadia-users-group/20231031-intro-to-r/surveys_complete_77_89.csv . You will be prompted to save the file on your computer somewhere. Save it in your ~/Downloads folder so that we'll all be working with the same file path. More information on project organization and management. For this lesson, we aren't worrying about our file and folder organization. In general, it's best practice to keep your project organized in a specific folder. The exact organization strategy will depend on the project you're doing and the tools you choose to use. For more information on how to keep your project organized, a discussion of some best practices, and strategies you can use, see the lesson Project organization and file & resource management . File paths When we reference other files from an R script, we need to give R precise instructions on where those files are. We do that using something called a file path . It looks something like this: \"~/Documents/Manuscripts/Chapter_2.txt\" . This path would tell your computer how to get from whatever folder contains the Documents folder all the way to the .txt file. There are two kinds of paths: absolute and relative . Absolute paths are specific to a particular computer, whereas relative paths are relative to a certain folder. For more information on fie paths and directory structures, see this lesson . Let's read our CSV file into R and store it in an object named surveys . We will use the read_csv function from the tidyverse 's readr package. surveys <- read_csv ( \"~/Downloads/surveys_complete_77_89.csv\" ) ## Rows: 16878 Columns: 13 ## \u2500\u2500 Column specification \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 ## Delimiter: \",\" ## chr (6): species_id, sex, genus, species, taxa, plot_type ## dbl (7): record_id, month, day, year, plot_id, hindfoot_length, weight ## ## \u2139 Use `spec()` to retrieve the full column specification for this data. ## \u2139 Specify the column types or set `show_col_types = FALSE` to quiet this message. Using tab completion to solve the file path. Typing out paths can be error prone, so we can utilize a keyboard shortcut. Just like functions and other objects in RStudio, you can use tab completion on file paths. Inside the parentheses of read_csv() , type out a pair of quotes and put your cursor between them. Then hit Tab . A small menu showing your folders and files should show up. You can use the \u2191 and \u2193 keys to move through the options, or start typing to narrow them down. You can hit Enter to select a file or folder, and hit Tab again to continue building the file path. This might take a bit of getting used to, but once you get the hang of it, it will speed up writing file paths and reduce the number of mistakes you make. By default, the read_csv() function prints information about the files it reads to the Console. We got some useful information about the CSV file we read in. We can see: the number of rows and columns the delimiter of the file, which is how values are separated, a comma \",\" a set of columns that were parsed as various vector types the file has 6 character columns and 7 numeric columns we can see the names of the columns for each type When working with the output of a new function, it's often a good idea to check the class() : class ( surveys ) ## [1] \"spec_tbl_df\" \"tbl_df\" \"tbl\" \"data.frame\" Whoa! What is this thing? It has multiple classes? Well, it's called a tibble , and it is the tidyverse version of a data.frame. It is a data.frame, but with some added perks. It prints out a little more nicely, it highlights NA values and negative values in red, and it will generally communicate with you more (in terms of warnings and errors, which is a good thing). The difference between tidyverse and base R As we begin to delve more deeply into the tidyverse , we should briefly pause to mention some of the reasons for focusing on the tidyverse set of tools. In R, there are often many ways to get a job done, and there are other approaches that can accomplish tasks similar to the tidyverse . The phrase base R is used to refer to approaches that utilize functions contained in R's default packages. We have already used some base R functions, such as str() , head() , and mean() , and we will be using more scattered throughout this lesson. However, this lesson won't cover some functionalities in base R such as sub-setting with square bracket notation and base plotting. You may come across code written by other people that looks like surveys[1:10, 2] or plot(surveys$weight, surveys$hindfoot_length) , which are base R commands. If you're interested in learning more about these approaches, you can check out other Carpentries lessons like the Software Carpentry Programming with R lesson. We choose to teach the tidyverse set of packages because they share a similar syntax and philosophy, making them consistent and producing highly readable code. They are also very flexible and powerful, with a growing number of packages designed according to similar principles and to work well with the rest of the packages. The tidyverse packages tend to have very clear documentation and wide array of learning materials that tend to be written with novice users in mind. Finally, the tidyverse has only continued to grow, and has strong support from Posti (the RStudio parent company), which implies that these approaches will be relevant into the future. Manipulating data One of the most important skills for working with data in R is the ability to manipulate, modify, and reshape data. The dplyr and tidyr packages in the tidyverse provide a series of powerful functions for many common data manipulation tasks. We'll start off with two of the most commonly used dplyr functions: select() , which selects certain columns of a data.frame, and filter() , which filters out rows according to certain criteria. Between select() and filter() , it can be hard to remember which operates on columns and which operates on rows. sele c t() has a c for c olumns and filte r () has an r for r ows. select() To use the select() function, the first argument is the name of the data.frame, and the rest of the arguments are unquoted names of the columns you want: select ( surveys , plot_id , species_id , hindfoot_length ) ## # A tibble: 16,878 \u00d7 3 ## plot_id species_id hindfoot_length ## <dbl> <chr> <dbl> ## 1 2 NL 32 ## 2 3 NL 33 ## 3 2 DM 37 ## 4 7 DM 36 ## 5 3 DM 35 ## 6 1 PF 14 ## 7 2 PE NA ## 8 1 DM 37 ## 9 1 DM 34 ## 10 6 PF 20 ## # \u2139 16,868 more rows The columns are arranged in the order we specified inside select() . To select all columns except specific columns, put a - in front of the column you want to exclude: select ( surveys , - record_id , - year ) ## # A tibble: 16,878 \u00d7 11 ## month day plot_id species_id sex hindfoot_length weight genus species ## <dbl> <dbl> <dbl> <chr> <chr> <dbl> <dbl> <chr> <chr> ## 1 7 16 2 NL M 32 NA Neotoma albigu\u2026 ## 2 7 16 3 NL M 33 NA Neotoma albigu\u2026 ## 3 7 16 2 DM F 37 NA Dipodomys merria\u2026 ## 4 7 16 7 DM M 36 NA Dipodomys merria\u2026 ## 5 7 16 3 DM M 35 NA Dipodomys merria\u2026 ## 6 7 16 1 PF M 14 NA Perognat\u2026 flavus ## 7 7 16 2 PE F NA NA Peromysc\u2026 eremic\u2026 ## 8 7 16 1 DM M 37 NA Dipodomys merria\u2026 ## 9 7 16 1 DM F 34 NA Dipodomys merria\u2026 ## 10 7 16 6 PF F 20 NA Perognat\u2026 flavus ## # \u2139 16,868 more rows ## # \u2139 2 more variables: taxa <chr>, plot_type <chr> filter() The filter() function is used to select rows that meet certain criteria. To get all the rows where the value of year is equal to 1985, we would run the following: filter ( surveys , year == 1985 ) ## # A tibble: 1,438 \u00d7 13 ## record_id month day year plot_id species_id sex hindfoot_length weight ## <dbl> <dbl> <dbl> <dbl> <dbl> <chr> <chr> <dbl> <dbl> ## 1 9790 1 19 1985 16 RM F 16 4 ## 2 9791 1 19 1985 17 OT F 20 16 ## 3 9792 1 19 1985 6 DO M 35 48 ## 4 9793 1 19 1985 12 DO F 35 40 ## 5 9794 1 19 1985 24 RM M 16 4 ## 6 9795 1 19 1985 12 DO M 34 48 ## 7 9796 1 19 1985 6 DM F 37 35 ## 8 9797 1 19 1985 14 DM M 36 45 ## 9 9798 1 19 1985 6 DM F 36 38 ## 10 9799 1 19 1985 19 RM M 16 4 ## # \u2139 1,428 more rows ## # \u2139 4 more variables: genus <chr>, species <chr>, taxa <chr>, plot_type <chr> The == sign means \"is equal to.\" There are several other operators we can use: >, >=, <, <=, and != (not equal to). Another useful operator is %in% , which asks if the value on the left hand side is found anywhere in the vector on the right hand side. For example, to get rows with specific species_id values, we could run: filter ( surveys , species_id %in% c ( \"RM\" , \"DO\" )) ## # A tibble: 2,835 \u00d7 13 ## record_id month day year plot_id species_id sex hindfoot_length weight ## <dbl> <dbl> <dbl> <dbl> <dbl> <chr> <chr> <dbl> <dbl> ## 1 68 8 19 1977 8 DO F 32 52 ## 2 292 10 17 1977 3 DO F 36 33 ## 3 294 10 17 1977 3 DO F 37 50 ## 4 311 10 17 1977 19 RM M 18 13 ## 5 317 10 17 1977 17 DO F 32 48 ## 6 323 10 17 1977 17 DO F 33 31 ## 7 337 10 18 1977 8 DO F 35 41 ## 8 356 11 12 1977 1 DO F 32 44 ## 9 378 11 12 1977 1 DO M 33 48 ## 10 397 11 13 1977 17 RM F 16 7 ## # \u2139 2,825 more rows ## # \u2139 4 more variables: genus <chr>, species <chr>, taxa <chr>, plot_type <chr> We can also use multiple conditions in one filter() statement. Here we will get rows with a year less than or equal to 1988 and whose hindfoot length values are not NA . The ! before the is.na() function means \"not\". filter ( surveys , year <= 1988 & ! is.na ( hindfoot_length )) ## # A tibble: 12,779 \u00d7 13 ## record_id month day year plot_id species_id sex hindfoot_length weight ## <dbl> <dbl> <dbl> <dbl> <dbl> <chr> <chr> <dbl> <dbl> ## 1 1 7 16 1977 2 NL M 32 NA ## 2 2 7 16 1977 3 NL M 33 NA ## 3 3 7 16 1977 2 DM F 37 NA ## 4 4 7 16 1977 7 DM M 36 NA ## 5 5 7 16 1977 3 DM M 35 NA ## 6 6 7 16 1977 1 PF M 14 NA ## 7 8 7 16 1977 1 DM M 37 NA ## 8 9 7 16 1977 1 DM F 34 NA ## 9 10 7 16 1977 6 PF F 20 NA ## 10 11 7 16 1977 5 DS F 53 NA ## # \u2139 12,769 more rows ## # \u2139 4 more variables: genus <chr>, species <chr>, taxa <chr>, plot_type <chr> Challenge 2: Filtering and selecting Use the surveys data to make a data.frame that has only data with years from 1980 to 1985. Challenge solution surveys_filtered <- filter ( surveys , year >= 1980 & year <= 1985 ) Use the surveys data to make a data.frame that has only the following columns, in order: year , month , species_id , plot_id . Challenge solution surveys_selected <- select ( surveys , year , month , species_id , plot_id ) The pipe: %>% What happens if we want to both select() and filter() our data? We have a couple options. For example, we can create intermediate objects: surveys_noday <- select ( surveys , - day ) filter ( surveys_noday , month >= 7 ) ## # A tibble: 8,244 \u00d7 12 ## record_id month year plot_id species_id sex hindfoot_length weight genus ## <dbl> <dbl> <dbl> <dbl> <chr> <chr> <dbl> <dbl> <chr> ## 1 1 7 1977 2 NL M 32 NA Neotoma ## 2 2 7 1977 3 NL M 33 NA Neotoma ## 3 3 7 1977 2 DM F 37 NA Dipodo\u2026 ## 4 4 7 1977 7 DM M 36 NA Dipodo\u2026 ## 5 5 7 1977 3 DM M 35 NA Dipodo\u2026 ## 6 6 7 1977 1 PF M 14 NA Perogn\u2026 ## 7 7 7 1977 2 PE F NA NA Peromy\u2026 ## 8 8 7 1977 1 DM M 37 NA Dipodo\u2026 ## 9 9 7 1977 1 DM F 34 NA Dipodo\u2026 ## 10 10 7 1977 6 PF F 20 NA Perogn\u2026 ## # \u2139 8,234 more rows ## # \u2139 3 more variables: species <chr>, taxa <chr>, plot_type <chr> This approach accumulates a lot of intermediate objects, often with confusing names and without clear relationships between those objects. An elegant solution to this problem is an operator called the pipe , which looks like %>% . You can insert it by using the keyboard shortcut Shift+Cmd+M (Mac) or Shift+Ctrl+M (Windows). Here's how you could use a pipe to select and filter in one step: surveys %>% select ( - day ) %>% filter ( month >= 7 ) ## # A tibble: 8,244 \u00d7 12 ## record_id month year plot_id species_id sex hindfoot_length weight genus ## <dbl> <dbl> <dbl> <dbl> <chr> <chr> <dbl> <dbl> <chr> ## 1 1 7 1977 2 NL M 32 NA Neotoma ## 2 2 7 1977 3 NL M 33 NA Neotoma ## 3 3 7 1977 2 DM F 37 NA Dipodo\u2026 ## 4 4 7 1977 7 DM M 36 NA Dipodo\u2026 ## 5 5 7 1977 3 DM M 35 NA Dipodo\u2026 ## 6 6 7 1977 1 PF M 14 NA Perogn\u2026 ## 7 7 7 1977 2 PE F NA NA Peromy\u2026 ## 8 8 7 1977 1 DM M 37 NA Dipodo\u2026 ## 9 9 7 1977 1 DM F 34 NA Dipodo\u2026 ## 10 10 7 1977 6 PF F 20 NA Perogn\u2026 ## # \u2139 8,234 more rows ## # \u2139 3 more variables: species <chr>, taxa <chr>, plot_type <chr> What it does is take the thing on the left hand side and insert it as the first argument of the function on the right hand side. By putting each of our functions onto a new line, we can build a nice, readable chunk of code. It can be useful to think of this as a little assembly line for our data. It starts at the top and gets piped into a select() function, and it comes out modified somewhat. It then gets sent into the filter() function, where it is further modified, and then the final product gets printed out to our console. It can also be helpful to think of %>% as meaning \"and then\". Since many tidyverse functions have verbs for names, these chunks of code can be read like a sentence. If we want to store this final product as an object, we use an assignment arrow at the start: surveys_sub <- surveys %>% select ( - day ) %>% filter ( month >= 7 ) One approach is to build a piped together code chunk step by step prior to assignment. You add functions to the chunk as you go, with the results printing in the console for you to view. Once you're satisfied with your final result, go back and add the assignment arrow statement at the start. This approach is very interactive, allowing you to see the results of each step as you build the chunk, and produces nicely readable code. Challenge 3: Using pipes Use the surveys data to make a data.frame that has the columns record_id , month , and species_id , with data from the year 1988. Use a pipe between the function calls. Challenge solution surveys_1988 <- surveys %>% filter ( year == 1988 ) %>% select ( record_id , month , species_id ) Make sure to `filter()` before you `select()`; you need to use the `year` column for filtering rows, but it is discarded in the `select()` step. You also need to make sure to use `==` instead of `=` when you are filtering rows where `year` is equal to 1988. Making new columns with mutate() Another common task is creating a new column based on values in existing columns. For example, we could add a new column that has the weight in kilograms instead of grams: surveys %>% mutate ( weight_kg = weight / 1000 ) ## # A tibble: 16,878 \u00d7 14 ## record_id month day year plot_id species_id sex hindfoot_length weight ## <dbl> <dbl> <dbl> <dbl> <dbl> <chr> <chr> <dbl> <dbl> ## 1 1 7 16 1977 2 NL M 32 NA ## 2 2 7 16 1977 3 NL M 33 NA ## 3 3 7 16 1977 2 DM F 37 NA ## 4 4 7 16 1977 7 DM M 36 NA ## 5 5 7 16 1977 3 DM M 35 NA ## 6 6 7 16 1977 1 PF M 14 NA ## 7 7 7 16 1977 2 PE F NA NA ## 8 8 7 16 1977 1 DM M 37 NA ## 9 9 7 16 1977 1 DM F 34 NA ## 10 10 7 16 1977 6 PF F 20 NA ## # \u2139 16,868 more rows ## # \u2139 5 more variables: genus <chr>, species <chr>, taxa <chr>, plot_type <chr>, ## # weight_kg <dbl> You can create multiple columns in one mutate() call, and they will get created in the order you write them. This means you can even reference the first new column in the second new column: surveys %>% mutate ( weight_kg = weight / 1000 , weight_lbs = weight_kg * 2.2 ) ## # A tibble: 16,878 \u00d7 15 ## record_id month day year plot_id species_id sex hindfoot_length weight ## <dbl> <dbl> <dbl> <dbl> <dbl> <chr> <chr> <dbl> <dbl> ## 1 1 7 16 1977 2 NL M 32 NA ## 2 2 7 16 1977 3 NL M 33 NA ## 3 3 7 16 1977 2 DM F 37 NA ## 4 4 7 16 1977 7 DM M 36 NA ## 5 5 7 16 1977 3 DM M 35 NA ## 6 6 7 16 1977 1 PF M 14 NA ## 7 7 7 16 1977 2 PE F NA NA ## 8 8 7 16 1977 1 DM M 37 NA ## 9 9 7 16 1977 1 DM F 34 NA ## 10 10 7 16 1977 6 PF F 20 NA ## # \u2139 16,868 more rows ## # \u2139 6 more variables: genus <chr>, species <chr>, taxa <chr>, plot_type <chr>, ## # weight_kg <dbl>, weight_lbs <dbl> The split-apply-combine approach Many data analysis tasks can be achieved using the split-apply-combine approach: you split the data into groups, apply some analysis to each group, and combine the results in some way. dplyr has a few convenient functions to enable this approach, the main two being group_by() and summarize() . group_by() takes a data.frame and the name of one or more columns with categorical values that define the groups. summarize() then collapses each group into a one-row summary of the group, giving you back a data.frame with one row per group. The syntax for summarize() is similar to mutate() , where you define new columns based on values of other columns. Let's try calculating the mean weight of all our animals by sex. surveys %>% group_by ( sex ) %>% summarize ( mean_weight = mean ( weight , na.rm = T )) ## # A tibble: 3 \u00d7 2 ## sex mean_weight ## <chr> <dbl> ## 1 F 53.1 ## 2 M 53.2 ## 3 <NA> 74.0 You can see that the mean weight for males is slightly higher than for females, but that animals whose sex is unknown have much higher weights. This is probably due to small sample size, but we should check to be sure. Like mutate() , we can define multiple columns in one summarize() call. The function n() will count the number of rows in each group. surveys %>% group_by ( sex ) %>% summarize ( mean_weight = mean ( weight , na.rm = T ), n = n ()) ## # A tibble: 3 \u00d7 3 ## sex mean_weight n ## <chr> <dbl> <int> ## 1 F 53.1 7318 ## 2 M 53.2 8260 ## 3 <NA> 74.0 1300 You will often want to create groups based on multiple columns. For example, we might be interested in the mean weight of every species + sex combination. All we have to do is add another column to our group_by() call. surveys %>% group_by ( species_id , sex ) %>% summarize ( mean_weight = mean ( weight , na.rm = T ), n = n ()) ## `summarise()` has grouped output by 'species_id'. You can override using the ## `.groups` argument. ## # A tibble: 67 \u00d7 4 ## # Groups: species_id [36] ## species_id sex mean_weight n ## <chr> <chr> <dbl> <int> ## 1 AB <NA> NaN 223 ## 2 AH <NA> NaN 136 ## 3 BA M 7 3 ## 4 CB <NA> NaN 23 ## 5 CM <NA> NaN 13 ## 6 CQ <NA> NaN 16 ## 7 CS <NA> NaN 1 ## 8 CV <NA> NaN 1 ## 9 DM F 40.7 2522 ## 10 DM M 44.0 3108 ## # \u2139 57 more rows Our resulting data.frame is much larger, since we have a greater number of groups. We also see a strange value showing up in our mean_weight column: NaN . This stands for \"Not a Number\", and it often results from trying to do an operation a vector with zero entries. How can a vector have zero entries? Well, if a particular group (like the AB species ID + NA sex group) has only NA values for weight, then the na.rm = T argument in mean() will remove all the values prior to calculating the mean. The result will be a value of NaN . Since we are not particularly interested in these values, let's add a step to our pipeline to remove rows where weight is NA before doing any other steps. This means that any groups with only NA values will disappear from our data.frame before we formally create the groups with group_by() . surveys %>% filter ( ! is.na ( weight )) %>% group_by ( species_id , sex ) %>% summarize ( mean_weight = mean ( weight ), n = n ()) ## `summarise()` has grouped output by 'species_id'. You can override using the ## `.groups` argument. ## # A tibble: 46 \u00d7 4 ## # Groups: species_id [18] ## species_id sex mean_weight n ## <chr> <chr> <dbl> <int> ## 1 BA M 7 3 ## 2 DM F 40.7 2460 ## 3 DM M 44.0 3013 ## 4 DM <NA> 37 8 ## 5 DO F 48.4 679 ## 6 DO M 49.3 748 ## 7 DO <NA> 44 1 ## 8 DS F 118. 1055 ## 9 DS M 123. 1184 ## 10 DS <NA> 121. 16 ## # \u2139 36 more rows That looks better! It's often useful to take a look at the results in some order, like the lowest mean weight to highest. We can use the arrange() function for that: surveys %>% filter ( ! is.na ( weight )) %>% group_by ( species_id , sex ) %>% summarize ( mean_weight = mean ( weight ), n = n ()) %>% arrange ( mean_weight ) ## `summarise()` has grouped output by 'species_id'. You can override using the ## `.groups` argument. ## # A tibble: 46 \u00d7 4 ## # Groups: species_id [18] ## species_id sex mean_weight n ## <chr> <chr> <dbl> <int> ## 1 PF <NA> 6 2 ## 2 BA M 7 3 ## 3 PF F 7.09 215 ## 4 PF M 7.10 296 ## 5 RM M 9.92 678 ## 6 RM <NA> 10.4 7 ## 7 RM F 10.7 629 ## 8 RF M 12.4 16 ## 9 RF F 13.7 46 ## 10 PP <NA> 15 2 ## # \u2139 36 more rows If we want to reverse the order, we can wrap the column name in desc() : surveys %>% filter ( ! is.na ( weight )) %>% group_by ( species_id , sex ) %>% summarize ( mean_weight = mean ( weight ), n = n ()) %>% arrange ( desc ( mean_weight )) ## `summarise()` has grouped output by 'species_id'. You can override using the ## `.groups` argument. ## # A tibble: 46 \u00d7 4 ## # Groups: species_id [18] ## species_id sex mean_weight n ## <chr> <chr> <dbl> <int> ## 1 NL M 168. 355 ## 2 NL <NA> 164. 9 ## 3 NL F 151. 460 ## 4 SS M 130 1 ## 5 DS M 123. 1184 ## 6 DS <NA> 121. 16 ## 7 DS F 118. 1055 ## 8 SH F 79.2 61 ## 9 SH M 67.6 34 ## 10 SF F 58.3 3 ## # \u2139 36 more rows You may have seen several messages saying `summarise() has grouped output by 'species_id'. These are warning you that your resulting data.frame has retained some group structure, which means any subsequent operations on that data.frame will happen at the group level. If you look at the resulting data.frame printed out in your console, you will see these lines: # A tibble: 46 \u00d7 4 # Groups: species_id [18] They tell us we have a data.frame with 46 rows, 4 columns, and a group variable species_id , for which there are 18 groups. We will see something similar if we use group_by() alone: surveys %>% group_by ( species_id , sex ) ## # A tibble: 16,878 \u00d7 13 ## # Groups: species_id, sex [67] ## record_id month day year plot_id species_id sex hindfoot_length weight ## <dbl> <dbl> <dbl> <dbl> <dbl> <chr> <chr> <dbl> <dbl> ## 1 1 7 16 1977 2 NL M 32 NA ## 2 2 7 16 1977 3 NL M 33 NA ## 3 3 7 16 1977 2 DM F 37 NA ## 4 4 7 16 1977 7 DM M 36 NA ## 5 5 7 16 1977 3 DM M 35 NA ## 6 6 7 16 1977 1 PF M 14 NA ## 7 7 7 16 1977 2 PE F NA NA ## 8 8 7 16 1977 1 DM M 37 NA ## 9 9 7 16 1977 1 DM F 34 NA ## 10 10 7 16 1977 6 PF F 20 NA ## # \u2139 16,868 more rows ## # \u2139 4 more variables: genus <chr>, species <chr>, taxa <chr>, plot_type <chr> What we get back is the entire surveys data.frame, but with the grouping variables added: 67 groups of species_id + sex combinations. Groups are often maintained throughout a pipeline, and if you assign the resulting data.frame to a new object, it will also have those groups. This can lead to confusing results if you forget about the grouping and want to carry out operations on the whole data.frame, not by group. Therefore, it is a good habit to remove the groups at the end of a chunk of piped code containing group_by() : surveys %>% filter ( ! is.na ( weight )) %>% group_by ( species_id , sex ) %>% summarize ( mean_weight = mean ( weight ), n = n ()) %>% arrange ( desc ( mean_weight )) %>% ungroup () ## `summarise()` has grouped output by 'species_id'. You can override using the ## `.groups` argument. ## # A tibble: 46 \u00d7 4 ## species_id sex mean_weight n ## <chr> <chr> <dbl> <int> ## 1 NL M 168. 355 ## 2 NL <NA> 164. 9 ## 3 NL F 151. 460 ## 4 SS M 130 1 ## 5 DS M 123. 1184 ## 6 DS <NA> 121. 16 ## 7 DS F 118. 1055 ## 8 SH F 79.2 61 ## 9 SH M 67.6 34 ## 10 SF F 58.3 3 ## # \u2139 36 more rows Now our data.frame just says # A tibble: 46 \u00d7 4 at the top, with no groups. Challenge 4: Making a time series Use the split-apply-combine approach to make a data.frame that counts the total number of animals of each sex caught on each day in the surveys data. Challenge solution surveys_daily_counts <- surveys %>% mutate ( date = ymd ( paste ( year , month , day , sep = \"-\" ))) %>% group_by ( date , sex ) %>% summarize ( n = n ()) ## `summarise()` has grouped output by 'date'. You can override using the ## `.groups` argument. # shorter approach using count() surveys_daily_counts <- surveys %>% mutate ( date = ymd ( paste ( year , month , day , sep = \"-\" ))) %>% count ( date , sex ) Now use the data.frame you just made to plot the daily number of animals of each sex caught over time. It's up to you what geom to use, but a line plot might be a good choice. You should also think about how to differentiate which data corresponds to which sex. Challenge solution surveys_daily_counts %>% ggplot ( aes ( x = date , y = n , color = sex )) + geom_line () ![](lesson_files/figure-html/time-series-challenge-answer-1.png) Joining data together with the *join() functions Often times, all the data you need to do a project might be contained in multiple CSV files. When you read those files into R, they will be in separate data.frames. To combine two different data.frames that both contain at least one shared column of information, you can use the dplyr *join() commands. We'll practice a *join() command by downloading and reading in a CSV file that annotates the plot_type for each plot in our surveys data.frame. download.file ( \"https://ndownloader.figshare.com/files/3299474\" , \"plots.csv\" ) plots <- read_csv ( \"plots.csv\" ) ## Rows: 24 Columns: 2 ## \u2500\u2500 Column specification \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 ## Delimiter: \",\" ## chr (1): plot_type ## dbl (1): plot_id ## ## \u2139 Use `spec()` to retrieve the full column specification for this data. ## \u2139 Specify the column types or set `show_col_types = FALSE` to quiet this message. head ( plots ) ## # A tibble: 6 \u00d7 2 ## plot_id plot_type ## <dbl> <chr> ## 1 1 Spectab exclosure ## 2 2 Control ## 3 3 Long-term Krat Exclosure ## 4 4 Control ## 5 5 Rodent Exclosure ## 6 6 Short-term Krat Exclosure There are multiple join commands: left_join() , right_join() , inner_join() , and full_join() . We'll practice doing a left_join() : for two data.frames, keep all rows in the left data.frame even if this introduces NA values because there is no corresponding information in the right data.frame. left_join ( surveys , plots , by = \"plot_id\" ) ## # A tibble: 16,878 \u00d7 14 ## record_id month day year plot_id species_id sex hindfoot_length weight ## <dbl> <dbl> <dbl> <dbl> <dbl> <chr> <chr> <dbl> <dbl> ## 1 1 7 16 1977 2 NL M 32 NA ## 2 2 7 16 1977 3 NL M 33 NA ## 3 3 7 16 1977 2 DM F 37 NA ## 4 4 7 16 1977 7 DM M 36 NA ## 5 5 7 16 1977 3 DM M 35 NA ## 6 6 7 16 1977 1 PF M 14 NA ## 7 7 7 16 1977 2 PE F NA NA ## 8 8 7 16 1977 1 DM M 37 NA ## 9 9 7 16 1977 1 DM F 34 NA ## 10 10 7 16 1977 6 PF F 20 NA ## # \u2139 16,868 more rows ## # \u2139 5 more variables: genus <chr>, species <chr>, taxa <chr>, ## # plot_type.x <chr>, plot_type.y <chr> In this command, we tell R to join the two data.frames by the plot_id column. In order for the two data.frames to be combined, they must share the same value in the plot_id column. However, when we look at the output, we see that we've introduced new columns, plot_type.x and plot_typs.y . It turns out we already had this information in our data.frame and didn't need to do a join. When we did, we only specified that we wanted to join on plot_id , but both data.frames shared the plot_type variable name, so the left_join() functions appended the suffixes .x and .y . Instead, let's join on two column names: left_join ( surveys , plots , by = c ( \"plot_id\" , \"plot_type\" )) ## # A tibble: 16,878 \u00d7 13 ## record_id month day year plot_id species_id sex hindfoot_length weight ## <dbl> <dbl> <dbl> <dbl> <dbl> <chr> <chr> <dbl> <dbl> ## 1 1 7 16 1977 2 NL M 32 NA ## 2 2 7 16 1977 3 NL M 33 NA ## 3 3 7 16 1977 2 DM F 37 NA ## 4 4 7 16 1977 7 DM M 36 NA ## 5 5 7 16 1977 3 DM M 35 NA ## 6 6 7 16 1977 1 PF M 14 NA ## 7 7 7 16 1977 2 PE F NA NA ## 8 8 7 16 1977 1 DM M 37 NA ## 9 9 7 16 1977 1 DM F 34 NA ## 10 10 7 16 1977 6 PF F 20 NA ## # \u2139 16,868 more rows ## # \u2139 4 more variables: genus <chr>, species <chr>, taxa <chr>, plot_type <chr> Now no suffixes are added to our column names. Exporting data Let's say we want to send the wide version of our surveys_1988 data.frame to a colleague who doesn't use R. In this case, we might want to save it as a CSV file. We can save this data.frame to a CSV using the write_csv() function from the readr package. The first argument is the name of the data.frame, and the second is the path to the new file we want to create, including the file extension .csv . write_csv ( surveys_1988 , \"~/Downloads/surveys_1988.csv\" ) If we go look into our Downloads folder, we will see this new CSV file. Extras Building vectors from scratch While the c() function is very flexible, it doesn't necessarily scale well. If you want to generate a long vector from scratch, you probably don't want to type everything out manually. There are a few functions that can help generate vectors. First, putting : between two numbers will generate a vector of integers starting with the first number and ending with the last. The seq() function allows you to generate similar sequences, but changing by any amount. # generates a sequence of integers 1 : 10 ## [1] 1 2 3 4 5 6 7 8 9 10 # with seq() you can generate sequences with a combination of: # from: starting value # to: ending value # by: how much should each entry increase # length.out: how long should the resulting vector be seq ( from = 0 , to = 1 , by = 0.1 ) ## [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 seq ( from = 0 , to = 1 , length.out = 50 ) ## [1] 0.00000000 0.02040816 0.04081633 0.06122449 0.08163265 0.10204082 ## [7] 0.12244898 0.14285714 0.16326531 0.18367347 0.20408163 0.22448980 ## [13] 0.24489796 0.26530612 0.28571429 0.30612245 0.32653061 0.34693878 ## [19] 0.36734694 0.38775510 0.40816327 0.42857143 0.44897959 0.46938776 ## [25] 0.48979592 0.51020408 0.53061224 0.55102041 0.57142857 0.59183673 ## [31] 0.61224490 0.63265306 0.65306122 0.67346939 0.69387755 0.71428571 ## [37] 0.73469388 0.75510204 0.77551020 0.79591837 0.81632653 0.83673469 ## [43] 0.85714286 0.87755102 0.89795918 0.91836735 0.93877551 0.95918367 ## [49] 0.97959184 1.00000000 seq ( from = 0 , by = 0.01 , length.out = 20 ) ## [1] 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.10 0.11 0.12 0.13 0.14 ## [16] 0.15 0.16 0.17 0.18 0.19 Finally, the rep() function allows you to repeat a value, or even a whole vector, as many times as you want, and works with any type of vector. # repeats \"a\" 12 times rep ( \"a\" , times = 12 ) ## [1] \"a\" \"a\" \"a\" \"a\" \"a\" \"a\" \"a\" \"a\" \"a\" \"a\" \"a\" \"a\" # repeats this whole sequence 4 times rep ( c ( \"a\" , \"b\" , \"c\" ), times = 4 ) ## [1] \"a\" \"b\" \"c\" \"a\" \"b\" \"c\" \"a\" \"b\" \"c\" \"a\" \"b\" \"c\" # repeats each value 4 times rep ( 1 : 10 , each = 4 ) ## [1] 1 1 1 1 2 2 2 2 3 3 3 3 4 4 4 4 5 5 5 5 6 6 6 6 7 ## [26] 7 7 7 8 8 8 8 9 9 9 9 10 10 10 10 Extra Challenge : Creating sequences Write some code to generate the following vector: ## [1] -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 Challenge solution rep ( -3 : 3 , 3 ) ## [1] -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 # this also works rep ( seq ( from = -3 , to = 3 , by = 1 ), 3 ) ## [1] -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 # you might also store the sequence as an intermediate vector my_seq <- seq ( from = -3 , to = 3 , by = 1 ) rep ( my_seq , 3 ) ## [1] -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 Calculate the quantiles for the complete_old hindfoot lengths at every 5% level (0%, 5%, 10%, 15%, etc.) Challenge solution quantile ( complete_old $ hindfoot_length , probs = seq ( from = 0 , to = 1 , by = 0.05 ), na.rm = T ) ## 0% 5% 10% 15% 20% 25% 30% 35% 40% 45% 50% 55% 60% 65% 70% 75% ## 6 16 17 19 20 21 22 31 33 34 35 35 36 36 36 37 ## 80% 85% 90% 95% 100% ## 37 39 49 51 70 An explanation and exploration of factors We will spend a bit more time talking about factors, since they are often a challenging type of data to work with. We can create a factor from scratch by putting a character vector made using c() into the factor() function: sex <- factor ( c ( \"male\" , \"female\" , \"female\" , \"male\" , \"female\" , NA )) sex ## [1] male female female male female <NA> ## Levels: female male We can inspect the levels of the factor using the levels() function: levels ( sex ) ## [1] \"female\" \"male\" In general, it is a good practice to leave your categorical data as a character vector until you need to use a factor. Here are some reasons you might need a factor: Another function requires you to use a factor You are plotting categorical data and want to control the ordering of categories in the plot Since factors can behave differently from character vectors, it is always a good idea to check what type of data you're working with. You might use a new function for the first time and be confused by the results, only to realize later that it produced a factor as an output, when you thought it was a character vector. You can convert a factor to a character vector using the as.character() function: as.character ( sex ) ## [1] \"male\" \"female\" \"female\" \"male\" \"female\" NA However, you need to be careful if you're somehow working with a factor that has numbers as its levels: f_num <- factor ( c ( 1990 , 1983 , 1977 , 1998 , 1990 )) # this will pull out the underlying integers, not the levels as.numeric ( f_num ) ## [1] 3 2 1 4 3 # if we first convert to characters, we can then convert to numbers as.numeric ( as.character ( f_num )) ## [1] 1990 1983 1977 1998 1990 Reshaping data with tidyr Let's say we are interested in comparing the mean weights of each species across our different plots. We can begin this process using the group_by() + summarize() approach: sp_by_plot <- surveys %>% filter ( ! is.na ( weight )) %>% group_by ( species_id , plot_id ) %>% summarise ( mean_weight = mean ( weight )) %>% arrange ( species_id , plot_id ) ## `summarise()` has grouped output by 'species_id'. You can override using the ## `.groups` argument. sp_by_plot ## # A tibble: 300 \u00d7 3 ## # Groups: species_id [18] ## species_id plot_id mean_weight ## <chr> <dbl> <dbl> ## 1 BA 3 8 ## 2 BA 21 6.5 ## 3 DM 1 42.7 ## 4 DM 2 42.6 ## 5 DM 3 41.2 ## 6 DM 4 41.9 ## 7 DM 5 42.6 ## 8 DM 6 42.1 ## 9 DM 7 43.2 ## 10 DM 8 43.4 ## # \u2139 290 more rows That looks great, but it is a bit difficult to compare values across plots. It would be nice if we could reshape this data.frame to make those comparisons easier Well, the tidyr package from the tidyverse has a pair of functions that allow you to reshape data by pivoting it: pivot_wider() and pivot_longer() . pivot_wider() will make the data wider, which means increasing the number of columns and reducing the number of rows. pivot_longer() will do the opposite, reducing the number of columns and increasing the number of rows. In this case, it might be nice to create a data.frame where each species has its own row, and each plot has its own column containing the mean weight for a given species. We will use pivot_wider() to reshape our data in this way. It takes 3 arguments: the name of the data.frame names_from : which column should be used to generate the names of the new columns? values_from : which column should be used to fill in the values of the new columns? Any columns not used for names_from or values_from will not be pivoted. {alt='Diagram depicting the behavior of pivot_wider() on a small tabular dataset.'} In our case, we want the new columns to be named from our plot_id column, with the values coming from the mean_weight column. We can pipe our data.frame right into pivot_wider() and add those two arguments: sp_by_plot_wide <- sp_by_plot %>% pivot_wider ( names_from = plot_id , values_from = mean_weight ) sp_by_plot_wide ## # A tibble: 18 \u00d7 25 ## # Groups: species_id [18] ## species_id `3` `21` `1` `2` `4` `5` `6` `7` `8` ## <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> ## 1 BA 8 6.5 NA NA NA NA NA NA NA ## 2 DM 41.2 41.5 42.7 42.6 41.9 42.6 42.1 43.2 43.4 ## 3 DO 42.7 NA 50.1 50.3 46.8 50.4 49.0 52 49.2 ## 4 DS 128. NA 129. 125. 118. 111. 114. 126. 128. ## 5 NL 171. 136. 154. 171. 164. 192. 176. 170. 134. ## 6 OL 32.1 28.6 35.5 34 33.0 32.6 31.8 NA 30.3 ## 7 OT 24.1 24.1 23.7 24.9 26.5 23.6 23.5 22 24.1 ## 8 OX 22 NA NA 22 NA 20 NA NA NA ## 9 PE 22.7 19.6 21.6 22.0 NA 21 21.6 22.8 19.4 ## 10 PF 7.12 7.23 6.57 6.89 6.75 7.5 7.54 7 6.78 ## 11 PH 28 31 NA NA NA 29 NA NA NA ## 12 PM 20.1 23.6 23.7 23.9 NA 23.7 22.3 23.4 23 ## 13 PP 17.1 13.6 14.3 16.4 14.8 19.8 16.8 NA 13.9 ## 14 RF 14.8 17 NA 16 NA 14 12.1 13 NA ## 15 RM 10.3 9.89 10.9 10.6 10.4 10.8 10.6 10.7 9 ## 16 SF NA 49 NA NA NA NA NA NA NA ## 17 SH 76.0 79.9 NA 88 NA 82.7 NA NA NA ## 18 SS NA NA NA NA NA NA NA NA NA ## # \u2139 15 more variables: `9` <dbl>, `10` <dbl>, `11` <dbl>, `12` <dbl>, ## # `13` <dbl>, `14` <dbl>, `15` <dbl>, `16` <dbl>, `17` <dbl>, `18` <dbl>, ## # `19` <dbl>, `20` <dbl>, `22` <dbl>, `23` <dbl>, `24` <dbl> Now we've got our reshaped data.frame. There are a few things to notice. First, we have a new column for each plot_id value. There is one old column left in the data.frame: species_id . It wasn't used in pivot_wider() , so it stays, and now contains a single entry for each unique species_id value. Finally, a lot of NA s have appeared. Some species aren't found in every plot, but because a data.frame has to have a value in every row and every column, an NA is inserted. We can double-check this to verify what is going on. Looking in our new pivoted data.frame, we can see that there is an NA value for the species BA in plot 1 . Let's take our sp_by_plot data.frame and look for the mean_weight of that species + plot combination. sp_by_plot %>% filter ( species_id == \"BA\" & plot_id == 1 ) ## # A tibble: 0 \u00d7 3 ## # Groups: species_id [0] ## # \u2139 3 variables: species_id <chr>, plot_id <dbl>, mean_weight <dbl> We get back 0 rows. There is no mean_weight for the species BA in plot 1 . This either happened because no BA were ever caught in plot 1 , or because every BA caught in plot 1 had an NA weight value and all the rows got removed when we used filter(!is.na(weight)) in the process of making sp_by_plot . Because there are no rows with that species + plot combination, in our pivoted data.frame, the value gets filled with NA . There is another pivot_ function that does the opposite, moving data from a wide to long format, called pivot_longer() . It takes 3 arguments: cols for the columns you want to pivot, names_to for the name of the new column which will contain the old column names, and values_to for the name of the new column which will contain the old values. {alt='Diagram depicting the behavior of pivot_longer() on a small tabular dataset.'} We can pivot our new wide data.frame to a long format using pivot_longer() . We want to pivot all the columns except species_id , and we will use PLOT for the new column of plot IDs, and MEAN_WT for the new column of mean weight values. sp_by_plot_wide %>% pivot_longer ( cols = - species_id , names_to = \"PLOT\" , values_to = \"MEAN_WT\" ) ## # A tibble: 432 \u00d7 3 ## # Groups: species_id [18] ## species_id PLOT MEAN_WT ## <chr> <chr> <dbl> ## 1 BA 3 8 ## 2 BA 21 6.5 ## 3 BA 1 NA ## 4 BA 2 NA ## 5 BA 4 NA ## 6 BA 5 NA ## 7 BA 6 NA ## 8 BA 7 NA ## 9 BA 8 NA ## 10 BA 9 NA ## # \u2139 422 more rows One thing you will notice is that all those NA values that got generated when we pivoted wider. However, we can filter those out, which gets us back to the same data as sp_by_plot , before we pivoted it wider. sp_by_plot_wide %>% pivot_longer ( cols = - species_id , names_to = \"PLOT\" , values_to = \"MEAN_WT\" ) %>% filter ( ! is.na ( MEAN_WT )) ## # A tibble: 300 \u00d7 3 ## # Groups: species_id [18] ## species_id PLOT MEAN_WT ## <chr> <chr> <dbl> ## 1 BA 3 8 ## 2 BA 21 6.5 ## 3 DM 3 41.2 ## 4 DM 21 41.5 ## 5 DM 1 42.7 ## 6 DM 2 42.6 ## 7 DM 4 41.9 ## 8 DM 5 42.6 ## 9 DM 6 42.1 ## 10 DM 7 43.2 ## # \u2139 290 more rows Data are often recorded in spreadsheets in a wider format, but lots of tidyverse tools, especially ggplot2 , like data in a longer format, so pivot_longer() is often very useful. Getting rid of any row where any value is NA with drop_na() In the split-apply-combine approach section above, we covered how to remove NA values for a specific column. If instead of removing NA s from a single column you would like to drop any row where any column is an NA , you can use the drop_na() function. surveys %>% drop_na () ## # A tibble: 13,773 \u00d7 13 ## record_id month day year plot_id species_id sex hindfoot_length weight ## <dbl> <dbl> <dbl> <dbl> <dbl> <chr> <chr> <dbl> <dbl> ## 1 63 8 19 1977 3 DM M 35 40 ## 2 64 8 19 1977 7 DM M 37 48 ## 3 65 8 19 1977 4 DM F 34 29 ## 4 66 8 19 1977 4 DM F 35 46 ## 5 67 8 19 1977 7 DM M 35 36 ## 6 68 8 19 1977 8 DO F 32 52 ## 7 69 8 19 1977 2 PF M 15 8 ## 8 70 8 19 1977 3 OX F 21 22 ## 9 71 8 19 1977 7 DM F 36 35 ## 10 74 8 19 1977 8 PF M 12 7 ## # \u2139 13,763 more rows ## # \u2139 4 more variables: genus <chr>, species <chr>, taxa <chr>, plot_type <chr> Alternatively, you can provide column names to the drop_na() function to specify which columns to remove NA values from. surveys %>% drop_na ( hindfoot_length , weight ) ## # A tibble: 13,797 \u00d7 13 ## record_id month day year plot_id species_id sex hindfoot_length weight ## <dbl> <dbl> <dbl> <dbl> <dbl> <chr> <chr> <dbl> <dbl> ## 1 63 8 19 1977 3 DM M 35 40 ## 2 64 8 19 1977 7 DM M 37 48 ## 3 65 8 19 1977 4 DM F 34 29 ## 4 66 8 19 1977 4 DM F 35 46 ## 5 67 8 19 1977 7 DM M 35 36 ## 6 68 8 19 1977 8 DO F 32 52 ## 7 69 8 19 1977 2 PF M 15 8 ## 8 70 8 19 1977 3 OX F 21 22 ## 9 71 8 19 1977 7 DM F 36 35 ## 10 74 8 19 1977 8 PF M 12 7 ## # \u2139 13,787 more rows ## # \u2139 4 more variables: genus <chr>, species <chr>, taxa <chr>, plot_type <chr> Tidy selection In the select section above, we demonstrated how to select columns based on column name. You can select columns based on whether they match a certain criteria by using tidyselect functions. Below, we'll demonstrate how to use the where() function and the ends_with() function, but there are many additional tidyselect functions such as starts_with() or contains()`. With the where() function, we can select columns that match a specific conditions. If we want all numeric columns, we can ask to select all the columns where the class is numeric : select ( surveys , where ( is.numeric )) ## # A tibble: 16,878 \u00d7 7 ## record_id month day year plot_id hindfoot_length weight ## <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> ## 1 1 7 16 1977 2 32 NA ## 2 2 7 16 1977 3 33 NA ## 3 3 7 16 1977 2 37 NA ## 4 4 7 16 1977 7 36 NA ## 5 5 7 16 1977 3 35 NA ## 6 6 7 16 1977 1 14 NA ## 7 7 7 16 1977 2 NA NA ## 8 8 7 16 1977 1 37 NA ## 9 9 7 16 1977 1 34 NA ## 10 10 7 16 1977 6 20 NA ## # \u2139 16,868 more rows Instead of giving names or positions of columns, we instead pass the where() function with the name of another function inside it, in this case is.numeric() , and we get all the columns for which that function returns TRUE . We can use this to select any columns that have any NA values in them: select ( surveys , where ( anyNA )) ## # A tibble: 16,878 \u00d7 7 ## species_id sex hindfoot_length weight genus species taxa ## <chr> <chr> <dbl> <dbl> <chr> <chr> <chr> ## 1 NL M 32 NA Neotoma albigula Rodent ## 2 NL M 33 NA Neotoma albigula Rodent ## 3 DM F 37 NA Dipodomys merriami Rodent ## 4 DM M 36 NA Dipodomys merriami Rodent ## 5 DM M 35 NA Dipodomys merriami Rodent ## 6 PF M 14 NA Perognathus flavus Rodent ## 7 PE F NA NA Peromyscus eremicus Rodent ## 8 DM M 37 NA Dipodomys merriami Rodent ## 9 DM F 34 NA Dipodomys merriami Rodent ## 10 PF F 20 NA Perognathus flavus Rodent ## # \u2139 16,868 more rows Instead of matching on a condition, the ends_with() function match variables (in this case column names) according to a given pattern. We'll use the string \"_id\" to select columns that end with the string _id : record_id , plot_id , and species_id . select ( surveys , ends_with ( \"_id\" )) ## # A tibble: 16,878 \u00d7 3 ## record_id plot_id species_id ## <dbl> <dbl> <chr> ## 1 1 2 NL ## 2 2 3 NL ## 3 3 2 DM ## 4 4 7 DM ## 5 5 3 DM ## 6 6 1 PF ## 7 7 2 PE ## 8 8 1 DM ## 9 9 1 DM ## 10 10 6 PF ## # \u2139 16,868 more rows More on joining data together with dplyr We briefly covered how to join two data.frames together above. If you would like more information and practice doing joins, this lesson provides more information and practice. It relies on a similar data set to the one we've been using in this lesson.","title":"Exploring & working with data in R and tidyverse"},{"location":"arcadia-users-group/20231031-intro-to-r/lesson/#exploring-working-with-data-in-r-and-tidyverse","text":"","title":"Exploring &amp; working with data in R and tidyverse"},{"location":"arcadia-users-group/20231031-intro-to-r/lesson/#exploring-data-in-r","text":"In this lesson, we'll spend time unpacking the basic data structures in R that we breezed past in our previous lesson . Then, we'll go over how to read in data and manipulate it in tidyverse. Note that this lesson has been modified from The Carpentries alternative version of the R Ecology lesson . Parts are reproduced in full, but the major changes were included to shorten the lesson to 60 minutes.","title":"Exploring data in R"},{"location":"arcadia-users-group/20231031-intro-to-r/lesson/#the-dataframe","text":"In the previous lesson , we created visualizations from the complete_old data, but we did not talk much about what this complete_old thing is. During this lesson, we'll cover how R represents, uses, and stores data. To start out, launch a fresh RStudio application window and load the two packages we'll work with during this lesson. library ( tidyverse ) library ( ratdat ) The complete_old data is stored in R as a data.frame , which is the most common way that R represents tabular data (data that can be stored in a table format, like a spreadsheet). We can check what complete_old is by using the class() function: class ( complete_old ) ## [1] \"data.frame\" We can view the first few rows with the head() function, and the last few rows with the tail() function: head ( complete_old ) ## record_id month day year plot_id species_id sex hindfoot_length weight ## 1 1 7 16 1977 2 NL M 32 NA ## 2 2 7 16 1977 3 NL M 33 NA ## 3 3 7 16 1977 2 DM F 37 NA ## 4 4 7 16 1977 7 DM M 36 NA ## 5 5 7 16 1977 3 DM M 35 NA ## 6 6 7 16 1977 1 PF M 14 NA ## genus species taxa plot_type ## 1 Neotoma albigula Rodent Control ## 2 Neotoma albigula Rodent Long-term Krat Exclosure ## 3 Dipodomys merriami Rodent Control ## 4 Dipodomys merriami Rodent Rodent Exclosure ## 5 Dipodomys merriami Rodent Long-term Krat Exclosure ## 6 Perognathus flavus Rodent Spectab exclosure tail ( complete_old ) ## record_id month day year plot_id species_id sex hindfoot_length weight ## 16873 16873 12 5 1989 8 DO M 37 51 ## 16874 16874 12 5 1989 16 RM F 18 15 ## 16875 16875 12 5 1989 5 RM M 17 9 ## 16876 16876 12 5 1989 4 DM M 37 31 ## 16877 16877 12 5 1989 11 DM M 37 50 ## 16878 16878 12 5 1989 8 DM F 37 42 ## genus species taxa plot_type ## 16873 Dipodomys ordii Rodent Control ## 16874 Reithrodontomys megalotis Rodent Rodent Exclosure ## 16875 Reithrodontomys megalotis Rodent Rodent Exclosure ## 16876 Dipodomys merriami Rodent Control ## 16877 Dipodomys merriami Rodent Control ## 16878 Dipodomys merriami Rodent Control Refresher on named arguments & argument order We used these functions with just one argument, the object complete_old , and we didn't give the argument a name, like we often did with ggplot2 . In R, a function's arguments come in a particular order, and if you put them in the correct order, you don't need to name them. In this case, the name of the argument is x , so we can name it if we want, but since we know it's the first argument, we don't need to. To learn more about a function, you can type a ? in front of the name of the function, which will bring up the official documentation for that function: ? head Some arguments are optional. For example, the n argument in head() specifies the number of rows to print. It defaults to 6, but we can override that by specifying a different number: head ( complete_old , n = 10 ) If we order them correctly, we don't have to name either: head ( complete_old , 10 ) Additionally, if we name them, we can put them in any order we want: head ( n = 10 , x = complete_old ) Generally, it's good practice to start with the required arguments, like the data.frame whose rows you want to see, and then to name the optional arguments. If you are ever unsure, it never hurts to explicitly name an argument. As we have already done, we can use str() to look at the structure of an object: str ( complete_old ) ## 'data.frame': 16878 obs. of 13 variables: ## $ record_id : int 1 2 3 4 5 6 7 8 9 10 ... ## $ month : int 7 7 7 7 7 7 7 7 7 7 ... ## $ day : int 16 16 16 16 16 16 16 16 16 16 ... ## $ year : int 1977 1977 1977 1977 1977 1977 1977 1977 1977 1977 ... ## $ plot_id : int 2 3 2 7 3 1 2 1 1 6 ... ## $ species_id : chr \"NL\" \"NL\" \"DM\" \"DM\" ... ## $ sex : chr \"M\" \"M\" \"F\" \"M\" ... ## $ hindfoot_length: int 32 33 37 36 35 14 NA 37 34 20 ... ## $ weight : int NA NA NA NA NA NA NA NA NA NA ... ## $ genus : chr \"Neotoma\" \"Neotoma\" \"Dipodomys\" \"Dipodomys\" ... ## $ species : chr \"albigula\" \"albigula\" \"merriami\" \"merriami\" ... ## $ taxa : chr \"Rodent\" \"Rodent\" \"Rodent\" \"Rodent\" ... ## $ plot_type : chr \"Control\" \"Long-term Krat Exclosure\" \"Control\" \"Rodent Exclosure\" ... One thing we didn't cover previously when looking at the output of the str() function was the preponderance of $ -- each variable is preceded by a $ . The $ is an operator that allows us to select individual columns from a data.frame. Like functions and their arguments, we can use tab-completion after $ to select which variable you want from a given data.frame. For example, to get the year variable, we can type complete_old$ and then hit Tab . We get a list of the variables that we can move through with up and down arrow keys. Hit Enter when you reach year , which should finish this code: complete_old $ year This prints the values of the year column to the console.","title":"The data.frame"},{"location":"arcadia-users-group/20231031-intro-to-r/lesson/#vectors-the-building-block-of-data","text":"You might have noticed that our last result looked different from when we printed out the complete_old data.frame itself. That's because it is not a data.frame, it is a vector . A vector is a 1-dimensional series of values, in this case a vector of numbers representing years. Data.frames are made up of vectors; each column in a data.frame is a vector. Vectors are the basic building blocks of all data in R. Basically, everything in R is a vector, a bunch of vectors stitched together in some way, or a function. There are 4 main types of vectors (also known as atomic vectors ): \"character\" for strings of characters, like our genus or sex columns. Each entry in a character vector is wrapped in quotes. In other programming languages, this type of data may be referred to as \"strings\". \"integer\" for integers. All the numeric values in complete_old are integers. You may sometimes see integers represented like 2L or 20L . The L indicates to R that it is an integer, instead of the next data type, \"numeric\" . \"numeric\" , aka \"double\" , vectors can contain numbers including decimals. Other languages may refer to these as \"float\" or \"floating point\" numbers. \"logical\" for TRUE and FALSE , which can also be represented as T and F . In other contexts, these may be referred to as \"Boolean\" data. Vectors can only be of a single type . Since each column in a data.frame is a vector, this means an accidental character following a number, like 29, can change the type of the whole vector. To create a vector from scratch, we can use the c() function, putting values inside, separated by commas. c ( 1 , 2 , 5 , 12 , 4 ) ## [1] 1 2 5 12 4 As you can see, those values get printed out in the console, just like with complete_old$year . To store this vector so we can continue to work with it, we need to assign it to an object. num <- c ( 1 , 2 , 5 , 12 , 4 ) You can check what kind of object num is with the class() function. class ( num ) ## [1] \"numeric\" We see that num is a numeric vector. Let's try making a character vector: char <- c ( \"apple\" , \"pear\" , \"grape\" ) class ( char ) ## [1] \"character\" Remember that each entry, like \"apple\" , needs to be surrounded by quotes, and entries are separated with commas. If you do something like \"apple, pear, grape\" , you will have only a single entry containing that whole string. Finally, let's make a logical vector: logi <- c ( TRUE , FALSE , TRUE , TRUE ) class ( logi ) ## [1] \"logical\" Challenge 1: Coercion Since vectors can only hold one type of data, something has to be done when we try to combine different types of data into one vector. What type will each of these vectors be? Try to guess without running any code at first, then run the code and use class() to verify your answers. num_logi <- c ( 1 , 4 , 6 , TRUE ) num_char <- c ( 1 , 3 , \"10\" , 6 ) char_logi <- c ( \"a\" , \"b\" , TRUE ) tricky <- c ( \"a\" , \"b\" , \"1\" , FALSE ) Challenge solution class ( num_logi ) ## [1] \"numeric\" class ( num_char ) ## [1] \"character\" class ( char_logi ) ## [1] \"character\" class ( tricky ) ## [1] \"character\" R will automatically convert values in a vector so that they are all the same type, a process called coercion . How many values in combined_logical are \"TRUE\" (as a character)? combined_logical <- c ( num_logi , char_logi ) Challenge solution combined_logical ## [1] \"1\" \"4\" \"6\" \"1\" \"a\" \"b\" \"TRUE\" class ( combined_logical ) ## [1] \"character\" Only one value is \"TRUE\" . Coercion happens when each vector is created, so the TRUE in num_logi becomes a 1 , while the TRUE in char_logi becomes \"TRUE\" . When these two vectors are combined, R doesn't remember that the 1 in num_logi used to be a TRUE , it will just coerce the 1 to \"1\" . Now that you've seen a few examples of coercion, you might have started to see that there are some rules about how types get converted. There is a hierarchy to coercion. Can you draw a diagram that represents the hierarchy of what types get converted to other types? Challenge solution logical \u2192 integer \u2192 numeric \u2192 character Logical vectors can only take on two values: TRUE or FALSE . Integer vectors can only contain integers, so TRUE and FALSE can be coerced to 1 and 0 . Numeric vectors can contain numbers with decimals, so integers can be coerced from, say, 6 to 6.0 (though R will still display a numeric 6 as 6 .). Finally, any string of characters can be represented as a character vector, so any of the other types can be coerced to a character vector. Coercion most often happens when combining vectors when they contain different data types or reading data into R where a stray character may change an entire numeric vector into a character vector. Using the class() function can help confirm an object's class meets your expectations, particularly if you are running into confusing error messages.","title":"Vectors: the building block of data"},{"location":"arcadia-users-group/20231031-intro-to-r/lesson/#missing-data","text":"R represents missing data as NA , without quotes, in vectors of any type. The NA signals to R to treat that data differently than the rest of the entries in the vector. Let's make a numeric vector with an NA value: weights <- c ( 25 , 34 , 12 , NA , 42 ) By default, may R functions won't work when NA values are present; instead, if you use them they'll return NA themselves. min ( weights ) ## [1] NA This behavior protects the user from not considering missing data. If we decide to exclude our missing values, many basic math functions have an argument to r e m ove them: min ( weights , na.rm = TRUE ) ## [1] 12","title":"Missing data"},{"location":"arcadia-users-group/20231031-intro-to-r/lesson/#vectors-as-arguments","text":"A common reason to create a vector from scratch is to use in a function argument. For example, the quantile() function will calculate a quantile for a given vector of numeric values. We set the quantile using the probs argument. We also need to set na.rm = TRUE , since there are NA values in the weight column. quantile ( complete_old $ weight , probs = 0.25 , na.rm = TRUE ) ## 25% ## 24 Now we get back the 25% quantile value for weights. However, we often want to know more than one quantile. Luckily, the probs argument is vectorized , meaning it can take a whole vector of values. Let's try getting the 25%, 50% (median), and 75% quantiles all at once. quantile ( complete_old $ weight , probs = c ( 0.25 , 0.5 , 0.75 ), na.rm = TRUE ) ## 25% 50% 75% ## 24 42 53","title":"Vectors as arguments"},{"location":"arcadia-users-group/20231031-intro-to-r/lesson/#other-data-tyes-in-r","text":"We have now seen vectors in a few different forms: as columns in a data.frame and as single vectors. However, they can be manipulated into lots of other shapes and forms. Some other common forms are: matrices 2-dimensional numeric representations arrays many-dimensional numeric lists lists are very flexible ways to store vectors a list can contain vectors of many different types and lengths an entry in a list can be another list, so lists can get deeply nested a data.frame is a type of list where each column is an individual vector and each vector has to be the same length, since a data.frame has an entry in every column for each row factors a way to represent categorical data factors can be ordered or unordered they often look like character vectors, but behave differently under the hood, they are integers with character labels, called levels , for each integer We won't spend time on these data types in this lesson. If you want more information on these classes, you can read through this Software Carpentry lesson .","title":"Other data tyes in R"},{"location":"arcadia-users-group/20231031-intro-to-r/lesson/#working-with-data-in-r","text":"","title":"Working with data in R"},{"location":"arcadia-users-group/20231031-intro-to-r/lesson/#importing-data","text":"Up until this point, we have been working with the complete_old data.frame contained in the ratdat package. However, you typically won't access data from an R package; it is much more common to access data files stored somewhere on your computer. We are going to download a CSV file containing the surveys data to our computer, which we will then read into R. Click this link to download the file: https://github.com/Arcadia-Science/arcadia-computational-training/blob/main/docs/arcadia-users-group/20231031-intro-to-r/surveys_complete_77_89.csv . You will be prompted to save the file on your computer somewhere. Save it in your ~/Downloads folder so that we'll all be working with the same file path. More information on project organization and management. For this lesson, we aren't worrying about our file and folder organization. In general, it's best practice to keep your project organized in a specific folder. The exact organization strategy will depend on the project you're doing and the tools you choose to use. For more information on how to keep your project organized, a discussion of some best practices, and strategies you can use, see the lesson Project organization and file & resource management .","title":"Importing data"},{"location":"arcadia-users-group/20231031-intro-to-r/lesson/#file-paths","text":"When we reference other files from an R script, we need to give R precise instructions on where those files are. We do that using something called a file path . It looks something like this: \"~/Documents/Manuscripts/Chapter_2.txt\" . This path would tell your computer how to get from whatever folder contains the Documents folder all the way to the .txt file. There are two kinds of paths: absolute and relative . Absolute paths are specific to a particular computer, whereas relative paths are relative to a certain folder. For more information on fie paths and directory structures, see this lesson . Let's read our CSV file into R and store it in an object named surveys . We will use the read_csv function from the tidyverse 's readr package. surveys <- read_csv ( \"~/Downloads/surveys_complete_77_89.csv\" ) ## Rows: 16878 Columns: 13 ## \u2500\u2500 Column specification \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 ## Delimiter: \",\" ## chr (6): species_id, sex, genus, species, taxa, plot_type ## dbl (7): record_id, month, day, year, plot_id, hindfoot_length, weight ## ## \u2139 Use `spec()` to retrieve the full column specification for this data. ## \u2139 Specify the column types or set `show_col_types = FALSE` to quiet this message. Using tab completion to solve the file path. Typing out paths can be error prone, so we can utilize a keyboard shortcut. Just like functions and other objects in RStudio, you can use tab completion on file paths. Inside the parentheses of read_csv() , type out a pair of quotes and put your cursor between them. Then hit Tab . A small menu showing your folders and files should show up. You can use the \u2191 and \u2193 keys to move through the options, or start typing to narrow them down. You can hit Enter to select a file or folder, and hit Tab again to continue building the file path. This might take a bit of getting used to, but once you get the hang of it, it will speed up writing file paths and reduce the number of mistakes you make. By default, the read_csv() function prints information about the files it reads to the Console. We got some useful information about the CSV file we read in. We can see: the number of rows and columns the delimiter of the file, which is how values are separated, a comma \",\" a set of columns that were parsed as various vector types the file has 6 character columns and 7 numeric columns we can see the names of the columns for each type When working with the output of a new function, it's often a good idea to check the class() : class ( surveys ) ## [1] \"spec_tbl_df\" \"tbl_df\" \"tbl\" \"data.frame\" Whoa! What is this thing? It has multiple classes? Well, it's called a tibble , and it is the tidyverse version of a data.frame. It is a data.frame, but with some added perks. It prints out a little more nicely, it highlights NA values and negative values in red, and it will generally communicate with you more (in terms of warnings and errors, which is a good thing). The difference between tidyverse and base R As we begin to delve more deeply into the tidyverse , we should briefly pause to mention some of the reasons for focusing on the tidyverse set of tools. In R, there are often many ways to get a job done, and there are other approaches that can accomplish tasks similar to the tidyverse . The phrase base R is used to refer to approaches that utilize functions contained in R's default packages. We have already used some base R functions, such as str() , head() , and mean() , and we will be using more scattered throughout this lesson. However, this lesson won't cover some functionalities in base R such as sub-setting with square bracket notation and base plotting. You may come across code written by other people that looks like surveys[1:10, 2] or plot(surveys$weight, surveys$hindfoot_length) , which are base R commands. If you're interested in learning more about these approaches, you can check out other Carpentries lessons like the Software Carpentry Programming with R lesson. We choose to teach the tidyverse set of packages because they share a similar syntax and philosophy, making them consistent and producing highly readable code. They are also very flexible and powerful, with a growing number of packages designed according to similar principles and to work well with the rest of the packages. The tidyverse packages tend to have very clear documentation and wide array of learning materials that tend to be written with novice users in mind. Finally, the tidyverse has only continued to grow, and has strong support from Posti (the RStudio parent company), which implies that these approaches will be relevant into the future.","title":"File paths"},{"location":"arcadia-users-group/20231031-intro-to-r/lesson/#manipulating-data","text":"One of the most important skills for working with data in R is the ability to manipulate, modify, and reshape data. The dplyr and tidyr packages in the tidyverse provide a series of powerful functions for many common data manipulation tasks. We'll start off with two of the most commonly used dplyr functions: select() , which selects certain columns of a data.frame, and filter() , which filters out rows according to certain criteria. Between select() and filter() , it can be hard to remember which operates on columns and which operates on rows. sele c t() has a c for c olumns and filte r () has an r for r ows.","title":"Manipulating data"},{"location":"arcadia-users-group/20231031-intro-to-r/lesson/#select","text":"To use the select() function, the first argument is the name of the data.frame, and the rest of the arguments are unquoted names of the columns you want: select ( surveys , plot_id , species_id , hindfoot_length ) ## # A tibble: 16,878 \u00d7 3 ## plot_id species_id hindfoot_length ## <dbl> <chr> <dbl> ## 1 2 NL 32 ## 2 3 NL 33 ## 3 2 DM 37 ## 4 7 DM 36 ## 5 3 DM 35 ## 6 1 PF 14 ## 7 2 PE NA ## 8 1 DM 37 ## 9 1 DM 34 ## 10 6 PF 20 ## # \u2139 16,868 more rows The columns are arranged in the order we specified inside select() . To select all columns except specific columns, put a - in front of the column you want to exclude: select ( surveys , - record_id , - year ) ## # A tibble: 16,878 \u00d7 11 ## month day plot_id species_id sex hindfoot_length weight genus species ## <dbl> <dbl> <dbl> <chr> <chr> <dbl> <dbl> <chr> <chr> ## 1 7 16 2 NL M 32 NA Neotoma albigu\u2026 ## 2 7 16 3 NL M 33 NA Neotoma albigu\u2026 ## 3 7 16 2 DM F 37 NA Dipodomys merria\u2026 ## 4 7 16 7 DM M 36 NA Dipodomys merria\u2026 ## 5 7 16 3 DM M 35 NA Dipodomys merria\u2026 ## 6 7 16 1 PF M 14 NA Perognat\u2026 flavus ## 7 7 16 2 PE F NA NA Peromysc\u2026 eremic\u2026 ## 8 7 16 1 DM M 37 NA Dipodomys merria\u2026 ## 9 7 16 1 DM F 34 NA Dipodomys merria\u2026 ## 10 7 16 6 PF F 20 NA Perognat\u2026 flavus ## # \u2139 16,868 more rows ## # \u2139 2 more variables: taxa <chr>, plot_type <chr>","title":"select()"},{"location":"arcadia-users-group/20231031-intro-to-r/lesson/#filter","text":"The filter() function is used to select rows that meet certain criteria. To get all the rows where the value of year is equal to 1985, we would run the following: filter ( surveys , year == 1985 ) ## # A tibble: 1,438 \u00d7 13 ## record_id month day year plot_id species_id sex hindfoot_length weight ## <dbl> <dbl> <dbl> <dbl> <dbl> <chr> <chr> <dbl> <dbl> ## 1 9790 1 19 1985 16 RM F 16 4 ## 2 9791 1 19 1985 17 OT F 20 16 ## 3 9792 1 19 1985 6 DO M 35 48 ## 4 9793 1 19 1985 12 DO F 35 40 ## 5 9794 1 19 1985 24 RM M 16 4 ## 6 9795 1 19 1985 12 DO M 34 48 ## 7 9796 1 19 1985 6 DM F 37 35 ## 8 9797 1 19 1985 14 DM M 36 45 ## 9 9798 1 19 1985 6 DM F 36 38 ## 10 9799 1 19 1985 19 RM M 16 4 ## # \u2139 1,428 more rows ## # \u2139 4 more variables: genus <chr>, species <chr>, taxa <chr>, plot_type <chr> The == sign means \"is equal to.\" There are several other operators we can use: >, >=, <, <=, and != (not equal to). Another useful operator is %in% , which asks if the value on the left hand side is found anywhere in the vector on the right hand side. For example, to get rows with specific species_id values, we could run: filter ( surveys , species_id %in% c ( \"RM\" , \"DO\" )) ## # A tibble: 2,835 \u00d7 13 ## record_id month day year plot_id species_id sex hindfoot_length weight ## <dbl> <dbl> <dbl> <dbl> <dbl> <chr> <chr> <dbl> <dbl> ## 1 68 8 19 1977 8 DO F 32 52 ## 2 292 10 17 1977 3 DO F 36 33 ## 3 294 10 17 1977 3 DO F 37 50 ## 4 311 10 17 1977 19 RM M 18 13 ## 5 317 10 17 1977 17 DO F 32 48 ## 6 323 10 17 1977 17 DO F 33 31 ## 7 337 10 18 1977 8 DO F 35 41 ## 8 356 11 12 1977 1 DO F 32 44 ## 9 378 11 12 1977 1 DO M 33 48 ## 10 397 11 13 1977 17 RM F 16 7 ## # \u2139 2,825 more rows ## # \u2139 4 more variables: genus <chr>, species <chr>, taxa <chr>, plot_type <chr> We can also use multiple conditions in one filter() statement. Here we will get rows with a year less than or equal to 1988 and whose hindfoot length values are not NA . The ! before the is.na() function means \"not\". filter ( surveys , year <= 1988 & ! is.na ( hindfoot_length )) ## # A tibble: 12,779 \u00d7 13 ## record_id month day year plot_id species_id sex hindfoot_length weight ## <dbl> <dbl> <dbl> <dbl> <dbl> <chr> <chr> <dbl> <dbl> ## 1 1 7 16 1977 2 NL M 32 NA ## 2 2 7 16 1977 3 NL M 33 NA ## 3 3 7 16 1977 2 DM F 37 NA ## 4 4 7 16 1977 7 DM M 36 NA ## 5 5 7 16 1977 3 DM M 35 NA ## 6 6 7 16 1977 1 PF M 14 NA ## 7 8 7 16 1977 1 DM M 37 NA ## 8 9 7 16 1977 1 DM F 34 NA ## 9 10 7 16 1977 6 PF F 20 NA ## 10 11 7 16 1977 5 DS F 53 NA ## # \u2139 12,769 more rows ## # \u2139 4 more variables: genus <chr>, species <chr>, taxa <chr>, plot_type <chr> Challenge 2: Filtering and selecting Use the surveys data to make a data.frame that has only data with years from 1980 to 1985. Challenge solution surveys_filtered <- filter ( surveys , year >= 1980 & year <= 1985 ) Use the surveys data to make a data.frame that has only the following columns, in order: year , month , species_id , plot_id . Challenge solution surveys_selected <- select ( surveys , year , month , species_id , plot_id )","title":"filter()"},{"location":"arcadia-users-group/20231031-intro-to-r/lesson/#the-pipe","text":"What happens if we want to both select() and filter() our data? We have a couple options. For example, we can create intermediate objects: surveys_noday <- select ( surveys , - day ) filter ( surveys_noday , month >= 7 ) ## # A tibble: 8,244 \u00d7 12 ## record_id month year plot_id species_id sex hindfoot_length weight genus ## <dbl> <dbl> <dbl> <dbl> <chr> <chr> <dbl> <dbl> <chr> ## 1 1 7 1977 2 NL M 32 NA Neotoma ## 2 2 7 1977 3 NL M 33 NA Neotoma ## 3 3 7 1977 2 DM F 37 NA Dipodo\u2026 ## 4 4 7 1977 7 DM M 36 NA Dipodo\u2026 ## 5 5 7 1977 3 DM M 35 NA Dipodo\u2026 ## 6 6 7 1977 1 PF M 14 NA Perogn\u2026 ## 7 7 7 1977 2 PE F NA NA Peromy\u2026 ## 8 8 7 1977 1 DM M 37 NA Dipodo\u2026 ## 9 9 7 1977 1 DM F 34 NA Dipodo\u2026 ## 10 10 7 1977 6 PF F 20 NA Perogn\u2026 ## # \u2139 8,234 more rows ## # \u2139 3 more variables: species <chr>, taxa <chr>, plot_type <chr> This approach accumulates a lot of intermediate objects, often with confusing names and without clear relationships between those objects. An elegant solution to this problem is an operator called the pipe , which looks like %>% . You can insert it by using the keyboard shortcut Shift+Cmd+M (Mac) or Shift+Ctrl+M (Windows). Here's how you could use a pipe to select and filter in one step: surveys %>% select ( - day ) %>% filter ( month >= 7 ) ## # A tibble: 8,244 \u00d7 12 ## record_id month year plot_id species_id sex hindfoot_length weight genus ## <dbl> <dbl> <dbl> <dbl> <chr> <chr> <dbl> <dbl> <chr> ## 1 1 7 1977 2 NL M 32 NA Neotoma ## 2 2 7 1977 3 NL M 33 NA Neotoma ## 3 3 7 1977 2 DM F 37 NA Dipodo\u2026 ## 4 4 7 1977 7 DM M 36 NA Dipodo\u2026 ## 5 5 7 1977 3 DM M 35 NA Dipodo\u2026 ## 6 6 7 1977 1 PF M 14 NA Perogn\u2026 ## 7 7 7 1977 2 PE F NA NA Peromy\u2026 ## 8 8 7 1977 1 DM M 37 NA Dipodo\u2026 ## 9 9 7 1977 1 DM F 34 NA Dipodo\u2026 ## 10 10 7 1977 6 PF F 20 NA Perogn\u2026 ## # \u2139 8,234 more rows ## # \u2139 3 more variables: species <chr>, taxa <chr>, plot_type <chr> What it does is take the thing on the left hand side and insert it as the first argument of the function on the right hand side. By putting each of our functions onto a new line, we can build a nice, readable chunk of code. It can be useful to think of this as a little assembly line for our data. It starts at the top and gets piped into a select() function, and it comes out modified somewhat. It then gets sent into the filter() function, where it is further modified, and then the final product gets printed out to our console. It can also be helpful to think of %>% as meaning \"and then\". Since many tidyverse functions have verbs for names, these chunks of code can be read like a sentence. If we want to store this final product as an object, we use an assignment arrow at the start: surveys_sub <- surveys %>% select ( - day ) %>% filter ( month >= 7 ) One approach is to build a piped together code chunk step by step prior to assignment. You add functions to the chunk as you go, with the results printing in the console for you to view. Once you're satisfied with your final result, go back and add the assignment arrow statement at the start. This approach is very interactive, allowing you to see the results of each step as you build the chunk, and produces nicely readable code. Challenge 3: Using pipes Use the surveys data to make a data.frame that has the columns record_id , month , and species_id , with data from the year 1988. Use a pipe between the function calls. Challenge solution surveys_1988 <- surveys %>% filter ( year == 1988 ) %>% select ( record_id , month , species_id ) Make sure to `filter()` before you `select()`; you need to use the `year` column for filtering rows, but it is discarded in the `select()` step. You also need to make sure to use `==` instead of `=` when you are filtering rows where `year` is equal to 1988.","title":"The pipe: %&gt;%"},{"location":"arcadia-users-group/20231031-intro-to-r/lesson/#making-new-columns-with-mutate","text":"Another common task is creating a new column based on values in existing columns. For example, we could add a new column that has the weight in kilograms instead of grams: surveys %>% mutate ( weight_kg = weight / 1000 ) ## # A tibble: 16,878 \u00d7 14 ## record_id month day year plot_id species_id sex hindfoot_length weight ## <dbl> <dbl> <dbl> <dbl> <dbl> <chr> <chr> <dbl> <dbl> ## 1 1 7 16 1977 2 NL M 32 NA ## 2 2 7 16 1977 3 NL M 33 NA ## 3 3 7 16 1977 2 DM F 37 NA ## 4 4 7 16 1977 7 DM M 36 NA ## 5 5 7 16 1977 3 DM M 35 NA ## 6 6 7 16 1977 1 PF M 14 NA ## 7 7 7 16 1977 2 PE F NA NA ## 8 8 7 16 1977 1 DM M 37 NA ## 9 9 7 16 1977 1 DM F 34 NA ## 10 10 7 16 1977 6 PF F 20 NA ## # \u2139 16,868 more rows ## # \u2139 5 more variables: genus <chr>, species <chr>, taxa <chr>, plot_type <chr>, ## # weight_kg <dbl> You can create multiple columns in one mutate() call, and they will get created in the order you write them. This means you can even reference the first new column in the second new column: surveys %>% mutate ( weight_kg = weight / 1000 , weight_lbs = weight_kg * 2.2 ) ## # A tibble: 16,878 \u00d7 15 ## record_id month day year plot_id species_id sex hindfoot_length weight ## <dbl> <dbl> <dbl> <dbl> <dbl> <chr> <chr> <dbl> <dbl> ## 1 1 7 16 1977 2 NL M 32 NA ## 2 2 7 16 1977 3 NL M 33 NA ## 3 3 7 16 1977 2 DM F 37 NA ## 4 4 7 16 1977 7 DM M 36 NA ## 5 5 7 16 1977 3 DM M 35 NA ## 6 6 7 16 1977 1 PF M 14 NA ## 7 7 7 16 1977 2 PE F NA NA ## 8 8 7 16 1977 1 DM M 37 NA ## 9 9 7 16 1977 1 DM F 34 NA ## 10 10 7 16 1977 6 PF F 20 NA ## # \u2139 16,868 more rows ## # \u2139 6 more variables: genus <chr>, species <chr>, taxa <chr>, plot_type <chr>, ## # weight_kg <dbl>, weight_lbs <dbl>","title":"Making new columns with mutate()"},{"location":"arcadia-users-group/20231031-intro-to-r/lesson/#the-split-apply-combine-approach","text":"Many data analysis tasks can be achieved using the split-apply-combine approach: you split the data into groups, apply some analysis to each group, and combine the results in some way. dplyr has a few convenient functions to enable this approach, the main two being group_by() and summarize() . group_by() takes a data.frame and the name of one or more columns with categorical values that define the groups. summarize() then collapses each group into a one-row summary of the group, giving you back a data.frame with one row per group. The syntax for summarize() is similar to mutate() , where you define new columns based on values of other columns. Let's try calculating the mean weight of all our animals by sex. surveys %>% group_by ( sex ) %>% summarize ( mean_weight = mean ( weight , na.rm = T )) ## # A tibble: 3 \u00d7 2 ## sex mean_weight ## <chr> <dbl> ## 1 F 53.1 ## 2 M 53.2 ## 3 <NA> 74.0 You can see that the mean weight for males is slightly higher than for females, but that animals whose sex is unknown have much higher weights. This is probably due to small sample size, but we should check to be sure. Like mutate() , we can define multiple columns in one summarize() call. The function n() will count the number of rows in each group. surveys %>% group_by ( sex ) %>% summarize ( mean_weight = mean ( weight , na.rm = T ), n = n ()) ## # A tibble: 3 \u00d7 3 ## sex mean_weight n ## <chr> <dbl> <int> ## 1 F 53.1 7318 ## 2 M 53.2 8260 ## 3 <NA> 74.0 1300 You will often want to create groups based on multiple columns. For example, we might be interested in the mean weight of every species + sex combination. All we have to do is add another column to our group_by() call. surveys %>% group_by ( species_id , sex ) %>% summarize ( mean_weight = mean ( weight , na.rm = T ), n = n ()) ## `summarise()` has grouped output by 'species_id'. You can override using the ## `.groups` argument. ## # A tibble: 67 \u00d7 4 ## # Groups: species_id [36] ## species_id sex mean_weight n ## <chr> <chr> <dbl> <int> ## 1 AB <NA> NaN 223 ## 2 AH <NA> NaN 136 ## 3 BA M 7 3 ## 4 CB <NA> NaN 23 ## 5 CM <NA> NaN 13 ## 6 CQ <NA> NaN 16 ## 7 CS <NA> NaN 1 ## 8 CV <NA> NaN 1 ## 9 DM F 40.7 2522 ## 10 DM M 44.0 3108 ## # \u2139 57 more rows Our resulting data.frame is much larger, since we have a greater number of groups. We also see a strange value showing up in our mean_weight column: NaN . This stands for \"Not a Number\", and it often results from trying to do an operation a vector with zero entries. How can a vector have zero entries? Well, if a particular group (like the AB species ID + NA sex group) has only NA values for weight, then the na.rm = T argument in mean() will remove all the values prior to calculating the mean. The result will be a value of NaN . Since we are not particularly interested in these values, let's add a step to our pipeline to remove rows where weight is NA before doing any other steps. This means that any groups with only NA values will disappear from our data.frame before we formally create the groups with group_by() . surveys %>% filter ( ! is.na ( weight )) %>% group_by ( species_id , sex ) %>% summarize ( mean_weight = mean ( weight ), n = n ()) ## `summarise()` has grouped output by 'species_id'. You can override using the ## `.groups` argument. ## # A tibble: 46 \u00d7 4 ## # Groups: species_id [18] ## species_id sex mean_weight n ## <chr> <chr> <dbl> <int> ## 1 BA M 7 3 ## 2 DM F 40.7 2460 ## 3 DM M 44.0 3013 ## 4 DM <NA> 37 8 ## 5 DO F 48.4 679 ## 6 DO M 49.3 748 ## 7 DO <NA> 44 1 ## 8 DS F 118. 1055 ## 9 DS M 123. 1184 ## 10 DS <NA> 121. 16 ## # \u2139 36 more rows That looks better! It's often useful to take a look at the results in some order, like the lowest mean weight to highest. We can use the arrange() function for that: surveys %>% filter ( ! is.na ( weight )) %>% group_by ( species_id , sex ) %>% summarize ( mean_weight = mean ( weight ), n = n ()) %>% arrange ( mean_weight ) ## `summarise()` has grouped output by 'species_id'. You can override using the ## `.groups` argument. ## # A tibble: 46 \u00d7 4 ## # Groups: species_id [18] ## species_id sex mean_weight n ## <chr> <chr> <dbl> <int> ## 1 PF <NA> 6 2 ## 2 BA M 7 3 ## 3 PF F 7.09 215 ## 4 PF M 7.10 296 ## 5 RM M 9.92 678 ## 6 RM <NA> 10.4 7 ## 7 RM F 10.7 629 ## 8 RF M 12.4 16 ## 9 RF F 13.7 46 ## 10 PP <NA> 15 2 ## # \u2139 36 more rows If we want to reverse the order, we can wrap the column name in desc() : surveys %>% filter ( ! is.na ( weight )) %>% group_by ( species_id , sex ) %>% summarize ( mean_weight = mean ( weight ), n = n ()) %>% arrange ( desc ( mean_weight )) ## `summarise()` has grouped output by 'species_id'. You can override using the ## `.groups` argument. ## # A tibble: 46 \u00d7 4 ## # Groups: species_id [18] ## species_id sex mean_weight n ## <chr> <chr> <dbl> <int> ## 1 NL M 168. 355 ## 2 NL <NA> 164. 9 ## 3 NL F 151. 460 ## 4 SS M 130 1 ## 5 DS M 123. 1184 ## 6 DS <NA> 121. 16 ## 7 DS F 118. 1055 ## 8 SH F 79.2 61 ## 9 SH M 67.6 34 ## 10 SF F 58.3 3 ## # \u2139 36 more rows You may have seen several messages saying `summarise() has grouped output by 'species_id'. These are warning you that your resulting data.frame has retained some group structure, which means any subsequent operations on that data.frame will happen at the group level. If you look at the resulting data.frame printed out in your console, you will see these lines: # A tibble: 46 \u00d7 4 # Groups: species_id [18] They tell us we have a data.frame with 46 rows, 4 columns, and a group variable species_id , for which there are 18 groups. We will see something similar if we use group_by() alone: surveys %>% group_by ( species_id , sex ) ## # A tibble: 16,878 \u00d7 13 ## # Groups: species_id, sex [67] ## record_id month day year plot_id species_id sex hindfoot_length weight ## <dbl> <dbl> <dbl> <dbl> <dbl> <chr> <chr> <dbl> <dbl> ## 1 1 7 16 1977 2 NL M 32 NA ## 2 2 7 16 1977 3 NL M 33 NA ## 3 3 7 16 1977 2 DM F 37 NA ## 4 4 7 16 1977 7 DM M 36 NA ## 5 5 7 16 1977 3 DM M 35 NA ## 6 6 7 16 1977 1 PF M 14 NA ## 7 7 7 16 1977 2 PE F NA NA ## 8 8 7 16 1977 1 DM M 37 NA ## 9 9 7 16 1977 1 DM F 34 NA ## 10 10 7 16 1977 6 PF F 20 NA ## # \u2139 16,868 more rows ## # \u2139 4 more variables: genus <chr>, species <chr>, taxa <chr>, plot_type <chr> What we get back is the entire surveys data.frame, but with the grouping variables added: 67 groups of species_id + sex combinations. Groups are often maintained throughout a pipeline, and if you assign the resulting data.frame to a new object, it will also have those groups. This can lead to confusing results if you forget about the grouping and want to carry out operations on the whole data.frame, not by group. Therefore, it is a good habit to remove the groups at the end of a chunk of piped code containing group_by() : surveys %>% filter ( ! is.na ( weight )) %>% group_by ( species_id , sex ) %>% summarize ( mean_weight = mean ( weight ), n = n ()) %>% arrange ( desc ( mean_weight )) %>% ungroup () ## `summarise()` has grouped output by 'species_id'. You can override using the ## `.groups` argument. ## # A tibble: 46 \u00d7 4 ## species_id sex mean_weight n ## <chr> <chr> <dbl> <int> ## 1 NL M 168. 355 ## 2 NL <NA> 164. 9 ## 3 NL F 151. 460 ## 4 SS M 130 1 ## 5 DS M 123. 1184 ## 6 DS <NA> 121. 16 ## 7 DS F 118. 1055 ## 8 SH F 79.2 61 ## 9 SH M 67.6 34 ## 10 SF F 58.3 3 ## # \u2139 36 more rows Now our data.frame just says # A tibble: 46 \u00d7 4 at the top, with no groups. Challenge 4: Making a time series Use the split-apply-combine approach to make a data.frame that counts the total number of animals of each sex caught on each day in the surveys data. Challenge solution surveys_daily_counts <- surveys %>% mutate ( date = ymd ( paste ( year , month , day , sep = \"-\" ))) %>% group_by ( date , sex ) %>% summarize ( n = n ()) ## `summarise()` has grouped output by 'date'. You can override using the ## `.groups` argument. # shorter approach using count() surveys_daily_counts <- surveys %>% mutate ( date = ymd ( paste ( year , month , day , sep = \"-\" ))) %>% count ( date , sex ) Now use the data.frame you just made to plot the daily number of animals of each sex caught over time. It's up to you what geom to use, but a line plot might be a good choice. You should also think about how to differentiate which data corresponds to which sex. Challenge solution surveys_daily_counts %>% ggplot ( aes ( x = date , y = n , color = sex )) + geom_line () ![](lesson_files/figure-html/time-series-challenge-answer-1.png)","title":"The split-apply-combine approach"},{"location":"arcadia-users-group/20231031-intro-to-r/lesson/#joining-data-together-with-the-join-functions","text":"Often times, all the data you need to do a project might be contained in multiple CSV files. When you read those files into R, they will be in separate data.frames. To combine two different data.frames that both contain at least one shared column of information, you can use the dplyr *join() commands. We'll practice a *join() command by downloading and reading in a CSV file that annotates the plot_type for each plot in our surveys data.frame. download.file ( \"https://ndownloader.figshare.com/files/3299474\" , \"plots.csv\" ) plots <- read_csv ( \"plots.csv\" ) ## Rows: 24 Columns: 2 ## \u2500\u2500 Column specification \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 ## Delimiter: \",\" ## chr (1): plot_type ## dbl (1): plot_id ## ## \u2139 Use `spec()` to retrieve the full column specification for this data. ## \u2139 Specify the column types or set `show_col_types = FALSE` to quiet this message. head ( plots ) ## # A tibble: 6 \u00d7 2 ## plot_id plot_type ## <dbl> <chr> ## 1 1 Spectab exclosure ## 2 2 Control ## 3 3 Long-term Krat Exclosure ## 4 4 Control ## 5 5 Rodent Exclosure ## 6 6 Short-term Krat Exclosure There are multiple join commands: left_join() , right_join() , inner_join() , and full_join() . We'll practice doing a left_join() : for two data.frames, keep all rows in the left data.frame even if this introduces NA values because there is no corresponding information in the right data.frame. left_join ( surveys , plots , by = \"plot_id\" ) ## # A tibble: 16,878 \u00d7 14 ## record_id month day year plot_id species_id sex hindfoot_length weight ## <dbl> <dbl> <dbl> <dbl> <dbl> <chr> <chr> <dbl> <dbl> ## 1 1 7 16 1977 2 NL M 32 NA ## 2 2 7 16 1977 3 NL M 33 NA ## 3 3 7 16 1977 2 DM F 37 NA ## 4 4 7 16 1977 7 DM M 36 NA ## 5 5 7 16 1977 3 DM M 35 NA ## 6 6 7 16 1977 1 PF M 14 NA ## 7 7 7 16 1977 2 PE F NA NA ## 8 8 7 16 1977 1 DM M 37 NA ## 9 9 7 16 1977 1 DM F 34 NA ## 10 10 7 16 1977 6 PF F 20 NA ## # \u2139 16,868 more rows ## # \u2139 5 more variables: genus <chr>, species <chr>, taxa <chr>, ## # plot_type.x <chr>, plot_type.y <chr> In this command, we tell R to join the two data.frames by the plot_id column. In order for the two data.frames to be combined, they must share the same value in the plot_id column. However, when we look at the output, we see that we've introduced new columns, plot_type.x and plot_typs.y . It turns out we already had this information in our data.frame and didn't need to do a join. When we did, we only specified that we wanted to join on plot_id , but both data.frames shared the plot_type variable name, so the left_join() functions appended the suffixes .x and .y . Instead, let's join on two column names: left_join ( surveys , plots , by = c ( \"plot_id\" , \"plot_type\" )) ## # A tibble: 16,878 \u00d7 13 ## record_id month day year plot_id species_id sex hindfoot_length weight ## <dbl> <dbl> <dbl> <dbl> <dbl> <chr> <chr> <dbl> <dbl> ## 1 1 7 16 1977 2 NL M 32 NA ## 2 2 7 16 1977 3 NL M 33 NA ## 3 3 7 16 1977 2 DM F 37 NA ## 4 4 7 16 1977 7 DM M 36 NA ## 5 5 7 16 1977 3 DM M 35 NA ## 6 6 7 16 1977 1 PF M 14 NA ## 7 7 7 16 1977 2 PE F NA NA ## 8 8 7 16 1977 1 DM M 37 NA ## 9 9 7 16 1977 1 DM F 34 NA ## 10 10 7 16 1977 6 PF F 20 NA ## # \u2139 16,868 more rows ## # \u2139 4 more variables: genus <chr>, species <chr>, taxa <chr>, plot_type <chr> Now no suffixes are added to our column names.","title":"Joining data together with the *join() functions"},{"location":"arcadia-users-group/20231031-intro-to-r/lesson/#exporting-data","text":"Let's say we want to send the wide version of our surveys_1988 data.frame to a colleague who doesn't use R. In this case, we might want to save it as a CSV file. We can save this data.frame to a CSV using the write_csv() function from the readr package. The first argument is the name of the data.frame, and the second is the path to the new file we want to create, including the file extension .csv . write_csv ( surveys_1988 , \"~/Downloads/surveys_1988.csv\" ) If we go look into our Downloads folder, we will see this new CSV file.","title":"Exporting data"},{"location":"arcadia-users-group/20231031-intro-to-r/lesson/#extras","text":"","title":"Extras"},{"location":"arcadia-users-group/20231031-intro-to-r/lesson/#building-vectors-from-scratch","text":"While the c() function is very flexible, it doesn't necessarily scale well. If you want to generate a long vector from scratch, you probably don't want to type everything out manually. There are a few functions that can help generate vectors. First, putting : between two numbers will generate a vector of integers starting with the first number and ending with the last. The seq() function allows you to generate similar sequences, but changing by any amount. # generates a sequence of integers 1 : 10 ## [1] 1 2 3 4 5 6 7 8 9 10 # with seq() you can generate sequences with a combination of: # from: starting value # to: ending value # by: how much should each entry increase # length.out: how long should the resulting vector be seq ( from = 0 , to = 1 , by = 0.1 ) ## [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 seq ( from = 0 , to = 1 , length.out = 50 ) ## [1] 0.00000000 0.02040816 0.04081633 0.06122449 0.08163265 0.10204082 ## [7] 0.12244898 0.14285714 0.16326531 0.18367347 0.20408163 0.22448980 ## [13] 0.24489796 0.26530612 0.28571429 0.30612245 0.32653061 0.34693878 ## [19] 0.36734694 0.38775510 0.40816327 0.42857143 0.44897959 0.46938776 ## [25] 0.48979592 0.51020408 0.53061224 0.55102041 0.57142857 0.59183673 ## [31] 0.61224490 0.63265306 0.65306122 0.67346939 0.69387755 0.71428571 ## [37] 0.73469388 0.75510204 0.77551020 0.79591837 0.81632653 0.83673469 ## [43] 0.85714286 0.87755102 0.89795918 0.91836735 0.93877551 0.95918367 ## [49] 0.97959184 1.00000000 seq ( from = 0 , by = 0.01 , length.out = 20 ) ## [1] 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.10 0.11 0.12 0.13 0.14 ## [16] 0.15 0.16 0.17 0.18 0.19 Finally, the rep() function allows you to repeat a value, or even a whole vector, as many times as you want, and works with any type of vector. # repeats \"a\" 12 times rep ( \"a\" , times = 12 ) ## [1] \"a\" \"a\" \"a\" \"a\" \"a\" \"a\" \"a\" \"a\" \"a\" \"a\" \"a\" \"a\" # repeats this whole sequence 4 times rep ( c ( \"a\" , \"b\" , \"c\" ), times = 4 ) ## [1] \"a\" \"b\" \"c\" \"a\" \"b\" \"c\" \"a\" \"b\" \"c\" \"a\" \"b\" \"c\" # repeats each value 4 times rep ( 1 : 10 , each = 4 ) ## [1] 1 1 1 1 2 2 2 2 3 3 3 3 4 4 4 4 5 5 5 5 6 6 6 6 7 ## [26] 7 7 7 8 8 8 8 9 9 9 9 10 10 10 10 Extra Challenge : Creating sequences Write some code to generate the following vector: ## [1] -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 Challenge solution rep ( -3 : 3 , 3 ) ## [1] -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 # this also works rep ( seq ( from = -3 , to = 3 , by = 1 ), 3 ) ## [1] -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 # you might also store the sequence as an intermediate vector my_seq <- seq ( from = -3 , to = 3 , by = 1 ) rep ( my_seq , 3 ) ## [1] -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 Calculate the quantiles for the complete_old hindfoot lengths at every 5% level (0%, 5%, 10%, 15%, etc.) Challenge solution quantile ( complete_old $ hindfoot_length , probs = seq ( from = 0 , to = 1 , by = 0.05 ), na.rm = T ) ## 0% 5% 10% 15% 20% 25% 30% 35% 40% 45% 50% 55% 60% 65% 70% 75% ## 6 16 17 19 20 21 22 31 33 34 35 35 36 36 36 37 ## 80% 85% 90% 95% 100% ## 37 39 49 51 70","title":"Building vectors from scratch"},{"location":"arcadia-users-group/20231031-intro-to-r/lesson/#an-explanation-and-exploration-of-factors","text":"We will spend a bit more time talking about factors, since they are often a challenging type of data to work with. We can create a factor from scratch by putting a character vector made using c() into the factor() function: sex <- factor ( c ( \"male\" , \"female\" , \"female\" , \"male\" , \"female\" , NA )) sex ## [1] male female female male female <NA> ## Levels: female male We can inspect the levels of the factor using the levels() function: levels ( sex ) ## [1] \"female\" \"male\" In general, it is a good practice to leave your categorical data as a character vector until you need to use a factor. Here are some reasons you might need a factor: Another function requires you to use a factor You are plotting categorical data and want to control the ordering of categories in the plot Since factors can behave differently from character vectors, it is always a good idea to check what type of data you're working with. You might use a new function for the first time and be confused by the results, only to realize later that it produced a factor as an output, when you thought it was a character vector. You can convert a factor to a character vector using the as.character() function: as.character ( sex ) ## [1] \"male\" \"female\" \"female\" \"male\" \"female\" NA However, you need to be careful if you're somehow working with a factor that has numbers as its levels: f_num <- factor ( c ( 1990 , 1983 , 1977 , 1998 , 1990 )) # this will pull out the underlying integers, not the levels as.numeric ( f_num ) ## [1] 3 2 1 4 3 # if we first convert to characters, we can then convert to numbers as.numeric ( as.character ( f_num )) ## [1] 1990 1983 1977 1998 1990","title":"An explanation and exploration of factors"},{"location":"arcadia-users-group/20231031-intro-to-r/lesson/#reshaping-data-with-tidyr","text":"Let's say we are interested in comparing the mean weights of each species across our different plots. We can begin this process using the group_by() + summarize() approach: sp_by_plot <- surveys %>% filter ( ! is.na ( weight )) %>% group_by ( species_id , plot_id ) %>% summarise ( mean_weight = mean ( weight )) %>% arrange ( species_id , plot_id ) ## `summarise()` has grouped output by 'species_id'. You can override using the ## `.groups` argument. sp_by_plot ## # A tibble: 300 \u00d7 3 ## # Groups: species_id [18] ## species_id plot_id mean_weight ## <chr> <dbl> <dbl> ## 1 BA 3 8 ## 2 BA 21 6.5 ## 3 DM 1 42.7 ## 4 DM 2 42.6 ## 5 DM 3 41.2 ## 6 DM 4 41.9 ## 7 DM 5 42.6 ## 8 DM 6 42.1 ## 9 DM 7 43.2 ## 10 DM 8 43.4 ## # \u2139 290 more rows That looks great, but it is a bit difficult to compare values across plots. It would be nice if we could reshape this data.frame to make those comparisons easier Well, the tidyr package from the tidyverse has a pair of functions that allow you to reshape data by pivoting it: pivot_wider() and pivot_longer() . pivot_wider() will make the data wider, which means increasing the number of columns and reducing the number of rows. pivot_longer() will do the opposite, reducing the number of columns and increasing the number of rows. In this case, it might be nice to create a data.frame where each species has its own row, and each plot has its own column containing the mean weight for a given species. We will use pivot_wider() to reshape our data in this way. It takes 3 arguments: the name of the data.frame names_from : which column should be used to generate the names of the new columns? values_from : which column should be used to fill in the values of the new columns? Any columns not used for names_from or values_from will not be pivoted. {alt='Diagram depicting the behavior of pivot_wider() on a small tabular dataset.'} In our case, we want the new columns to be named from our plot_id column, with the values coming from the mean_weight column. We can pipe our data.frame right into pivot_wider() and add those two arguments: sp_by_plot_wide <- sp_by_plot %>% pivot_wider ( names_from = plot_id , values_from = mean_weight ) sp_by_plot_wide ## # A tibble: 18 \u00d7 25 ## # Groups: species_id [18] ## species_id `3` `21` `1` `2` `4` `5` `6` `7` `8` ## <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> ## 1 BA 8 6.5 NA NA NA NA NA NA NA ## 2 DM 41.2 41.5 42.7 42.6 41.9 42.6 42.1 43.2 43.4 ## 3 DO 42.7 NA 50.1 50.3 46.8 50.4 49.0 52 49.2 ## 4 DS 128. NA 129. 125. 118. 111. 114. 126. 128. ## 5 NL 171. 136. 154. 171. 164. 192. 176. 170. 134. ## 6 OL 32.1 28.6 35.5 34 33.0 32.6 31.8 NA 30.3 ## 7 OT 24.1 24.1 23.7 24.9 26.5 23.6 23.5 22 24.1 ## 8 OX 22 NA NA 22 NA 20 NA NA NA ## 9 PE 22.7 19.6 21.6 22.0 NA 21 21.6 22.8 19.4 ## 10 PF 7.12 7.23 6.57 6.89 6.75 7.5 7.54 7 6.78 ## 11 PH 28 31 NA NA NA 29 NA NA NA ## 12 PM 20.1 23.6 23.7 23.9 NA 23.7 22.3 23.4 23 ## 13 PP 17.1 13.6 14.3 16.4 14.8 19.8 16.8 NA 13.9 ## 14 RF 14.8 17 NA 16 NA 14 12.1 13 NA ## 15 RM 10.3 9.89 10.9 10.6 10.4 10.8 10.6 10.7 9 ## 16 SF NA 49 NA NA NA NA NA NA NA ## 17 SH 76.0 79.9 NA 88 NA 82.7 NA NA NA ## 18 SS NA NA NA NA NA NA NA NA NA ## # \u2139 15 more variables: `9` <dbl>, `10` <dbl>, `11` <dbl>, `12` <dbl>, ## # `13` <dbl>, `14` <dbl>, `15` <dbl>, `16` <dbl>, `17` <dbl>, `18` <dbl>, ## # `19` <dbl>, `20` <dbl>, `22` <dbl>, `23` <dbl>, `24` <dbl> Now we've got our reshaped data.frame. There are a few things to notice. First, we have a new column for each plot_id value. There is one old column left in the data.frame: species_id . It wasn't used in pivot_wider() , so it stays, and now contains a single entry for each unique species_id value. Finally, a lot of NA s have appeared. Some species aren't found in every plot, but because a data.frame has to have a value in every row and every column, an NA is inserted. We can double-check this to verify what is going on. Looking in our new pivoted data.frame, we can see that there is an NA value for the species BA in plot 1 . Let's take our sp_by_plot data.frame and look for the mean_weight of that species + plot combination. sp_by_plot %>% filter ( species_id == \"BA\" & plot_id == 1 ) ## # A tibble: 0 \u00d7 3 ## # Groups: species_id [0] ## # \u2139 3 variables: species_id <chr>, plot_id <dbl>, mean_weight <dbl> We get back 0 rows. There is no mean_weight for the species BA in plot 1 . This either happened because no BA were ever caught in plot 1 , or because every BA caught in plot 1 had an NA weight value and all the rows got removed when we used filter(!is.na(weight)) in the process of making sp_by_plot . Because there are no rows with that species + plot combination, in our pivoted data.frame, the value gets filled with NA . There is another pivot_ function that does the opposite, moving data from a wide to long format, called pivot_longer() . It takes 3 arguments: cols for the columns you want to pivot, names_to for the name of the new column which will contain the old column names, and values_to for the name of the new column which will contain the old values. {alt='Diagram depicting the behavior of pivot_longer() on a small tabular dataset.'} We can pivot our new wide data.frame to a long format using pivot_longer() . We want to pivot all the columns except species_id , and we will use PLOT for the new column of plot IDs, and MEAN_WT for the new column of mean weight values. sp_by_plot_wide %>% pivot_longer ( cols = - species_id , names_to = \"PLOT\" , values_to = \"MEAN_WT\" ) ## # A tibble: 432 \u00d7 3 ## # Groups: species_id [18] ## species_id PLOT MEAN_WT ## <chr> <chr> <dbl> ## 1 BA 3 8 ## 2 BA 21 6.5 ## 3 BA 1 NA ## 4 BA 2 NA ## 5 BA 4 NA ## 6 BA 5 NA ## 7 BA 6 NA ## 8 BA 7 NA ## 9 BA 8 NA ## 10 BA 9 NA ## # \u2139 422 more rows One thing you will notice is that all those NA values that got generated when we pivoted wider. However, we can filter those out, which gets us back to the same data as sp_by_plot , before we pivoted it wider. sp_by_plot_wide %>% pivot_longer ( cols = - species_id , names_to = \"PLOT\" , values_to = \"MEAN_WT\" ) %>% filter ( ! is.na ( MEAN_WT )) ## # A tibble: 300 \u00d7 3 ## # Groups: species_id [18] ## species_id PLOT MEAN_WT ## <chr> <chr> <dbl> ## 1 BA 3 8 ## 2 BA 21 6.5 ## 3 DM 3 41.2 ## 4 DM 21 41.5 ## 5 DM 1 42.7 ## 6 DM 2 42.6 ## 7 DM 4 41.9 ## 8 DM 5 42.6 ## 9 DM 6 42.1 ## 10 DM 7 43.2 ## # \u2139 290 more rows Data are often recorded in spreadsheets in a wider format, but lots of tidyverse tools, especially ggplot2 , like data in a longer format, so pivot_longer() is often very useful.","title":"Reshaping data with tidyr"},{"location":"arcadia-users-group/20231031-intro-to-r/lesson/#getting-rid-of-any-row-where-any-value-is-na-with-drop_na","text":"In the split-apply-combine approach section above, we covered how to remove NA values for a specific column. If instead of removing NA s from a single column you would like to drop any row where any column is an NA , you can use the drop_na() function. surveys %>% drop_na () ## # A tibble: 13,773 \u00d7 13 ## record_id month day year plot_id species_id sex hindfoot_length weight ## <dbl> <dbl> <dbl> <dbl> <dbl> <chr> <chr> <dbl> <dbl> ## 1 63 8 19 1977 3 DM M 35 40 ## 2 64 8 19 1977 7 DM M 37 48 ## 3 65 8 19 1977 4 DM F 34 29 ## 4 66 8 19 1977 4 DM F 35 46 ## 5 67 8 19 1977 7 DM M 35 36 ## 6 68 8 19 1977 8 DO F 32 52 ## 7 69 8 19 1977 2 PF M 15 8 ## 8 70 8 19 1977 3 OX F 21 22 ## 9 71 8 19 1977 7 DM F 36 35 ## 10 74 8 19 1977 8 PF M 12 7 ## # \u2139 13,763 more rows ## # \u2139 4 more variables: genus <chr>, species <chr>, taxa <chr>, plot_type <chr> Alternatively, you can provide column names to the drop_na() function to specify which columns to remove NA values from. surveys %>% drop_na ( hindfoot_length , weight ) ## # A tibble: 13,797 \u00d7 13 ## record_id month day year plot_id species_id sex hindfoot_length weight ## <dbl> <dbl> <dbl> <dbl> <dbl> <chr> <chr> <dbl> <dbl> ## 1 63 8 19 1977 3 DM M 35 40 ## 2 64 8 19 1977 7 DM M 37 48 ## 3 65 8 19 1977 4 DM F 34 29 ## 4 66 8 19 1977 4 DM F 35 46 ## 5 67 8 19 1977 7 DM M 35 36 ## 6 68 8 19 1977 8 DO F 32 52 ## 7 69 8 19 1977 2 PF M 15 8 ## 8 70 8 19 1977 3 OX F 21 22 ## 9 71 8 19 1977 7 DM F 36 35 ## 10 74 8 19 1977 8 PF M 12 7 ## # \u2139 13,787 more rows ## # \u2139 4 more variables: genus <chr>, species <chr>, taxa <chr>, plot_type <chr>","title":"Getting rid of any row where any value is NA with drop_na()"},{"location":"arcadia-users-group/20231031-intro-to-r/lesson/#tidy-selection","text":"In the select section above, we demonstrated how to select columns based on column name. You can select columns based on whether they match a certain criteria by using tidyselect functions. Below, we'll demonstrate how to use the where() function and the ends_with() function, but there are many additional tidyselect functions such as starts_with() or contains()`. With the where() function, we can select columns that match a specific conditions. If we want all numeric columns, we can ask to select all the columns where the class is numeric : select ( surveys , where ( is.numeric )) ## # A tibble: 16,878 \u00d7 7 ## record_id month day year plot_id hindfoot_length weight ## <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> ## 1 1 7 16 1977 2 32 NA ## 2 2 7 16 1977 3 33 NA ## 3 3 7 16 1977 2 37 NA ## 4 4 7 16 1977 7 36 NA ## 5 5 7 16 1977 3 35 NA ## 6 6 7 16 1977 1 14 NA ## 7 7 7 16 1977 2 NA NA ## 8 8 7 16 1977 1 37 NA ## 9 9 7 16 1977 1 34 NA ## 10 10 7 16 1977 6 20 NA ## # \u2139 16,868 more rows Instead of giving names or positions of columns, we instead pass the where() function with the name of another function inside it, in this case is.numeric() , and we get all the columns for which that function returns TRUE . We can use this to select any columns that have any NA values in them: select ( surveys , where ( anyNA )) ## # A tibble: 16,878 \u00d7 7 ## species_id sex hindfoot_length weight genus species taxa ## <chr> <chr> <dbl> <dbl> <chr> <chr> <chr> ## 1 NL M 32 NA Neotoma albigula Rodent ## 2 NL M 33 NA Neotoma albigula Rodent ## 3 DM F 37 NA Dipodomys merriami Rodent ## 4 DM M 36 NA Dipodomys merriami Rodent ## 5 DM M 35 NA Dipodomys merriami Rodent ## 6 PF M 14 NA Perognathus flavus Rodent ## 7 PE F NA NA Peromyscus eremicus Rodent ## 8 DM M 37 NA Dipodomys merriami Rodent ## 9 DM F 34 NA Dipodomys merriami Rodent ## 10 PF F 20 NA Perognathus flavus Rodent ## # \u2139 16,868 more rows Instead of matching on a condition, the ends_with() function match variables (in this case column names) according to a given pattern. We'll use the string \"_id\" to select columns that end with the string _id : record_id , plot_id , and species_id . select ( surveys , ends_with ( \"_id\" )) ## # A tibble: 16,878 \u00d7 3 ## record_id plot_id species_id ## <dbl> <dbl> <chr> ## 1 1 2 NL ## 2 2 3 NL ## 3 3 2 DM ## 4 4 7 DM ## 5 5 3 DM ## 6 6 1 PF ## 7 7 2 PE ## 8 8 1 DM ## 9 9 1 DM ## 10 10 6 PF ## # \u2139 16,868 more rows","title":"Tidy selection"},{"location":"arcadia-users-group/20231031-intro-to-r/lesson/#more-on-joining-data-together-with-dplyr","text":"We briefly covered how to join two data.frames together above. If you would like more information and practice doing joins, this lesson provides more information and practice. It relies on a similar data set to the one we've been using in this lesson.","title":"More on joining data together with dplyr"},{"location":"arcadia-users-group/20231104-testing-concepts/lesson/","text":"Testing concepts and terminology Note that this lesson has been modified from The Carpentries Incubator lesson on Python Testing . Parts are reproduced in full, but the major changes were included to shorten the lesson to 60 minutes. Software testing is a critical practice in the development process aimed at ensuring that a program functions as intended and does not break when changes are made. It encompasses a range of activities designed to evaluate the correctness, performance, and usability among other aspects of a software application. At its core, software testing seeks to verify that your software does what it is supposed to do, handles various cases gracefully, and remains stable even as changes are introduced over time. It is the safety net that catches bugs or errors before they reach the end-users, and it's what gives developers the confidence to continue improving their code. Everyone engages in software testing to some extent, often informally. This could be as simple as running the software to see what happens, or doing some exploratory testing as new code is written or old code is modified. Such testing might involve printing or plotting the outputs of your code as you work. However, informal testing has its limitations. It's manual, sporadic, and can become unmanageable, especially as the codebase grows and evolves. This is where systematic testing comes into play. Systematic testing takes the informal testing behaviors and codifies them, allowing for automation. This automation enables tests to be run quickly and repeatedly across the entire codebase, ensuring that every part of the software is validated every time a change is made. Without automation, it's nearly impossible to ensure that a change in one part of the software hasn't introduced a new bug elsewhere. This lesson introduces the concepts of automated testing and provides guidance on how to utilize Python constructs to start writing tests. Although we focus on Python, many of the higher-level concepts discussed are applicable to writing tests in other programming languages as well. Implementing tests in other languages While this lesson uses Python, almost all programming languages have robust (and often loved) packages dedicated to testing. In R, testing is orchestrated with the testthat package . The R packages guide has a chapter dedicated to testing . Other programming languages, such as Rust, have built-in testing features. Lesson setup This lesson will take advantage of the skills we've learned in many previous lessons. We'll use Jupyter Notebooks , GitHub , conda , and Python . This is more overhead than we typically strive for in a lesson, but we hope that it's a chance to practice these skills to achieve a new goal. In the future, we may provide a GitPod environment for learners to use while working through this lesson, however if possible we would prefer to empower users to start implementing tests on their own computers using their own setup. To start this lesson, we'll begin by creating a conda environment that has the tools we'll need. mamba create -n augtest jupyter pytest conda activate augtest We'll also create a folder to help us stay organized. mkdir 20231114-aug-testing-lesson cd 20231114-aug-testing-lesson Once our environment is activated, start a Jupyter notebook jupyter notebook Now we can start learning about testing! An introduction to testing concepts There are many ways to test software, such as assertions, exceptions, unit tests, integration tests, and regression tests. Exceptions and assertions : While writing code, exceptions and assertions can be added to sound an alarm as runtime problems come up. These kinds of tests, are embedded in the software itself and handle, as their name implies, exceptional cases rather than the norm. Unit tests : Unit tests investigate the behavior of units of code (such as functions, classes, or data structures). By validating each software unit across the valid range of its input and output parameters, tracking down unexpected behavior that may appear when the units are combined is made vastly simpler. Some examples of things a unit test might test include functions, individual Snakemake rules, a process in Nextflow, or a cell in a Jupyter notebook. Integration tests : Integration tests check that various pieces of the software work together as expected. Some examples of things an integration test might test include a set of Snakemake rule or Nextflow processes or the execution of an entire Jupyter notebook. Regression tests : Regression tests defend against new bugs, or regressions, which might appear due to new software and updates. Regression tests can also refer to tests for decreases in performance (run time, memory usage, etc.) or in the quality of some output (the resolution of a rendered graph, accuracy of a set of predictions, etc.). While each of these types of tests has a different definition, in practice there isn't always a firm delineation between each type. Assertions Perhaps the simplest test we can use in Python is to directly test, using an if statement, whether some condition is True , and exit the program otherwise: size = 10 if size > 5: exit() Python provides a shortcut for these type of checks, called assertions . Assertions are the simplest type of test. They are used as a tool for bounding acceptable behavior during runtime. The assert keyword in Python has the same behavior as the if statement above, but it also provides some information about where the check happened and an optional message: assert True == False Traceback (most recent call last): File \"<stdin>\", line 1, in <module> AssertionError assert True == True Assertions halt code execution instantly if the comparison is false and do nothing if the comparison is true. These are therefore a good tool for guarding the function against inappropriate input: def mean ( num_list ): assert len ( num_list ) != 0 return sum ( num_list ) / len ( num_list ) The advantage of assertions is their ease of use. They are rarely more than one line of code. The disadvantage is that assertions halt execution indiscriminately and the helpfulness of the resulting error message is usually quite limited. In practice, assertions are almost never directly used in Python programs. Exceptions Exceptions are more sophisticated than assertions. When an error is encountered, an informative exception is 'thrown' or 'raised'. For example, instead of the assertion in the case before, an exception can be used. def mean ( num_list ): if len ( num_list ) == 0 : raise Exception ( \"The algebraic mean of an empty list is undefined. \" \"Please provide a list of numbers.\" ) else : return sum ( num_list ) / len ( num_list ) Once an exception is raised, it will be passed upward in the program scope. An exception can be used to trigger additional error messages or an alternative behavior. Rather than immediately halting code execution, the exception can be 'caught' upstream with a try-except block. When wrapped in a try-except block, the exception can be intercepted before it reaches global scope and halts execution. To add information or replace the message before it is passed upstream, the try-catch block can be used to catch-and-reraise the exception: def mean ( num_list ): try : return sum ( num_list ) / len ( num_list ) except ZeroDivisionError as original_error : msg = \"The algebraic mean of an empty list is undefined. Please provide a list of numbers.\" raise ZeroDivisionError ( original_error . __str__ () + \" \\n \" + msg ) Alternatively, the exception can be handled appropriately for the use case. If an alternative behavior is preferred, the exception can be disregarded and a responsive behavior can be implemented like so: def mean ( num_list ): try : return sum ( num_list ) / len ( num_list ) except ZeroDivisionError : return 0 If a single function might raise more than one type of exception, each can be caught and handled separately. def mean ( num_list ): try : return sum ( num_list ) / len ( num_list ) except ZeroDivisionError : return 0 except TypeError as original_error : msg = \"The algebraic mean of an non-numerical list is undefined. \\ Please provide a list of numbers.\" raise TypeError ( original_error . __str__ () + \" \\n \" + msg ) Exceptions have the advantage of being simple to include and when accompanied by useful help message, can be helpful to the user. However, not all behaviors can or should be found with runtime exceptions. Most behaviors should be validated with unit tests. Unit tests Unit tests are so called because they test the functionality of the code by interrogating individual functions and methods. Functions and methods can often be considered the atomic units of software but what is considered to be the smallest code unit is subjective. Implementing unit tests often has the effect of encouraging both the code and the tests to be as small, well-defined, and modular as possible. In Python, unit tests typically take the form of test functions that call and make assertions about methods and functions in the code base. For now, we'll write some tests for the mean function and simply run them individually to see whether they fail. Later in this lesson, we'll use a test framework to collect and run them. Using a test framework makes running tests streamlined. Unit tests are typically made of three pieces, some set-up, a number of assertions, and some tear-down. Set-up can be as simple as initializing the input values or as complex as creating and initializing concrete instances of a class. Ultimately, the test occurs when an assertion is made, comparing the observed and expected values. For example, let us test that our mean function successfully calculates the known value for a simple list. Before running the next code, save your mean function to a file called mean.py in the working directory. You can use this code to save to file: def mean ( num_list ): try : return sum ( num_list ) / len ( num_list ) except ZeroDivisionError : return 0 except TypeError as original_error : msg = \"The algebraic mean of an non-numerical list is undefined. \\ Please provide a list of numbers.\" raise TypeError ( original_error . __str__ () + \" \\n \" + msg ) Now, back in your Jupyter Notebook run the following code: from mean import * def test_mean_with_ints (): num_list = [ 1 , 2 , 3 , 4 , 5 ] observed_value = mean ( num_list ) expected_value = 3 assert observed_value == expected_value The test above: sets up the input parameters (the list [1, 2, 3, 4, 5] ); collects the observed result; declares the expected result (calculated with our human brain); and compares the two with an assertion. A unit test suite is made up of many tests just like this one. A single implemented function may be tested in numerous ways. In a file called test_mean.py , implement the following code: from mean import * def test_mean_with_ints (): num_list = [ 1 , 2 , 3 , 4 , 5 ] observed_value = mean ( num_list ) expected_value = 3 assert observed_value == expected_value def test_mean_with_zero (): num_list = [ 0 , 2 , 4 , 6 ] observed_value = mean ( num_list ) expected_value = 3 assert observed_value == expected_value def test_mean_with_double (): # This one will fail in Python 2 num_list = [ 1 , 2 , 3 , 4 ] observed_value = mean ( num_list ) expected_value = 2.5 assert observed_value == expected_value def test_mean_with_long (): big = 100000000 observed_value = mean ( range ( 1 , big )) expected_value = big / 2.0 assert observed_value == expected_value Use Jupyter Notebook to import the test_mean package and run each test like this: from test_mean import * test_mean_with_ints () test_mean_with_zero () test_mean_with_double () test_mean_with_long () We just wrote and ran five tests for our mean() function. You may have noticed that several of the tests look very similar to each other -- they introduce an input, call mean() , and test its output against an expected value. We'll come back to this later, offering a way to write a single test that applies to multiple inputs. Using the test framework pytest We created a suite of tests for our mean function, but it was annoying to run them one at a time. It would be a lot better if there were some way to run them all at once, just reporting which tests fail and which succeed. Thankfully, that exists. Recall our tests: from mean import * def test_mean_with_ints (): num_list = [ 1 , 2 , 3 , 4 , 5 ] observed_value = mean ( num_list ) expected_value = 3 assert observed_value == expected_value def test_mean_with_zero (): num_list = [ 0 , 2 , 4 , 6 ] observed_value = mean ( num_list ) expected_value = 3 assert observed_value == expected_value def test_mean_with_double (): # This one will fail in Python 2 num_list = [ 1 , 2 , 3 , 4 ] observed_value = mean ( num_list ) expected_value = 2.5 assert observed_value == expected_value def test_mean_with_long (): big = 100000000 observed_value = mean ( range ( 1 , big )) expected_value = big / 2.0 assert observed_value == expected_value Once these tests are written in a file called test_mean.py , the command pytest can be run on the terminal or command line from the directory containing the tests (note that you'll have to use py.test for older versions of the pytest package): pytest collected 5 items test_mean.py ..... ========================== 4 passed in 2.68 seconds =========================== In the above case, the pytest package sniffed out the tests in the directory and ran them together to produce a report of the sum of the files and functions matching the regular expression [Tt]est[-_].* . The major benefit a testing framework provides is exactly that, a utility to find and run the tests automatically. With pytest, this is the command-line tool called pytest . When pytest is run, it will search all directories below where it was called, find all of the Python files in these directories whose names start or end with test , import them, and run all of the functions and classes whose names start with test or Test . This automatic registration of test code saves tons of time and provides a consistent organization framework across Python projects. When you run pytest , it will print a dot ( . ) on the screen for every test that passes, an F for every test that fails or where there was an unexpected error. After the dots, pytest will print summary information. To see what a test failure looks like, modify the expected_value in one of the tests and run pytest again. You should see something like this: collected 5 items test_mean.py F.... [100%] ========================================= FAILURES ========================================= ___________________________________ test_mean_with_ints ____________________________________ def test_mean_with_ints(): num_list = [1, 2, 3, 4, 5] observed_value = mean(num_list) expected_value = 4 > assert observed_value == expected_value E assert 3.0 == 4 test_mean.py:8: AssertionError ================================= short test summary info ================================== FAILED test_mean.py::test_mean_with_ints - assert 3.0 == 4 =============================== 1 failed, 4 passed in 0.86s ================================ Notice that pytest detects the failed test and provides detailed information about why the test failed by displaying the values of the variables used in the assert comparison. In this case, this information is not surprising, since we triggered the failure by deliberately modifying the expected_value to an incorrect number. But in more realistic scenarios, this information is often very helpful for understanding why a test is failing (and whether the failure indicates a true bug in the code or a bug in the test itself). Testing for expected exceptions In many cases, it is important to check that our code responds to unexpected inputs appropriately. Recall that our mean function checks for a TypeError when attempting to calculate the mean. How can we test that it performs this check correctly and raises the expected exception? Pytest provides a mechanism to test for expected exceptions. It looks like this: def test_mean_with_non_numeric_list (): num_list = [ '0' , '1' , '2' ] with pytest . raises ( TypeError ): mean ( num_list ) Here, we use a with block to tell pytest that we expect the code within the with block to raise an exception. Pytest checks that this code does indeed raise an exception of the correct type, and if not, flags the test as a failure. Try adding this function to test_mean.py and verify that all the tests pass. (hint: don't forget to add import pytest at the top of test_mean.py , since our new test uses pytest.raises ). Challenge 1: Altering functions to pass all tests A scenario we did not consider when writing our mean function is when the list of numbers passed to mean contains complex numbers. Because the arithmetic mean of complex numbers may be difficult to interpret, suppose we decide that we don't want our function to handle this case. Let's write a test to reflect this. Add the following test to test_mean.py : def test_mean_with_complex (): num_list = [ 0 , 1 , 1 + 1 j ] with pytest . raises ( TypeError ): mean ( num_list ) Try running the tests again. You should see that this test fails. This is because our mean function does not check for complex numbers, and Python's builtin sum function is able to handle complex numbers, so mean returns a result rather than raising an exception. Now, modify the function mean in mean.py from the previous section until our new test passes. When it passes, pytest will produce results like the following: pytest collected 5 items test_mean.py ..... ========================== 5 passed in 2.68 seconds =========================== Challenge solution There are many ways this challenge could be solved. One way is to check for the presence of complex numbers before calculating the mean. def mean ( num_list ): if any ( isinstance ( num , complex ) for num in num_list ): raise TypeError ( \"Calculating the mean of complex numbers is not supported.\" ) else : try : return sum ( num_list ) / len ( num_list ) except ZeroDivisionError : return 0 except TypeError as original_error : msg = \"The algebraic mean of an non-numerical list is undefined. \\ Please provide a list of numbers.\" raise TypeError ( original_error . __str__ () + \" \\n \" + msg ) Note, using pytest -v (the 'v' stands for 'verbose') will result in pytest listing which tests are executed and whether they pass or not: pytest collected 5 items test_mean.py ..... test_mean.py::test_mean_with_ints PASSED test_mean.py::test_mean_with_zero PASSED test_mean.py::test_mean_with_double PASSED test_mean.py::test_mean_with_long PASSED test_mean.py::test_mean_with_complex PASSED ========================== 5 passed in 2.57 seconds =========================== As we write more code, we would write more tests, and pytest would produce more dots. Parametrized tests Often, different unit tests of the same \"unit\" have similar logic, but are applied to different inputs; this is the case with our mean() test suite above. pytest offers a convenient mechanism to write just a single test and apply it to multiple inputs, called test parameterization. This way, adding a new test case is as simple as defining one more input and expected output. To parametrize a test, we \"decorate\" it with its expected inputs and outputs, and pytest will expand the test to run as many times as needed to check all inputs. In the pytest output, this will look exactly like running multiple tests that have different functions. About Python decorators \"Decorators\" are a Python way to enhance or wrap a function with additional behavior. They are added just before a function definition, using a name that starts with `@`, and can take arguments separate from the function arguments, for example: @log_to_file(\"output.txt\") def mean(num_list): ... This code is equivalent to wrapping or \"decorating\" the mean function with the function log_to_file(\"output.txt\") : def _mean(num_list): ... mean = log_to_file(\"output.txt\")(_mean) The ampersand-based \"decorator\" syntax is just a clearer and more readable way of wrapping a function with another function. In this example, that other function is in fact log_to_file(\"output.txt\") . This is possible because log_to_file is a function that takes a filename as an input and returns another function that itself takes a function as an argument and then returns a new \"wrapped\" function. For example, here's a parametrized version of our test suite for mean() . In this case, we decorate the test with two parameters that can be used within the test: num_list and expected_value . The first parameter will be used to call the mean() function, and the second is its expected result, which the test will validate. We then provide a list of matching pairs of these parameters, each of which will run as a test: import pytest @pytest . mark . parametrize ( \"num_list,expected_value\" , [ ([ 1 , 2 , 3 , 4 , 5 ], 3 ), ([ 0 , 2 , 4 , 6 ], 3 ), ( range ( 1 , 10000 ), 10000 / 2.0 ), ]) def test_mean ( num_list , expected_value ): observed_value = mean ( num_list ) assert observed_value == expected_value Integration tests Integration tests focus on gluing code together or the results of code when multiple functions are used. See below for an conceptual example of an integration test. Consider three functions add_one() , multiply_by_two() , and add_one_and_multiply_by_two() as a simplistic example. Function add_one() increments a number by one, multiply_by_two() multiplies a number by two, and add_one_and_multiply_by_two() composes them as defined below: def add_one ( x ): return x + 1 def multiply_by_two ( x ): return 2 * x def add_one_and_multiply_by_two ( x ): return multiply_by_two ( add_one ( x )) Functions add_one() and multiply_by_two() can be unit tested since they perform singular operations. However, add_one_and_multiply_by_two() can't be truly unit tested as it delegates the real work to add_one() and multiply_by_two() . Testing add_one_and_multiply_by_two() will evaluate the integration of add_one() and multiply_by_two() . Integration tests still adhere to the practice of comparing expected outcomes with observed results. A sample test_add_one_and_multiply_by_two() is illustrated below: def test_add_one_and_multiply_by_two (): expected_value = 6 observed_value = add_one_and_multiply_by_two ( 2 ) assert observed_value == expected_value The definition of a code unit is somewhat ambiguous, making the distinction between integration tests and unit tests a bit unclear. Integration tests can range from extremely simple to highly complex, contrasting with unit tests. If a function or class merely amalgamates two or more unit-tested code pieces, an integration test is necessary. If a function introduces new untested behavior, a unit test is needed. The structure of integration tests closely resembles that of unit tests, comparing expected results with observed values. However, deriving the expected result or preparing the code for execution can be significantly more complex. Integration tests are generally more time-consuming due to their extensive nature. This distinction is helpful to differentiate between straightforward (unit) and more nuanced (integration) test-writing requirements. Regression tests Regression tests refer to past outputs for expected behavior. The anticipated outcome is based on previous computations for the same inputs. Regression tests hold the past as \"correct.\" They notify developers about how and when a codebase has evolved such that it produces different results. However, they don't provide insights into why the changes occurred. The discrepancy between current and previous code outputs is termed a regression. Like integration tests, regression tests are high-level and often encompass the entire code base. A prevalent regression test strategy extends across multiple code versions. For instance, an input file for version X of a workflow is processed, and the output file is saved, typically online. While developing version Y, the test suite automatically fetches the output for version X, processes the same input file for version Y, and contrasts the two output files. Any significant discrepancies trigger a test failure. Regression tests can identify failures missed by integration and unit tests. Each project may adopt a slightly varied approach to regression testing, based on its software requirements. Testing frameworks aid in constructing regression tests but don\u2019t provide additional sophistication beyond the discussed concepts. Continuous integration Continuous integration makes running tests as easy as possible by integrating the test suite into the development process. Every time a change is made to the repository, the continuous integration system builds and checks that code. Based on the instructions you provide, a continuous integration server can: check out new code from a repository spin up instances of supported operating systems (i.e. various versions of OSX, Linux, Windows, etc.). spin up those instances with different software versions (i.e. python 2.7 and python 3.0) run the build and test scripts check for errors report the results. Since the first step the server conducts is to check out the code from a repository, we'll need to put our code online to make use of this kind of server. Set Up a Mean Git Repository on GitHub Our mean.py test_mean.py files can be the contents of a repository on GitHub. Go to GitHub and create a repository called aug-mean. Do this in your own user account and don't add any files to the new repo (no README/LICENSE, etc.). Turn the aug-mean directory that we've been working in on your computer into a git repository following the \"\u2026or create a new repository on the command line\" instructions on GitHub: echo \"# aug-mean\" >> README.md git init git add README.md git commit -m \"first commit\" git branch -M main git remote add origin git@github.com:yourusername/aug-mean.git git push -u origin main Create a new branch ( git checkout -b yourinitials/init ). Use git to add , commit , and push the two files mean.py and test_mean.py to GitHub. GitHub Actions GitHub Actions is a continuous integration service provided by GitHub. It's integrated directly into GitHub repositories and does not require additional accounts or external services. Note that GitHub Actions usage is free for standard GitHub-hosted runners in public repositories, and for self-hosted runners. For private repositories, each GitHub account receives a certain amount of free minutes and storage for use with GitHub-hosted runners, depending on the account's plan (see here for more information). To use GitHub Actions, create a directory called .github and within it, create another directory called workflows . mkdir -p .github/workflows Inside the workflows directory, you can create a YAML file (e.g. ci.yml ) to define your continuous integration process: name : pytest CI on : [ push , pull_request ] jobs : build : runs-on : ubuntu-latest steps : - uses : actions/checkout@v2 - name : Set up Python 3.10 uses : actions/setup-python@v2 with : python-version : '3.10' - name : Install dependencies run : | python -m pip install --upgrade pip pip install -r requirements.txt - name : Run tests run : pytest Below, we break down how this workflow works: * name : Our workflow is named pytest CI . * on : The workflow will be triggered to run automatically whenever commits are pushed to the repository or whenever a pull request is created or updated with new commits. * jobs : Specifies what the workflow will actually run. In this case, we specify that we want to run on the latest version of the ubuntu operating system using python version 3.10. These instructions are enough to launch a computer with python running on it. Then, we specify that we want to install dependencies from a requirements.txt file using pip. Lastly, we run our tests using pytest . Running GitHub Actions workflows on multiple operating systems using a matrix Often times, developers want to check that their tests will pass not just with an ubuntu operating system and one version of Python, but with many operating systems and many versions of Python. This can be done using a matrix, which we demonstrate below. We specify lists of operating systems and python versions that we want to run our CI with, and then use a matrix call to run them all. name : pytest CI on : [ push , pull_request ] jobs : build : runs-on : ${{ matrix.os }} strategy : matrix : os : [ ubuntu-latest , macos-latest , windows-latest ] python-version : [ '3.7' , '3.8' , '3.9' , '3.10' , '3.11' ] steps : - uses : actions/checkout@v2 - name : Set up Python ${{ matrix.python-version }} uses : actions/setup-python@v2 with : python-version : ${{ matrix.python-version }} - name : Install dependencies run : | python -m pip install --upgrade pip pip install -r requirements.txt - name : Run tests run : pytest Triggering CI Add .github/workflows/ci.yml to your repository Commit and push it. Open a pull request with your changes. Check how the CI is going in the PR. Right now, the CI is failing. Why is that? When we set up for today's lesson, we used conda to install pytest. In the CI workflow, we specified that our dependencies (in this case pytest) should be installed from a requirements.txt file. That requirements.txt file is a conventional way to list all of the python packages that we need. We haven't created that file yet. Let's go ahead an create it, add it, commit it, and push it. Since we need pytest, the requirements.txt file looks like this: pytest==7.4.3 Pushing new changes to our branch automatically re-triggers the CI tests to re-run. When all of our tests pass, we'll see a big green check mark next stating that \"All checks have passed\" and a green check next to our commit in our commit history. If some of your checks don't pass, you can see what went wrong by clicking on the check which will launch the runtime information. Also note that while in this example we use pip and a requirements.txt file to install dependencies, dependencies can be installed in other ways such as with conda .","title":"Testing concepts and terminology"},{"location":"arcadia-users-group/20231104-testing-concepts/lesson/#testing-concepts-and-terminology","text":"Note that this lesson has been modified from The Carpentries Incubator lesson on Python Testing . Parts are reproduced in full, but the major changes were included to shorten the lesson to 60 minutes. Software testing is a critical practice in the development process aimed at ensuring that a program functions as intended and does not break when changes are made. It encompasses a range of activities designed to evaluate the correctness, performance, and usability among other aspects of a software application. At its core, software testing seeks to verify that your software does what it is supposed to do, handles various cases gracefully, and remains stable even as changes are introduced over time. It is the safety net that catches bugs or errors before they reach the end-users, and it's what gives developers the confidence to continue improving their code. Everyone engages in software testing to some extent, often informally. This could be as simple as running the software to see what happens, or doing some exploratory testing as new code is written or old code is modified. Such testing might involve printing or plotting the outputs of your code as you work. However, informal testing has its limitations. It's manual, sporadic, and can become unmanageable, especially as the codebase grows and evolves. This is where systematic testing comes into play. Systematic testing takes the informal testing behaviors and codifies them, allowing for automation. This automation enables tests to be run quickly and repeatedly across the entire codebase, ensuring that every part of the software is validated every time a change is made. Without automation, it's nearly impossible to ensure that a change in one part of the software hasn't introduced a new bug elsewhere. This lesson introduces the concepts of automated testing and provides guidance on how to utilize Python constructs to start writing tests. Although we focus on Python, many of the higher-level concepts discussed are applicable to writing tests in other programming languages as well. Implementing tests in other languages While this lesson uses Python, almost all programming languages have robust (and often loved) packages dedicated to testing. In R, testing is orchestrated with the testthat package . The R packages guide has a chapter dedicated to testing . Other programming languages, such as Rust, have built-in testing features.","title":"Testing concepts and terminology"},{"location":"arcadia-users-group/20231104-testing-concepts/lesson/#lesson-setup","text":"This lesson will take advantage of the skills we've learned in many previous lessons. We'll use Jupyter Notebooks , GitHub , conda , and Python . This is more overhead than we typically strive for in a lesson, but we hope that it's a chance to practice these skills to achieve a new goal. In the future, we may provide a GitPod environment for learners to use while working through this lesson, however if possible we would prefer to empower users to start implementing tests on their own computers using their own setup. To start this lesson, we'll begin by creating a conda environment that has the tools we'll need. mamba create -n augtest jupyter pytest conda activate augtest We'll also create a folder to help us stay organized. mkdir 20231114-aug-testing-lesson cd 20231114-aug-testing-lesson Once our environment is activated, start a Jupyter notebook jupyter notebook Now we can start learning about testing!","title":"Lesson setup"},{"location":"arcadia-users-group/20231104-testing-concepts/lesson/#an-introduction-to-testing-concepts","text":"There are many ways to test software, such as assertions, exceptions, unit tests, integration tests, and regression tests. Exceptions and assertions : While writing code, exceptions and assertions can be added to sound an alarm as runtime problems come up. These kinds of tests, are embedded in the software itself and handle, as their name implies, exceptional cases rather than the norm. Unit tests : Unit tests investigate the behavior of units of code (such as functions, classes, or data structures). By validating each software unit across the valid range of its input and output parameters, tracking down unexpected behavior that may appear when the units are combined is made vastly simpler. Some examples of things a unit test might test include functions, individual Snakemake rules, a process in Nextflow, or a cell in a Jupyter notebook. Integration tests : Integration tests check that various pieces of the software work together as expected. Some examples of things an integration test might test include a set of Snakemake rule or Nextflow processes or the execution of an entire Jupyter notebook. Regression tests : Regression tests defend against new bugs, or regressions, which might appear due to new software and updates. Regression tests can also refer to tests for decreases in performance (run time, memory usage, etc.) or in the quality of some output (the resolution of a rendered graph, accuracy of a set of predictions, etc.). While each of these types of tests has a different definition, in practice there isn't always a firm delineation between each type.","title":"An introduction to testing concepts"},{"location":"arcadia-users-group/20231104-testing-concepts/lesson/#assertions","text":"Perhaps the simplest test we can use in Python is to directly test, using an if statement, whether some condition is True , and exit the program otherwise: size = 10 if size > 5: exit() Python provides a shortcut for these type of checks, called assertions . Assertions are the simplest type of test. They are used as a tool for bounding acceptable behavior during runtime. The assert keyword in Python has the same behavior as the if statement above, but it also provides some information about where the check happened and an optional message: assert True == False Traceback (most recent call last): File \"<stdin>\", line 1, in <module> AssertionError assert True == True Assertions halt code execution instantly if the comparison is false and do nothing if the comparison is true. These are therefore a good tool for guarding the function against inappropriate input: def mean ( num_list ): assert len ( num_list ) != 0 return sum ( num_list ) / len ( num_list ) The advantage of assertions is their ease of use. They are rarely more than one line of code. The disadvantage is that assertions halt execution indiscriminately and the helpfulness of the resulting error message is usually quite limited. In practice, assertions are almost never directly used in Python programs.","title":"Assertions"},{"location":"arcadia-users-group/20231104-testing-concepts/lesson/#exceptions","text":"Exceptions are more sophisticated than assertions. When an error is encountered, an informative exception is 'thrown' or 'raised'. For example, instead of the assertion in the case before, an exception can be used. def mean ( num_list ): if len ( num_list ) == 0 : raise Exception ( \"The algebraic mean of an empty list is undefined. \" \"Please provide a list of numbers.\" ) else : return sum ( num_list ) / len ( num_list ) Once an exception is raised, it will be passed upward in the program scope. An exception can be used to trigger additional error messages or an alternative behavior. Rather than immediately halting code execution, the exception can be 'caught' upstream with a try-except block. When wrapped in a try-except block, the exception can be intercepted before it reaches global scope and halts execution. To add information or replace the message before it is passed upstream, the try-catch block can be used to catch-and-reraise the exception: def mean ( num_list ): try : return sum ( num_list ) / len ( num_list ) except ZeroDivisionError as original_error : msg = \"The algebraic mean of an empty list is undefined. Please provide a list of numbers.\" raise ZeroDivisionError ( original_error . __str__ () + \" \\n \" + msg ) Alternatively, the exception can be handled appropriately for the use case. If an alternative behavior is preferred, the exception can be disregarded and a responsive behavior can be implemented like so: def mean ( num_list ): try : return sum ( num_list ) / len ( num_list ) except ZeroDivisionError : return 0 If a single function might raise more than one type of exception, each can be caught and handled separately. def mean ( num_list ): try : return sum ( num_list ) / len ( num_list ) except ZeroDivisionError : return 0 except TypeError as original_error : msg = \"The algebraic mean of an non-numerical list is undefined. \\ Please provide a list of numbers.\" raise TypeError ( original_error . __str__ () + \" \\n \" + msg ) Exceptions have the advantage of being simple to include and when accompanied by useful help message, can be helpful to the user. However, not all behaviors can or should be found with runtime exceptions. Most behaviors should be validated with unit tests.","title":"Exceptions"},{"location":"arcadia-users-group/20231104-testing-concepts/lesson/#unit-tests","text":"Unit tests are so called because they test the functionality of the code by interrogating individual functions and methods. Functions and methods can often be considered the atomic units of software but what is considered to be the smallest code unit is subjective. Implementing unit tests often has the effect of encouraging both the code and the tests to be as small, well-defined, and modular as possible. In Python, unit tests typically take the form of test functions that call and make assertions about methods and functions in the code base. For now, we'll write some tests for the mean function and simply run them individually to see whether they fail. Later in this lesson, we'll use a test framework to collect and run them. Using a test framework makes running tests streamlined. Unit tests are typically made of three pieces, some set-up, a number of assertions, and some tear-down. Set-up can be as simple as initializing the input values or as complex as creating and initializing concrete instances of a class. Ultimately, the test occurs when an assertion is made, comparing the observed and expected values. For example, let us test that our mean function successfully calculates the known value for a simple list. Before running the next code, save your mean function to a file called mean.py in the working directory. You can use this code to save to file: def mean ( num_list ): try : return sum ( num_list ) / len ( num_list ) except ZeroDivisionError : return 0 except TypeError as original_error : msg = \"The algebraic mean of an non-numerical list is undefined. \\ Please provide a list of numbers.\" raise TypeError ( original_error . __str__ () + \" \\n \" + msg ) Now, back in your Jupyter Notebook run the following code: from mean import * def test_mean_with_ints (): num_list = [ 1 , 2 , 3 , 4 , 5 ] observed_value = mean ( num_list ) expected_value = 3 assert observed_value == expected_value The test above: sets up the input parameters (the list [1, 2, 3, 4, 5] ); collects the observed result; declares the expected result (calculated with our human brain); and compares the two with an assertion. A unit test suite is made up of many tests just like this one. A single implemented function may be tested in numerous ways. In a file called test_mean.py , implement the following code: from mean import * def test_mean_with_ints (): num_list = [ 1 , 2 , 3 , 4 , 5 ] observed_value = mean ( num_list ) expected_value = 3 assert observed_value == expected_value def test_mean_with_zero (): num_list = [ 0 , 2 , 4 , 6 ] observed_value = mean ( num_list ) expected_value = 3 assert observed_value == expected_value def test_mean_with_double (): # This one will fail in Python 2 num_list = [ 1 , 2 , 3 , 4 ] observed_value = mean ( num_list ) expected_value = 2.5 assert observed_value == expected_value def test_mean_with_long (): big = 100000000 observed_value = mean ( range ( 1 , big )) expected_value = big / 2.0 assert observed_value == expected_value Use Jupyter Notebook to import the test_mean package and run each test like this: from test_mean import * test_mean_with_ints () test_mean_with_zero () test_mean_with_double () test_mean_with_long () We just wrote and ran five tests for our mean() function. You may have noticed that several of the tests look very similar to each other -- they introduce an input, call mean() , and test its output against an expected value. We'll come back to this later, offering a way to write a single test that applies to multiple inputs.","title":"Unit tests"},{"location":"arcadia-users-group/20231104-testing-concepts/lesson/#using-the-test-framework-pytest","text":"We created a suite of tests for our mean function, but it was annoying to run them one at a time. It would be a lot better if there were some way to run them all at once, just reporting which tests fail and which succeed. Thankfully, that exists. Recall our tests: from mean import * def test_mean_with_ints (): num_list = [ 1 , 2 , 3 , 4 , 5 ] observed_value = mean ( num_list ) expected_value = 3 assert observed_value == expected_value def test_mean_with_zero (): num_list = [ 0 , 2 , 4 , 6 ] observed_value = mean ( num_list ) expected_value = 3 assert observed_value == expected_value def test_mean_with_double (): # This one will fail in Python 2 num_list = [ 1 , 2 , 3 , 4 ] observed_value = mean ( num_list ) expected_value = 2.5 assert observed_value == expected_value def test_mean_with_long (): big = 100000000 observed_value = mean ( range ( 1 , big )) expected_value = big / 2.0 assert observed_value == expected_value Once these tests are written in a file called test_mean.py , the command pytest can be run on the terminal or command line from the directory containing the tests (note that you'll have to use py.test for older versions of the pytest package): pytest collected 5 items test_mean.py ..... ========================== 4 passed in 2.68 seconds =========================== In the above case, the pytest package sniffed out the tests in the directory and ran them together to produce a report of the sum of the files and functions matching the regular expression [Tt]est[-_].* . The major benefit a testing framework provides is exactly that, a utility to find and run the tests automatically. With pytest, this is the command-line tool called pytest . When pytest is run, it will search all directories below where it was called, find all of the Python files in these directories whose names start or end with test , import them, and run all of the functions and classes whose names start with test or Test . This automatic registration of test code saves tons of time and provides a consistent organization framework across Python projects. When you run pytest , it will print a dot ( . ) on the screen for every test that passes, an F for every test that fails or where there was an unexpected error. After the dots, pytest will print summary information. To see what a test failure looks like, modify the expected_value in one of the tests and run pytest again. You should see something like this: collected 5 items test_mean.py F.... [100%] ========================================= FAILURES ========================================= ___________________________________ test_mean_with_ints ____________________________________ def test_mean_with_ints(): num_list = [1, 2, 3, 4, 5] observed_value = mean(num_list) expected_value = 4 > assert observed_value == expected_value E assert 3.0 == 4 test_mean.py:8: AssertionError ================================= short test summary info ================================== FAILED test_mean.py::test_mean_with_ints - assert 3.0 == 4 =============================== 1 failed, 4 passed in 0.86s ================================ Notice that pytest detects the failed test and provides detailed information about why the test failed by displaying the values of the variables used in the assert comparison. In this case, this information is not surprising, since we triggered the failure by deliberately modifying the expected_value to an incorrect number. But in more realistic scenarios, this information is often very helpful for understanding why a test is failing (and whether the failure indicates a true bug in the code or a bug in the test itself).","title":"Using the test framework pytest"},{"location":"arcadia-users-group/20231104-testing-concepts/lesson/#testing-for-expected-exceptions","text":"In many cases, it is important to check that our code responds to unexpected inputs appropriately. Recall that our mean function checks for a TypeError when attempting to calculate the mean. How can we test that it performs this check correctly and raises the expected exception? Pytest provides a mechanism to test for expected exceptions. It looks like this: def test_mean_with_non_numeric_list (): num_list = [ '0' , '1' , '2' ] with pytest . raises ( TypeError ): mean ( num_list ) Here, we use a with block to tell pytest that we expect the code within the with block to raise an exception. Pytest checks that this code does indeed raise an exception of the correct type, and if not, flags the test as a failure. Try adding this function to test_mean.py and verify that all the tests pass. (hint: don't forget to add import pytest at the top of test_mean.py , since our new test uses pytest.raises ).","title":"Testing for expected exceptions"},{"location":"arcadia-users-group/20231104-testing-concepts/lesson/#challenge-1-altering-functions-to-pass-all-tests","text":"A scenario we did not consider when writing our mean function is when the list of numbers passed to mean contains complex numbers. Because the arithmetic mean of complex numbers may be difficult to interpret, suppose we decide that we don't want our function to handle this case. Let's write a test to reflect this. Add the following test to test_mean.py : def test_mean_with_complex (): num_list = [ 0 , 1 , 1 + 1 j ] with pytest . raises ( TypeError ): mean ( num_list ) Try running the tests again. You should see that this test fails. This is because our mean function does not check for complex numbers, and Python's builtin sum function is able to handle complex numbers, so mean returns a result rather than raising an exception. Now, modify the function mean in mean.py from the previous section until our new test passes. When it passes, pytest will produce results like the following: pytest collected 5 items test_mean.py ..... ========================== 5 passed in 2.68 seconds =========================== Challenge solution There are many ways this challenge could be solved. One way is to check for the presence of complex numbers before calculating the mean. def mean ( num_list ): if any ( isinstance ( num , complex ) for num in num_list ): raise TypeError ( \"Calculating the mean of complex numbers is not supported.\" ) else : try : return sum ( num_list ) / len ( num_list ) except ZeroDivisionError : return 0 except TypeError as original_error : msg = \"The algebraic mean of an non-numerical list is undefined. \\ Please provide a list of numbers.\" raise TypeError ( original_error . __str__ () + \" \\n \" + msg ) Note, using pytest -v (the 'v' stands for 'verbose') will result in pytest listing which tests are executed and whether they pass or not: pytest collected 5 items test_mean.py ..... test_mean.py::test_mean_with_ints PASSED test_mean.py::test_mean_with_zero PASSED test_mean.py::test_mean_with_double PASSED test_mean.py::test_mean_with_long PASSED test_mean.py::test_mean_with_complex PASSED ========================== 5 passed in 2.57 seconds =========================== As we write more code, we would write more tests, and pytest would produce more dots.","title":"Challenge 1: Altering functions to pass all tests"},{"location":"arcadia-users-group/20231104-testing-concepts/lesson/#parametrized-tests","text":"Often, different unit tests of the same \"unit\" have similar logic, but are applied to different inputs; this is the case with our mean() test suite above. pytest offers a convenient mechanism to write just a single test and apply it to multiple inputs, called test parameterization. This way, adding a new test case is as simple as defining one more input and expected output. To parametrize a test, we \"decorate\" it with its expected inputs and outputs, and pytest will expand the test to run as many times as needed to check all inputs. In the pytest output, this will look exactly like running multiple tests that have different functions. About Python decorators \"Decorators\" are a Python way to enhance or wrap a function with additional behavior. They are added just before a function definition, using a name that starts with `@`, and can take arguments separate from the function arguments, for example: @log_to_file(\"output.txt\") def mean(num_list): ... This code is equivalent to wrapping or \"decorating\" the mean function with the function log_to_file(\"output.txt\") : def _mean(num_list): ... mean = log_to_file(\"output.txt\")(_mean) The ampersand-based \"decorator\" syntax is just a clearer and more readable way of wrapping a function with another function. In this example, that other function is in fact log_to_file(\"output.txt\") . This is possible because log_to_file is a function that takes a filename as an input and returns another function that itself takes a function as an argument and then returns a new \"wrapped\" function. For example, here's a parametrized version of our test suite for mean() . In this case, we decorate the test with two parameters that can be used within the test: num_list and expected_value . The first parameter will be used to call the mean() function, and the second is its expected result, which the test will validate. We then provide a list of matching pairs of these parameters, each of which will run as a test: import pytest @pytest . mark . parametrize ( \"num_list,expected_value\" , [ ([ 1 , 2 , 3 , 4 , 5 ], 3 ), ([ 0 , 2 , 4 , 6 ], 3 ), ( range ( 1 , 10000 ), 10000 / 2.0 ), ]) def test_mean ( num_list , expected_value ): observed_value = mean ( num_list ) assert observed_value == expected_value","title":"Parametrized tests"},{"location":"arcadia-users-group/20231104-testing-concepts/lesson/#integration-tests","text":"Integration tests focus on gluing code together or the results of code when multiple functions are used. See below for an conceptual example of an integration test. Consider three functions add_one() , multiply_by_two() , and add_one_and_multiply_by_two() as a simplistic example. Function add_one() increments a number by one, multiply_by_two() multiplies a number by two, and add_one_and_multiply_by_two() composes them as defined below: def add_one ( x ): return x + 1 def multiply_by_two ( x ): return 2 * x def add_one_and_multiply_by_two ( x ): return multiply_by_two ( add_one ( x )) Functions add_one() and multiply_by_two() can be unit tested since they perform singular operations. However, add_one_and_multiply_by_two() can't be truly unit tested as it delegates the real work to add_one() and multiply_by_two() . Testing add_one_and_multiply_by_two() will evaluate the integration of add_one() and multiply_by_two() . Integration tests still adhere to the practice of comparing expected outcomes with observed results. A sample test_add_one_and_multiply_by_two() is illustrated below: def test_add_one_and_multiply_by_two (): expected_value = 6 observed_value = add_one_and_multiply_by_two ( 2 ) assert observed_value == expected_value The definition of a code unit is somewhat ambiguous, making the distinction between integration tests and unit tests a bit unclear. Integration tests can range from extremely simple to highly complex, contrasting with unit tests. If a function or class merely amalgamates two or more unit-tested code pieces, an integration test is necessary. If a function introduces new untested behavior, a unit test is needed. The structure of integration tests closely resembles that of unit tests, comparing expected results with observed values. However, deriving the expected result or preparing the code for execution can be significantly more complex. Integration tests are generally more time-consuming due to their extensive nature. This distinction is helpful to differentiate between straightforward (unit) and more nuanced (integration) test-writing requirements.","title":"Integration tests"},{"location":"arcadia-users-group/20231104-testing-concepts/lesson/#regression-tests","text":"Regression tests refer to past outputs for expected behavior. The anticipated outcome is based on previous computations for the same inputs. Regression tests hold the past as \"correct.\" They notify developers about how and when a codebase has evolved such that it produces different results. However, they don't provide insights into why the changes occurred. The discrepancy between current and previous code outputs is termed a regression. Like integration tests, regression tests are high-level and often encompass the entire code base. A prevalent regression test strategy extends across multiple code versions. For instance, an input file for version X of a workflow is processed, and the output file is saved, typically online. While developing version Y, the test suite automatically fetches the output for version X, processes the same input file for version Y, and contrasts the two output files. Any significant discrepancies trigger a test failure. Regression tests can identify failures missed by integration and unit tests. Each project may adopt a slightly varied approach to regression testing, based on its software requirements. Testing frameworks aid in constructing regression tests but don\u2019t provide additional sophistication beyond the discussed concepts.","title":"Regression tests"},{"location":"arcadia-users-group/20231104-testing-concepts/lesson/#continuous-integration","text":"Continuous integration makes running tests as easy as possible by integrating the test suite into the development process. Every time a change is made to the repository, the continuous integration system builds and checks that code. Based on the instructions you provide, a continuous integration server can: check out new code from a repository spin up instances of supported operating systems (i.e. various versions of OSX, Linux, Windows, etc.). spin up those instances with different software versions (i.e. python 2.7 and python 3.0) run the build and test scripts check for errors report the results. Since the first step the server conducts is to check out the code from a repository, we'll need to put our code online to make use of this kind of server.","title":"Continuous integration"},{"location":"arcadia-users-group/20231104-testing-concepts/lesson/#set-up-a-mean-git-repository-on-github","text":"Our mean.py test_mean.py files can be the contents of a repository on GitHub. Go to GitHub and create a repository called aug-mean. Do this in your own user account and don't add any files to the new repo (no README/LICENSE, etc.). Turn the aug-mean directory that we've been working in on your computer into a git repository following the \"\u2026or create a new repository on the command line\" instructions on GitHub: echo \"# aug-mean\" >> README.md git init git add README.md git commit -m \"first commit\" git branch -M main git remote add origin git@github.com:yourusername/aug-mean.git git push -u origin main Create a new branch ( git checkout -b yourinitials/init ). Use git to add , commit , and push the two files mean.py and test_mean.py to GitHub.","title":"Set Up a Mean Git Repository on GitHub"},{"location":"arcadia-users-group/20231104-testing-concepts/lesson/#github-actions","text":"GitHub Actions is a continuous integration service provided by GitHub. It's integrated directly into GitHub repositories and does not require additional accounts or external services. Note that GitHub Actions usage is free for standard GitHub-hosted runners in public repositories, and for self-hosted runners. For private repositories, each GitHub account receives a certain amount of free minutes and storage for use with GitHub-hosted runners, depending on the account's plan (see here for more information). To use GitHub Actions, create a directory called .github and within it, create another directory called workflows . mkdir -p .github/workflows Inside the workflows directory, you can create a YAML file (e.g. ci.yml ) to define your continuous integration process: name : pytest CI on : [ push , pull_request ] jobs : build : runs-on : ubuntu-latest steps : - uses : actions/checkout@v2 - name : Set up Python 3.10 uses : actions/setup-python@v2 with : python-version : '3.10' - name : Install dependencies run : | python -m pip install --upgrade pip pip install -r requirements.txt - name : Run tests run : pytest Below, we break down how this workflow works: * name : Our workflow is named pytest CI . * on : The workflow will be triggered to run automatically whenever commits are pushed to the repository or whenever a pull request is created or updated with new commits. * jobs : Specifies what the workflow will actually run. In this case, we specify that we want to run on the latest version of the ubuntu operating system using python version 3.10. These instructions are enough to launch a computer with python running on it. Then, we specify that we want to install dependencies from a requirements.txt file using pip. Lastly, we run our tests using pytest . Running GitHub Actions workflows on multiple operating systems using a matrix Often times, developers want to check that their tests will pass not just with an ubuntu operating system and one version of Python, but with many operating systems and many versions of Python. This can be done using a matrix, which we demonstrate below. We specify lists of operating systems and python versions that we want to run our CI with, and then use a matrix call to run them all. name : pytest CI on : [ push , pull_request ] jobs : build : runs-on : ${{ matrix.os }} strategy : matrix : os : [ ubuntu-latest , macos-latest , windows-latest ] python-version : [ '3.7' , '3.8' , '3.9' , '3.10' , '3.11' ] steps : - uses : actions/checkout@v2 - name : Set up Python ${{ matrix.python-version }} uses : actions/setup-python@v2 with : python-version : ${{ matrix.python-version }} - name : Install dependencies run : | python -m pip install --upgrade pip pip install -r requirements.txt - name : Run tests run : pytest","title":"GitHub Actions"},{"location":"arcadia-users-group/20231104-testing-concepts/lesson/#triggering-ci","text":"Add .github/workflows/ci.yml to your repository Commit and push it. Open a pull request with your changes. Check how the CI is going in the PR. Right now, the CI is failing. Why is that? When we set up for today's lesson, we used conda to install pytest. In the CI workflow, we specified that our dependencies (in this case pytest) should be installed from a requirements.txt file. That requirements.txt file is a conventional way to list all of the python packages that we need. We haven't created that file yet. Let's go ahead an create it, add it, commit it, and push it. Since we need pytest, the requirements.txt file looks like this: pytest==7.4.3 Pushing new changes to our branch automatically re-triggers the CI tests to re-run. When all of our tests pass, we'll see a big green check mark next stating that \"All checks have passed\" and a green check next to our commit in our commit history. If some of your checks don't pass, you can see what went wrong by clicking on the check which will launch the runtime information. Also note that while in this example we use pip and a requirements.txt file to install dependencies, dependencies can be installed in other ways such as with conda .","title":"Triggering CI"},{"location":"arcadia-users-group/20231128-testing-datasets/lesson/","text":"Building test data sets In our previous lesson, we covered different types of tests. In the unit test section, we used five functions to test that our mean() function produced the output that we expected it to. For each of these functions, we started by defining our test data, num_list , a list of numbers for which we wanted to calculate the mean. These test data were small; each list only had five numbers in it. The \"correct\" answer to the test was clear with relatively little human effort (we could take a few seconds to calculate the mean of each list in our own heads). The creation of each test data set was encoded in the test itself. Below we discuss strategies for creating test data sets to help maintain some of these desirable qualities in real code bases. Lesson set up Software installation This lesson will take advantage of the skills we've learned in many previous lessons. We'll use Jupyter Notebooks , GitHub , conda , and Python . This is more overhead than we typically strive for in a lesson, but we hope that it's a chance to practice these skills to achieve a new goal. In the future, we may provide a GitPod environment for learners to use while working through this lesson, however if possible we would prefer to empower users to start implementing tests on their own computers using their own setup. To start this lesson, we'll begin by creating a conda environment that has the tools we'll need. mamba create -n augtest2 jupyter pytest pandas matplotlib seaborn pillow conda activate augtest2 We'll also create a folder to help us stay organized. mkdir 20231128-aug-testing-lesson cd 20231128-aug-testing-lesson Once our environment is activated, start a Jupyter notebook jupyter notebook Example function As a running example, similar to the mean() function we used in the previous lesson, we'll use distribution() , a simple function that returns the distribution of characters in text. def distribution ( text ): return { char : text . count ( char ) / len ( text ) for char in text . lower () } As an aside, this function is intentionally implemented inefficiently; the same functionality can be implemented much faster in Python. We do this to later demonstrate what to do when a complex function -- of the type typically found in computational biology workflows -- is too slow to run on longer test inputs. If you're curious, the reason distribution() is inefficient is that an input text of length n will be read n^2 times -- for each character in the text, count() is called, and it will in turn traverse the entire input. Instead, a single pass over the text can be used to keep running counts of the characters observed. Let's try it: distribution('testing datasets') {'t': 0.25, 'e': 0.125, 's': 0.1875, 'i': 0.0625, 'n': 0.0625, 'g': 0.0625, ' ': 0.0625, 'd': 0.0625, 'a': 0.125} Test data for functions and scripts Creating data within the test itself As seen in our previous lesson and described above, one strategy is to create test data within the test using built-in functions or data structures. When possible, this is a great strategy. These test data are small and self-contained and the correct answer to the test is easy to figure out. Functions like [] (list creation), or range() in Python and c() , rep() , seq() , data.frame() , and list() in R can be used to create test data that are relevant to a function. Some data-creation functions might be non-deterministic, but setting the seed ensures that the same output are produced each time. Often, we'll test a combination of \"typical\" inputs for which we verified the expected outputs, alongside edge cases - empty, very large, or nonsensical inputs. For distribution() , a typical in-test set of test cases could include: DISTRIBUTION_INPUTS_AND_EXPECTED_OUTPUTS = [ ( '' , {}), # An empty string should produce an empty output ( 'abcc' , { 'a' : 0.25 , 'b' : 0.25 , 'c' : 0.5 }), # Typical short input ( 100 , None ), # Nonsensical input (wrong type) ... ] Testing input files Some of the functions or workflows you will seek to test may take input files and produce output files. We'll extend our example function to to the same: def distribution_in_file ( filepath_in , filepath_out ): with open ( filepath_in ) as file_in , open ( filepath_out , 'w' ) as file_out : counts = distribution ( file_in . read ()) file_out . write ( str ( counts )) Test input files are often checked in alongside the code and the tests. For simplicity, we'll just use files that are already present in the operating system. Let's try it: distribution_in_file('/usr/share/dict/propernames', '/tmp/propernames.txt') And examine the results: cat /tmp/propernames.txt { 'a' : 0 .0917388251813714, 'r' : 0 .06576175988766675, 'o' : 0 .0452843435525392, 'n' : 0 .06798502223262345, '\\n' : 0 .15305406037912472, 'd' : 0 .021647554411420546, 'm' : 0 .014158670723145332, 'l' : 0 .04118886028551369, 'i' : 0 .06482564942663234, 'g' : 0 .00877603557219752, 't' : 0 .03276386613620407, 'h' : 0 .023636789141118653, 'e' : 0 .08190966534051018, 's' : 0 .025626023870816757, 'b' : 0 .005967704189094313, 'j' : 0 .002808331383103206, 'x' : 0 .0015211794991809033, 'f' : 0 .0062017318043529135, 'v' : 0 .007020828457758015, 'y' : 0 .024221858179265154, 'w' : 0 .003627428036508308, 'u' : 0 .021998595834308448, 'c' : 0 .015562836414696935, 'k' : 0 .010999297917154224, 'p' : 0 .005148607535689211, 'z' : 0 .0025743037678446056, 'q' : 0 .000468055230517201, '-' : 0 .0002340276152586005 } What is the /usr/share/dict/propernames file? The /usr/share/dict/propernames on macOS systems is is a text file that contains a list of common proper names, one per line. It's similar to the words file located at /usr/share/dict/words which contains a list of common words. These files are typically used by spell-checking programs to verify the spelling of words and proper names. Comparing file outputs We can directly compare an output file to a \"reference\" or \"golden\" output that is accessible to the test. Checksums can also be used to compare the results of a function against other types of files. For example, if you have a function that produces a data frame, you can write that object to a CSV file. Then, you could compare the output CSV file against a reference CSV file by their checksums. When using this strategy, it might be necessary to sort your data frame prior to writing a file. An example of comparing checksums: import hashlib with open ( 'expected.txt' , 'rb' ) as expected , '/tmp/propernames.txt' , 'rb' ) as actual : assert hashlib . md5 ( expected . read ()) . hexdigest () == hashlib . md5 ( actual . read ()) . hexdigest () Checksums can also be used to compare the results of a function against other types of files. For example, if you have a function that produces a data frame, you can write that object to a CSV file. Then, you could compare the output CSV file against a reference CSV file by their checksums. When using this strategy, it might be necessary to sort your data frame prior to writing a file. Testing image outputs Often times, a function or a script produces an image output. Images are trickier to test than some other outputs, but can still be used in unit tests. If a function produces the image and an image object is produced in your coding environment, you can sometimes check the metadata of the image to make sure it matches expected values. This includes dimensions, color profiles, or file formats. These types of tests have the built-in benefit of testing whether the function produces an image successfully on top of confirming attributes about the image. Even if you check metadata attributes about an image, the image itself might still be different than expected. If you want to test the output image more thoroughly, you can compare the output produced by a test to a reference image stored in your test data. Both checksum comparisons and pixel comparisons are useful in this scenario. We'll create another function to generate an image -- a histogram of letter counts: import matplotlib.pyplot as plt import seaborn as sns import pandas as pd def histogram ( file_in , file_out ): with open ( filepath_in ) as file_in , open ( filepath_out , 'w' ) as file_out : counts = distribution ( file_in . read ()) df = pd . DataFrame ( list ( counts . items ()), columns = [ 'Character' , 'Count' ] ) . sort_values ( by = 'Character' ) sns . barplot ( data = df , x = 'Character' , y = 'Count' ) plt . savefig ( filepath_out ) Let's try it: histogram('/usr/share/dict/propernames', '/tmp/propernames.png') To compare generated images to ground truth, we can use checksums as before, or we can visually compare them. For this, we'll use a Python image processing library. from PIL import Image , ImageChops def image_diff ( file1 , file2 , file_out ): \"\"\"Compare file1 and file2; if different, store diff to file_out and assert.\"\"\" img1 = Image . open ( file1 ) img2 = Image . open ( file2 ) diff = ImageChops . difference ( img1 , img2 ) if diff . getbbox (): # Diff is the same as an empty image, i.e. `img1` and `img2` are identical. return True else : # Images differ, store a comparison of them. ImageChops . blend ( img1 , diff , alpha = 0.5 ) . save ( file_out ) assert False , f \"Images differ, comparison in { file_out } \" For example, the histograms of the two samples discussed earlier -- the head of /usr/share/dict/web2a and a random sample from it - look relatively similar: But applying image_diff() highlights areas that have changed: Test data sets for workflow integration tests Identifying a small data set that will quickly run through the entire workflow can be tricky. Below we cover some strategies to help achieve this. Subsetting input data Sometimes, typical inputs (say, genomics data or images) are large and result in slow test times. For example, let's try distribution_in_file on a larger input (feel free to terminate the cell, if it takes too long): distribution_in_file ( '/usr/share/dict/web2a' , '/tmp/web2a.txt' ) Depending on the type of machine, this could take several minutes -- certainly more than we'd like a test to run. In this case, subsetting the data could shorten the test time significantly, while still checking core functionality. Sometimes, you can subset your data by running head or tail on your input files, or sub sampling input samples to a lower number, and your workflow will still run. This is a great option when it works! For example, let's try taking just the first 1,000 lines of /usr/share/dict/web2a as a test file: !head -1000 /usr/share/dict/web2a > /tmp/web2a.head distribution_in_file('/tmp/web2a.head', '/tmp/web2a.head.txt') Runtime is much faster. Unfortunately, results are significantly different - because the input file is sorted, we're testing words that are more likely to start with letters like 'a', skewing the character distribution. Examining /tmp/web2a.txt vs /tmp/web2a.head.txt , we see that the letter 'a' appears at a frequency of 6.7% in the former, but 12.1% in the latter. In this case can try sampling random lines from the file, instead of the first lines: !sort -R /usr/share/dict/web2a | head -1000 > /tmp/web2a.rand distribution_in_file('/tmp/web2a.rand', '/tmp/web2a.rand.txt') The frequency of 'a' is again 6.7%, and the distribution as a whole more similar to the distribution over the full dataset. Because it's relatively fast to subset data this way, you can always try this strategy first and see if produces a representative test data set before moving on to other strategies. Often times, with biological files, simple subsetting strategies like this will fail in even more problematic ways -- subsampling in this manner can cause some tools in your workflow to produce no results, rendering the rest of the workflow unable to run or producing blank, meaningless outputs. Subset input data by results One strategy to circumvent this issue is to run the workflow from start to finish on a full data set and then to cleverly subset the results. For example, if you are writing a workflow that performs genome assembly, you could assemble all reads, map the reads back to your assembly, and extract all of the reads that map to one contiguous sequence. This would effectively subset the number of reads you're working with but ensure that those reads will still assemble. Making databases small This is true for databases as well. Many workflows will rely on a database for at least one step -- identifying BLAST hits, performing functional annotation, etc. The size of the database will often be the driving factor in search times. When possible, you can generate a small test database as a drop in replacement. Sometimes, the developers of a tools will have already generated a small test database that you can use (see this issue for an example for the EggNOG ortholog annotation tool ). Other times, you can take the same strategy as suggested above: run a full data set through your pipeline, identify a database hit that is in your data, and then subset the database and your input data to retain information relevant only to that hit. Parameterization to keep tests lightweight You may need to introduce new user-controlled parameters into your workflow to reduce run times or API calls. For example, if your workflow queries a database or an API and by default returns the top 500 hits, you may need to parameterize this value to reduce the number of returned hits to something much smaller (like 3). (An alternative in this case would be to select a test data set that only returns a small number of hits because it doesn't have many matches in the database.) Parameterization can also reduce run time or RAM required to run your pipeline. For example, the default k-mer length in MMSeq2 clustering uses more RAM than is available for GitHub Actions workflows; parameterizing this value reduces the RAM required to run an integration test on workflows that use MMSeqs2. Guiding principles for test data set creation Small data : Data should be small and run fast. Even for large multi-step workflows or big software libraries, tests shouldn't take more than a few minutes to run. The data itself should be storable in a GitHub repository and should be as small as possible to reliably capture the behavior of the tool or workflow. If possible, files should be very small, but files under 50M and repos under 1G is reasonable when necessary. Note that GitHub disallows files larger than 100M. Address known bugs : Create new test each time someone submits an issue identifying a bug in your code. If possible, build and test and use test data from a minimum reproducible example that recreates the behavior of the error. Representative data : The test data should be representative of the actual data the software will handle. It should cover a variety of data types, structures, and edge cases to ensure the software behaves as expected in different scenarios. Deterministic results : Choose test data that will produce deterministic results, making the tests predictable and repeatable. This is crucial for verifying the accuracy and consistency of the software over time. Modular and reusable : When possible, design test data in a modular fashion, making it reusable across different tests. This promotes efficiency and consistency in testing. Documented : Document the source and structure of the test data, along with any assumptions or simplifications made. This helps other developers understand the purpose and limitations of the test data.","title":"Building test data sets"},{"location":"arcadia-users-group/20231128-testing-datasets/lesson/#building-test-data-sets","text":"In our previous lesson, we covered different types of tests. In the unit test section, we used five functions to test that our mean() function produced the output that we expected it to. For each of these functions, we started by defining our test data, num_list , a list of numbers for which we wanted to calculate the mean. These test data were small; each list only had five numbers in it. The \"correct\" answer to the test was clear with relatively little human effort (we could take a few seconds to calculate the mean of each list in our own heads). The creation of each test data set was encoded in the test itself. Below we discuss strategies for creating test data sets to help maintain some of these desirable qualities in real code bases.","title":"Building test data sets"},{"location":"arcadia-users-group/20231128-testing-datasets/lesson/#lesson-set-up","text":"","title":"Lesson set up"},{"location":"arcadia-users-group/20231128-testing-datasets/lesson/#software-installation","text":"This lesson will take advantage of the skills we've learned in many previous lessons. We'll use Jupyter Notebooks , GitHub , conda , and Python . This is more overhead than we typically strive for in a lesson, but we hope that it's a chance to practice these skills to achieve a new goal. In the future, we may provide a GitPod environment for learners to use while working through this lesson, however if possible we would prefer to empower users to start implementing tests on their own computers using their own setup. To start this lesson, we'll begin by creating a conda environment that has the tools we'll need. mamba create -n augtest2 jupyter pytest pandas matplotlib seaborn pillow conda activate augtest2 We'll also create a folder to help us stay organized. mkdir 20231128-aug-testing-lesson cd 20231128-aug-testing-lesson Once our environment is activated, start a Jupyter notebook jupyter notebook","title":"Software installation"},{"location":"arcadia-users-group/20231128-testing-datasets/lesson/#example-function","text":"As a running example, similar to the mean() function we used in the previous lesson, we'll use distribution() , a simple function that returns the distribution of characters in text. def distribution ( text ): return { char : text . count ( char ) / len ( text ) for char in text . lower () } As an aside, this function is intentionally implemented inefficiently; the same functionality can be implemented much faster in Python. We do this to later demonstrate what to do when a complex function -- of the type typically found in computational biology workflows -- is too slow to run on longer test inputs. If you're curious, the reason distribution() is inefficient is that an input text of length n will be read n^2 times -- for each character in the text, count() is called, and it will in turn traverse the entire input. Instead, a single pass over the text can be used to keep running counts of the characters observed. Let's try it: distribution('testing datasets') {'t': 0.25, 'e': 0.125, 's': 0.1875, 'i': 0.0625, 'n': 0.0625, 'g': 0.0625, ' ': 0.0625, 'd': 0.0625, 'a': 0.125}","title":"Example function"},{"location":"arcadia-users-group/20231128-testing-datasets/lesson/#test-data-for-functions-and-scripts","text":"","title":"Test data for functions and scripts"},{"location":"arcadia-users-group/20231128-testing-datasets/lesson/#creating-data-within-the-test-itself","text":"As seen in our previous lesson and described above, one strategy is to create test data within the test using built-in functions or data structures. When possible, this is a great strategy. These test data are small and self-contained and the correct answer to the test is easy to figure out. Functions like [] (list creation), or range() in Python and c() , rep() , seq() , data.frame() , and list() in R can be used to create test data that are relevant to a function. Some data-creation functions might be non-deterministic, but setting the seed ensures that the same output are produced each time. Often, we'll test a combination of \"typical\" inputs for which we verified the expected outputs, alongside edge cases - empty, very large, or nonsensical inputs. For distribution() , a typical in-test set of test cases could include: DISTRIBUTION_INPUTS_AND_EXPECTED_OUTPUTS = [ ( '' , {}), # An empty string should produce an empty output ( 'abcc' , { 'a' : 0.25 , 'b' : 0.25 , 'c' : 0.5 }), # Typical short input ( 100 , None ), # Nonsensical input (wrong type) ... ]","title":"Creating data within the test itself"},{"location":"arcadia-users-group/20231128-testing-datasets/lesson/#testing-input-files","text":"Some of the functions or workflows you will seek to test may take input files and produce output files. We'll extend our example function to to the same: def distribution_in_file ( filepath_in , filepath_out ): with open ( filepath_in ) as file_in , open ( filepath_out , 'w' ) as file_out : counts = distribution ( file_in . read ()) file_out . write ( str ( counts )) Test input files are often checked in alongside the code and the tests. For simplicity, we'll just use files that are already present in the operating system. Let's try it: distribution_in_file('/usr/share/dict/propernames', '/tmp/propernames.txt') And examine the results: cat /tmp/propernames.txt { 'a' : 0 .0917388251813714, 'r' : 0 .06576175988766675, 'o' : 0 .0452843435525392, 'n' : 0 .06798502223262345, '\\n' : 0 .15305406037912472, 'd' : 0 .021647554411420546, 'm' : 0 .014158670723145332, 'l' : 0 .04118886028551369, 'i' : 0 .06482564942663234, 'g' : 0 .00877603557219752, 't' : 0 .03276386613620407, 'h' : 0 .023636789141118653, 'e' : 0 .08190966534051018, 's' : 0 .025626023870816757, 'b' : 0 .005967704189094313, 'j' : 0 .002808331383103206, 'x' : 0 .0015211794991809033, 'f' : 0 .0062017318043529135, 'v' : 0 .007020828457758015, 'y' : 0 .024221858179265154, 'w' : 0 .003627428036508308, 'u' : 0 .021998595834308448, 'c' : 0 .015562836414696935, 'k' : 0 .010999297917154224, 'p' : 0 .005148607535689211, 'z' : 0 .0025743037678446056, 'q' : 0 .000468055230517201, '-' : 0 .0002340276152586005 } What is the /usr/share/dict/propernames file? The /usr/share/dict/propernames on macOS systems is is a text file that contains a list of common proper names, one per line. It's similar to the words file located at /usr/share/dict/words which contains a list of common words. These files are typically used by spell-checking programs to verify the spelling of words and proper names.","title":"Testing input files"},{"location":"arcadia-users-group/20231128-testing-datasets/lesson/#comparing-file-outputs","text":"We can directly compare an output file to a \"reference\" or \"golden\" output that is accessible to the test. Checksums can also be used to compare the results of a function against other types of files. For example, if you have a function that produces a data frame, you can write that object to a CSV file. Then, you could compare the output CSV file against a reference CSV file by their checksums. When using this strategy, it might be necessary to sort your data frame prior to writing a file. An example of comparing checksums: import hashlib with open ( 'expected.txt' , 'rb' ) as expected , '/tmp/propernames.txt' , 'rb' ) as actual : assert hashlib . md5 ( expected . read ()) . hexdigest () == hashlib . md5 ( actual . read ()) . hexdigest () Checksums can also be used to compare the results of a function against other types of files. For example, if you have a function that produces a data frame, you can write that object to a CSV file. Then, you could compare the output CSV file against a reference CSV file by their checksums. When using this strategy, it might be necessary to sort your data frame prior to writing a file.","title":"Comparing file outputs"},{"location":"arcadia-users-group/20231128-testing-datasets/lesson/#testing-image-outputs","text":"Often times, a function or a script produces an image output. Images are trickier to test than some other outputs, but can still be used in unit tests. If a function produces the image and an image object is produced in your coding environment, you can sometimes check the metadata of the image to make sure it matches expected values. This includes dimensions, color profiles, or file formats. These types of tests have the built-in benefit of testing whether the function produces an image successfully on top of confirming attributes about the image. Even if you check metadata attributes about an image, the image itself might still be different than expected. If you want to test the output image more thoroughly, you can compare the output produced by a test to a reference image stored in your test data. Both checksum comparisons and pixel comparisons are useful in this scenario. We'll create another function to generate an image -- a histogram of letter counts: import matplotlib.pyplot as plt import seaborn as sns import pandas as pd def histogram ( file_in , file_out ): with open ( filepath_in ) as file_in , open ( filepath_out , 'w' ) as file_out : counts = distribution ( file_in . read ()) df = pd . DataFrame ( list ( counts . items ()), columns = [ 'Character' , 'Count' ] ) . sort_values ( by = 'Character' ) sns . barplot ( data = df , x = 'Character' , y = 'Count' ) plt . savefig ( filepath_out ) Let's try it: histogram('/usr/share/dict/propernames', '/tmp/propernames.png') To compare generated images to ground truth, we can use checksums as before, or we can visually compare them. For this, we'll use a Python image processing library. from PIL import Image , ImageChops def image_diff ( file1 , file2 , file_out ): \"\"\"Compare file1 and file2; if different, store diff to file_out and assert.\"\"\" img1 = Image . open ( file1 ) img2 = Image . open ( file2 ) diff = ImageChops . difference ( img1 , img2 ) if diff . getbbox (): # Diff is the same as an empty image, i.e. `img1` and `img2` are identical. return True else : # Images differ, store a comparison of them. ImageChops . blend ( img1 , diff , alpha = 0.5 ) . save ( file_out ) assert False , f \"Images differ, comparison in { file_out } \" For example, the histograms of the two samples discussed earlier -- the head of /usr/share/dict/web2a and a random sample from it - look relatively similar: But applying image_diff() highlights areas that have changed:","title":"Testing image outputs"},{"location":"arcadia-users-group/20231128-testing-datasets/lesson/#test-data-sets-for-workflow-integration-tests","text":"Identifying a small data set that will quickly run through the entire workflow can be tricky. Below we cover some strategies to help achieve this.","title":"Test data sets for workflow integration tests"},{"location":"arcadia-users-group/20231128-testing-datasets/lesson/#subsetting-input-data","text":"Sometimes, typical inputs (say, genomics data or images) are large and result in slow test times. For example, let's try distribution_in_file on a larger input (feel free to terminate the cell, if it takes too long): distribution_in_file ( '/usr/share/dict/web2a' , '/tmp/web2a.txt' ) Depending on the type of machine, this could take several minutes -- certainly more than we'd like a test to run. In this case, subsetting the data could shorten the test time significantly, while still checking core functionality. Sometimes, you can subset your data by running head or tail on your input files, or sub sampling input samples to a lower number, and your workflow will still run. This is a great option when it works! For example, let's try taking just the first 1,000 lines of /usr/share/dict/web2a as a test file: !head -1000 /usr/share/dict/web2a > /tmp/web2a.head distribution_in_file('/tmp/web2a.head', '/tmp/web2a.head.txt') Runtime is much faster. Unfortunately, results are significantly different - because the input file is sorted, we're testing words that are more likely to start with letters like 'a', skewing the character distribution. Examining /tmp/web2a.txt vs /tmp/web2a.head.txt , we see that the letter 'a' appears at a frequency of 6.7% in the former, but 12.1% in the latter. In this case can try sampling random lines from the file, instead of the first lines: !sort -R /usr/share/dict/web2a | head -1000 > /tmp/web2a.rand distribution_in_file('/tmp/web2a.rand', '/tmp/web2a.rand.txt') The frequency of 'a' is again 6.7%, and the distribution as a whole more similar to the distribution over the full dataset. Because it's relatively fast to subset data this way, you can always try this strategy first and see if produces a representative test data set before moving on to other strategies. Often times, with biological files, simple subsetting strategies like this will fail in even more problematic ways -- subsampling in this manner can cause some tools in your workflow to produce no results, rendering the rest of the workflow unable to run or producing blank, meaningless outputs.","title":"Subsetting input data"},{"location":"arcadia-users-group/20231128-testing-datasets/lesson/#subset-input-data-by-results","text":"One strategy to circumvent this issue is to run the workflow from start to finish on a full data set and then to cleverly subset the results. For example, if you are writing a workflow that performs genome assembly, you could assemble all reads, map the reads back to your assembly, and extract all of the reads that map to one contiguous sequence. This would effectively subset the number of reads you're working with but ensure that those reads will still assemble.","title":"Subset input data by results"},{"location":"arcadia-users-group/20231128-testing-datasets/lesson/#making-databases-small","text":"This is true for databases as well. Many workflows will rely on a database for at least one step -- identifying BLAST hits, performing functional annotation, etc. The size of the database will often be the driving factor in search times. When possible, you can generate a small test database as a drop in replacement. Sometimes, the developers of a tools will have already generated a small test database that you can use (see this issue for an example for the EggNOG ortholog annotation tool ). Other times, you can take the same strategy as suggested above: run a full data set through your pipeline, identify a database hit that is in your data, and then subset the database and your input data to retain information relevant only to that hit.","title":"Making databases small"},{"location":"arcadia-users-group/20231128-testing-datasets/lesson/#parameterization-to-keep-tests-lightweight","text":"You may need to introduce new user-controlled parameters into your workflow to reduce run times or API calls. For example, if your workflow queries a database or an API and by default returns the top 500 hits, you may need to parameterize this value to reduce the number of returned hits to something much smaller (like 3). (An alternative in this case would be to select a test data set that only returns a small number of hits because it doesn't have many matches in the database.) Parameterization can also reduce run time or RAM required to run your pipeline. For example, the default k-mer length in MMSeq2 clustering uses more RAM than is available for GitHub Actions workflows; parameterizing this value reduces the RAM required to run an integration test on workflows that use MMSeqs2.","title":"Parameterization to keep tests lightweight"},{"location":"arcadia-users-group/20231128-testing-datasets/lesson/#guiding-principles-for-test-data-set-creation","text":"Small data : Data should be small and run fast. Even for large multi-step workflows or big software libraries, tests shouldn't take more than a few minutes to run. The data itself should be storable in a GitHub repository and should be as small as possible to reliably capture the behavior of the tool or workflow. If possible, files should be very small, but files under 50M and repos under 1G is reasonable when necessary. Note that GitHub disallows files larger than 100M. Address known bugs : Create new test each time someone submits an issue identifying a bug in your code. If possible, build and test and use test data from a minimum reproducible example that recreates the behavior of the error. Representative data : The test data should be representative of the actual data the software will handle. It should cover a variety of data types, structures, and edge cases to ensure the software behaves as expected in different scenarios. Deterministic results : Choose test data that will produce deterministic results, making the tests predictable and repeatable. This is crucial for verifying the accuracy and consistency of the software over time. Modular and reusable : When possible, design test data in a modular fashion, making it reusable across different tests. This promotes efficiency and consistency in testing. Documented : Document the source and structure of the test data, along with any assumptions or simplifications made. This helps other developers understand the purpose and limitations of the test data.","title":"Guiding principles for test data set creation"},{"location":"arcadia-users-group/20240206-intro-to-formatting-and-linting/lesson/","text":"Introduction to code formatting, linting, and style guides This lesson explains some of the basic conventions around code formatting and style, with an emphasis on Python. It also explains how to use tools to automatically format code and check (or \"lint\") it for compliance with these style conventions. The importance of defining a consistent format and style for the code we write arises from two unfortunate realities: code is easier to write than it is to read, and code is read more often than it is written (or rewritten). Code is easier to write than read because, when we are writing it, we usually know exactly what we are trying to do and we have the context needed to understand why and how we're doing it. By contrast, neither of these two things are typically true when we are reading code: we often either don't remember or never knew exactly what the author was trying to do when they wrote the code we're reading, and the context necessary to understand the code is invariably also unknown to us or at least not fresh in our minds. This means that, when writing code, we need to have a system of rules and guidelines in place to ensure that the code we write is as easy as possible for others (either our future selves or other developers) to read and understand. This is the purpose of code formatting and style conventions. (At a higher level, this is also, in part, the purpose of design patterns and application architectures, but that is a topic for a future AUG lesson.) These conventions are particularly important for us at Arcadia, as most of our code is developed collaboratively. This necessarily means that multiple people will read it. In addition, we often intend for the code we produce to be useful and understandable to others outside of Arcadia, and part of that is ensuring our code meets high standards for readability and comprehensibility. An introductory example As an example to help illustrate why code formatting and style are so important, consider the following code: def qr ( a , b , c ): return - ( b - np . sqrt ( b ** 2 - 4 * a * c )) / 2 / a ,( - b - np . sqrt ( b ** 2 - 4 * a * c )) / 2 / a It is very hard to read this code, let alone understand what it does. Now consider the same code written in a more readable style: def calculate_quadratic_roots ( quadratic_coeff , linear_coeff , constant_coeff ): discriminant_square_root = np . sqrt ( linear_coeff ** 2 - 4 * quadratic_coeff * constant_coeff ) root_1 = ( - linear_coeff + discriminant_square_root ) / ( 2 * quadratic_coeff ) root_2 = ( - linear_coeff - discriminant_square_root ) / ( 2 * quadratic_coeff ) return root_1 , root_2 Notice how this code is easier to read and understand (if still somewhat opaque). This is due to several formatting and style changes. For one, spaces are used around the operators and the single long line has been broken into four separate lines, each of which does one specific thing. These changes generally makes code easier to read; they are a kind of formatting convention. In addition, the variable names are much longer and are more descriptive, which make it easier for reader to infer the meaning and intent the code; this is a kind of style convention. In this lesson, we'll discuss both formatting and style conventions, as well as tools to automatically format code and check that is complies with our style conventions. Lesson setup We'll create a Python project to demonstrate how to use the tools we'll be discussing. Create a new conda environment and activate it: mamba create -y -n aug-linting-lesson python = 3 .11 mamba activate aug-linting-lesson Create a new directory for the project: mkdir 20240206 -aug-linting-lesson cd 20240206 -aug-linting-lesson Finally, create a new file called main.py . We'll add code to this file later. Formatting conventions In programming, \"formatting\" refers to how code is laid out or arranged on the page or screen; for example, how many spaces to use for indentation, where to put spaces around operators, where to use line breaks (and how many), whether to use single or double quotes for strings, and so on. In Python and most other languages, these questions are not settled by the syntax of the language itself, so it is up to each programmer (or project, or team, or organization) to decide for themselves (or itself) how to resolve them. Unfortunately, because most formatting decisions are a matter of personal preference, a wide range of (often strongly-held) opinions about how code should be formatted has emerged. However, there is wide agreement that code should be formatted consistently . That is, once a set of formatting rules have been chosen, they should be applied consistently throughout a given codebase. This is because inconsistent formatting makes code significantly harder to read and maintain. More subtly, inconsistent formatting also pollutes the version history of a codebase with unnecessary and superficial changes, making it harder to understand the \"real\" or substantive changes that were made to the code. Example: breaking up long lines One important formatting convention is that lines of code should not be longer than some maximum length. This is to ensure that code is readable on a wide range of screen sizes. (In Python and many other languages, a maximum line length of 80 or 100 characters is typically chosen.) The tricky part lies in determining how to break up lines that are longer than this maximum length; there are many options and no technical reason to prefer one over another. For example, consider the following code: def calculate_total_income ( gross_wages , taxable_interest , total_dividends , qualified_dividends , other_income ): total_income = gross_wages + taxable_interest + ( total_dividends - qualified_dividends ) + other_income return total_income It is clear that the function definition is too long to fit in one line, so it needs to be broken up. But where should the line breaks go? One option is to break up the long lines after each comma or operator: def calculate_total_income ( gross_wages , taxable_interest , total_dividends , qualified_dividends , other_income ): total_income = ( gross_wages + taxable_interest + ( total_dividends - qualified_dividends ) + other_income ) return total_income Another option is to use a separate line for each argument and also for the closing parentheses: def calculate_total_income ( gross_wages , taxable_interest , total_dividends , qualified_dividends , other_income ): total_income = ( gross_wages + taxable_interest + ( total_dividends - qualified_dividends ) + other_income ) return total_income Although these two versions are syntactically identical and formatted similarly, the formatting rules they obey are quite different. Even worse, a \"diff\" or line-by-line comparison of the two versions is very hard to read, since almost every line is different in some trivial way. This means that if a programmer both reformatted the code from one format to the other and, at the same time, made a substantive change to it (like renaming one of the arguments of the calculate_total_income function), it would be hard to quickly determine which lines were meaningfully changed and which were just reformatted. Code formatters enforce formatting conventions To both eliminate these kinds of formatting ambiguities and to automate the otherwise-tedious process of formatting code by hand, tools to automatically format code have been developed. These tools take files of source code as input and output the same code, but reformatted as necessary according to a pre-specified set of formatting rules. In Python, the most popular code formatting tool is called black . This tool both automatically formats code and imposes its own opinionated set of formatting rules (hence its tagline: \"any color you want\"). This eliminates the need for developers to make (or argue about) these decisions themselves. Although black 's formatting rules can take some getting used to, the benefit of eliminating almost all formatting-related decisions is usually worth the effort of adjusting to its opinions. Setting up black To install black in our conda environment, run: mamba install -y black Next, let's add some poorly formatted code to our toy project. Add the following code (from our previous example) to main.py : def calculate_total_income ( gross_wages , taxable_interest , total_dividends , qualified_dividends , other_income ): total_income = gross_wages + taxable_interest + ( total_dividends - qualified_dividends ) + other_income return total_income Now, from the command line, run: black . This command tells black to reformat all the Python files in the current directory (the lone dot in the command is the relative path to the current directory). By default, black considers any file ending with .py to be a Python file, and it will look for and reformat all such files in the directory path it is given (or any of that directory's subdirectories). Take a look at main.py again. You should see that the code has been reformatted according to black 's rules. Now, try simplifying the calculation of total_income by removing the two dividend-related variables from the sum, then run black again. You should see that, because the total_income line is shorter, it can fit on one line, so black automatically converts it back to a single line. There is a succinct overview of black 's formatting rules in its documentation . While it's good to be familiar with these rules, there is no need to memorize them. Indeed, the purpose of using a formatting tool like black is to eliminate the need to manually write your code according to any particular formatting conventions or even to think about formatting at all. Aside about black and ruff Since its release in 2018, black has played an influential role in standardizing code formatting across the Python ecosystem. However, it has now been reimplemented by a tool called ruff that is designed to be a fast and comprehensive tool for formatting and linting code in Python. This lesson uses black as an homage to its historical importance, but in practice, ruff is the tool we should be using for both formatting and linting. Style guides Although formatting conventions help constrain the very low-level aspects of how code is written, additional rules and conventions are required to ensure that code is both readable and understandable. These conventions are usually expressed in the form of style guides. Style guides define rules, conventions, and guidelines to which all code in a codebase should adhere. They concern both both low-level questions like how to name variables and functions, as well as higher-level considerations like documentation standards and some aspects of how code is organized or structured. PEP8 and Python style guides Many language have an official style guide. For Python, the official style guide is called PEP8 . In addition, Google's Python style guide is an important extension of PEP8 that defines additional standards, especially around nomenclature, documentation, and code organization. Most major Python projects follow PEP8, and we should strive to follow it at Arcadia as well, as it is a foundational part of ensuring our code is readable and useful to others. This is true no matter the scale of the project; it is a good practice to follow PEP8 even for small scripts and one-off analyses, as it is often hard to anticipate the lifetime of a project in advance. It is therefore prudent to assume that all of the code we write will always be read by others (inside, if not also outside, of Arcadia). As projects grow and it becomes more likely that we will need to maintain them for a long period of time, we can then readily impose additional style conventions on top of PEP8. Many of the conventions defined in PEP8 concern formatting and are automatically enforced by using black to format our code. However, PEP8 also includes style conventions that we must follow manually. Here, we'll discuss two important categories of style conventions: naming conventions and documentation conventions. Naming conventions Naming conventions are an important element of code style. They determine how variables, functions, classes, modules, and other objects should be named. In Python, there are some strict rules about how to name each of these things: variables, functions, and modules* should be named in lower_snake_case . class names should be UpperCamelCase . global variables should be named in ALL_CAPS_SNAKE_CASE . names cannot begin with a number or contain spaces or any special characters (except for underscores). *\"modules\" are the individual python files that are contained with a python project or package; e.g., main.py is a module. In other words, all names should be lower_snake_case except for global variables (which should be used sparingly, if at all) and class names. Note that instances of a class are variables and should be named in lower_snake_case , not UpperCamelCase . Here is an example: class ProteinSequence : def __init__ ( self , sequence ): self . sequence = sequence protein_sequence = ProteinSequence ( 'MSKGEELFTG' ) All names should be specific, descriptive, and unambiguous. When in doubt, err on the side of verbosity, and always avoid unnecessary abbreviations. Although the meaning of \"descriptive\" is obviously subjective, there are some general guidelines that all names, no matter how brief, should follow: Function names should generally begin with a verb that corresponds to what they do. Functions that return a boolean value should have a name of the form is_<something> or has_<something> , functions that calculate something calculate_<something> or if they modify an object in-place, they should be named either set_<something> or update_<something> . Variable names should not include type information (e.g., list_of_ints or title_str ), since Python is a dynamically typed language and the type of a variable can change at runtime. The same goes for functions (e.g. calculate_tm_score instead of calculate_tm_score_fn ). To express type information, type hints should be used (this is a topic for a future lesson, but if you're curious, check out this overview of type hints in Python ). Where possible, use the plural form of a noun for variables that contain a collection of things (e.g., for a list of proteins, use proteins instead of protein , protein_list , protein_set , etc). Here are some examples of good and bad variable names: # bad (too short, ambiguous) pids = [ 'P12345' , 'P23456' , 'P34567' ] # bad (includes type information) protein_id_list = [ 'P12345' , 'P23456' , 'P34567' ] # good protein_ids = [ 'P12345' , 'P23456' , 'P34567' ] # bad (ambiguous and not descriptive) def calculate ( protein1 , protein2 ): ... # bad (not lower_snake_case) def calculateTMscore ( protein1 , protein2 ): ... # good def calculate_tm_score ( protein1 , protein2 ): ... # bad (unnecessary abbreviations) usr_inpt = input ( 'Enter your name: ' ) # bad (ambiguous abbreviation) user_in = input ( 'Enter your name: ' ) # good user_input = input ( 'Enter your name: ' ) Aside about single-letter variable names Although PEP8 and other style guides do not explicitly forbid the use of single-letter variable names, they are strongly discouraged in most contexts. This is because single-letter variable names are, by definition, not descriptive. They can also be quite literally ambiguous; it is often hard to visually detect the difference between i , j , l , and 1 , for example. Finally, a subtler reason to avoid single-character variable names is that they can make refactoring harder, as it may be more cumbersome to search for and rename single-character variable names than more descriptive ones. One exception to this general prohibition should be for variables defined in a very local or narrow scope such as a short for loop or in a lambda function. Here are two examples where single-letter variable names is used within a single line (which is as narrow a scope as possible): # A single-letter variable name `f` used in a list comprehension. filepaths = [ 'data_1.txt' , 'data_2.txt' , 'data_3.csv' ] txt_filepaths = [ f for f in filepaths if f . endswith ( '.txt' )] # A single-letter variable name `x` used in a lambda function. numbers = [ 1 , 2 , 3 , 4 , 5 ] squared_numbers = list ( map ( lambda x : x ** 2 , numbers )) In cases such as these, you may decide that the brevity of this code outweighs the loss of readability. However, even in these cases, it is often possible to use more descriptive variable names without sacrificing much brevity: txt_filepaths = [ filepath for filepath in filepaths if filepath . endswith ( '.txt' )] squared_numbers = list ( map ( lambda value : value ** 2 , numbers )) Another scenario in which single-letter variable names may be acceptable is when implementing mathematical formulas as they appear in a publication or as they are typically written in the literature. In these cases, it may be clearer to retain the original single-letter names rather than replacing them with more descriptive names. For example, consider the formula for the force of gravity between two objects: F = G * m_1 * m_2 / d ** 2 Here, the formula is well-known and the meaning of G , m_1 , m_2 , and d would be clear to anyone who is familiar with the formula. Using longer, more descriptive variable names in this case could make the code harder to read: force_of_gravity = gravitational_constant * mass_1 * mass_2 / distance_mass_1_mass_2 ** 2 However, if the single-letter names used in an equation are also used in other places in the code, then it is probably better to use more descriptive names throughout, as the clarity of the single-letter names heavily depends on their use in the context of a well-known or well-documented equation. More about single-letter variable names in scientific programming Unfortunately, single-letter variable names seem to be common in scientific programming. As we discussed above, this may be acceptable when they are are used in the context a well-known mathematical formula. But in many cases, their use is simply a shortcut that comes at the price of readability. Consider the following example of iterating over the pixels in a timelapse image: # Create a random timelapse image with 3 timepoints. image = np . random . rand ( 3 , 100 , 100 ) # Iterate over the pixels in the image. N_T , N_X , N_Y = image . shape for t in range ( N_T ): for x in range ( N_X ): for y in range ( N_Y ): v = image [ t , x , y ] print ( f 'The pixel intensity at time { t } and position ( { x } , { y } ) is { v } ' ) Although this code is compact, it is hard to keep track of the many single-character variables, and it will only become harder as the body of the nested for loops grows more complex. In addition, although they are not literally single characters, the names `N_X` and `N_Y` are ambiguous (and also wrongly capitalized). This version of the code, with more descriptive variable names, is much clearer: # Create a random timelapse image with 3 timepoints. image = np . random . rand ( 3 , 100 , 100 ) # Iterate over the pixels in the image. num_timepoints , num_rows , num_cols = image . shape for time_ind in range ( num_timepoints ): for row_ind in range ( num_rows ): for col_ind in range ( num_cols ): intensity = image [ time_ind , row_ind , col_ind ] print ( f 'The pixel intensity at timepoint { time_ind } ' f 'and position ( { row_ind } , { col_ind } ) is { intensity } ' ) Documentation conventions In the context of code style guides, \"documentation\" refers to human-readable text that is embedded in the source code to explain what the code does and how it works. It is very important to define standards and conventions for documentation because it is a major way--and sometimes the only way--to ensure that code is readily understandable by others. In Python, documentation takes two forms: comments that can appear anywhere in the code and docstrings that accompany modules, classes, and functions. Comments Comments are human-readable lines of text that can appear anywhere in the source code. In Python, they are denoted by the # character. Although they are ignored by the Python interpreter and might seem like an area where \"anything goes\", it is important to adhere to strict standards of grammar and style when writing comments, since they are often the only way to explain the purpose of a particular line or block of code. In particular, comments should be complete sentences with proper punctuation. They should be written in full English sentences and should be grammatically correct. Importantly, they should end with periods; this is not only grammatically correct but is also the only way to indicate to the reader that the comment is complete and was not accidentally truncated (or never completely written in the first place). Most importantly, comments should generally be used to explain why a particular line or block of code is doing what it is doing, not what it is doing or how it is doing it (usually, this should be apparent from the code itself). Of course, comments are also appropriate when what the code is doing is not obvious or may appear to be counter-intuitive. Ideally, these cases should be rare, particularly as a codebase matures. Finally, when a line or short block of code is known to be a temporary fix or otherwise sub-optimal, a comment is a good way to indicate this to future readers. (Needless to say, these cases should also be rare.) Here is an example of a good comment that explains why a CSV file is loaded with a particular set of parameters: # We can assume missing values are always represented by the string 'NaN' in the CSV file, # so the parameter 'na_values' is set to 'NaN'. df = pd . read_csv ( 'data.csv' , na_values = 'NaN' , keep_default_na = False ) Here is another example of a good comment that explains a non-obvious implementation detail when computing the nth Fibonacci number: def calculate_fibonacci ( n ): \"\"\" Calculate the nth Fibonacci number using naive recursion. \"\"\" # By definition, the Fibonacci sequence starts with zero and one. if n <= 0 : return 0 elif n == 1 : return 1 else : return fibonacci ( n - 1 ) + fibonacci ( n - 2 ) Things that comments should not be used for There are two major ways in which comments are commonly misused. Note that avoiding these misuses is not specific to Python but is a general best practice that applies to all programming languages. The first is that, as we alluded to above, comments should not be redundant with the code. They should not simply restate what the code is doing and they should not include information that is apparent or readily inferred from the code itself. In addition to cluttering the codebase with redundant information, redundant comments also introduce a maintenance liability, as they must be manually kept in sync with the code when it changes. Even worse, when comments are not updated along with the code, the contradiction between the comment and the code will create confusion and ambiguity for future readers. Here are some examples of such redundant comments: # find the minimum value and clamp it to 0. min_value = max ( min ( values ), 0 ) # get all the .txt files in the input directory. filepaths = input_dirpath . glob ( '*.txt' ) # create the directory if it doesn't already exist. if not os . path . exists ( dirpath ): os . mkdir ( dirpath ) Note, however, that the definition of \"redundant\" is somewhat subjective and context-dependent. In particular, it depends on the reader's familiarity with the domain and context of the project and with programming in general. When we can reasonably anticipate that readers of our code will be less familiar with one of these areas, it is perfectly acceptable to apply a more relaxed definition of redundancy. The second way that comments are sometimes misused is as a way to temporarily \"disable\" code by \"commenting it out.\" While this is a common and convenient practice, it leads to various problems over time. Commented-out code is difficult to document, easy to forget about, exempt from formatting and linting checks, and over time will pollute the version history. Instead, there are several clearer and more maintainable approaches to \"disabling\" code: it can be moved into a conditional block with an appropriate condition, moved to its own file, or moved to its own branch on GitHub. The best approach depends on the nature of the code and the reason it is being disabled; but in general, short blocks of code can often be moved into a conditional block, while longer blocks of code should be moved to their own file or branch. In all cases, always first think carefully about whether or not the code in question can in fact simply be deleted; often it can be. (And in the event that it is needed later on, it can usually be recovered from the commit history of the repo on GitHub.) Docstrings Docstrings (short for \"documentation strings\") are triple-quoted strings that appear directly after the declaration of a module, class, or function and are used to describe what the module, class, or function does, what its inputs are, and what its outputs are. They are treated in a special way by the Python interpreter; they are accessible at runtime and can be used to automatically generate documentation for a Python codebase. (As an aside, many languages have a similar feature, but the term \"docstring\" is Python-specific.) Python itself does not impose any constraints on the structure or contents of docstrings, but there is both an official Python style guide for docstrings and several widely adopted conventional styles, including the Google style and the Numpy style . In general, all of these conventions are similar. For functions, they require a docstring to begin with an explanation of what the function does. This explanation should be brief and should not discuss implementation details, but should provide enough information for someone to use the function without reading its implementation. After the summary, there should follow a description of each of the inputs and outputs of the function, followed finally by any exceptions it might raise. As an example, let's revisit the example from the introduction: def calculate_roots ( quadratic_coeff , linear_coeff , constant_coeff ): discriminant_square_root = np . sqrt ( linear_coeff ** 2 - 4 * quadratic_coeff * constant_coeff ) root_1 = ( - linear_coeff + discriminant_square_root ) / ( 2 * quadratic_coeff ) root_2 = ( - linear_coeff - discriminant_square_root ) / ( 2 * quadratic_coeff ) return root_1 , root_2 Although it is possible to infer the purpose of this function from the descriptive variable names, there is still some ambiguity, especially if we lack the mathematical knowledge to interpret the equations in the code. Here is the same function with a docstring (in the Google style): def calculate_roots ( quadratic_coeff , linear_coeff , constant_coeff ): \"\"\" Calculate the real roots of a second order polynomial of the form a * x^2 + b * x + c = 0. Args: quadratic_coeff: The coefficient of the quadratic term, or `a` in the above equation. linear_coeff: The coefficient of the linear term, or `b` in the above equation. constant_coeff: The constant term, or `c` in the above equation. Returns: A tuple containing the two real roots of the polynomial. \"\"\" discriminant_square_root = np . sqrt ( linear_coeff ** 2 - 4 * quadratic_coeff * constant_coeff ) root_1 = ( - linear_coeff + discriminant_square_root ) / ( 2 * quadratic_coeff ) root_2 = ( - linear_coeff - discriminant_square_root ) / ( 2 * quadratic_coeff ) return root_1 , root_2 This version of the function is both easy to read and easy to understand; the docstring explains exactly what the function does, what its inputs are, and what its outputs are, while the implementation is transparent and easy to follow thanks to the descriptive variable names. Documentation in code generated by ChatGPT Unfortunately, code generated by ChatGPT tends to include abundant redundant comments. While this may be beneficial from a pedagogical perspective, it is not conducive to generating code that can be readily incorporated into an existing codebase. When using code generated by ChatGPT, it is important to check for and remove any redundant comments before including the code in your project. This is in addition to, of course, verifying that the code itself is correct, idiomatic, and adheres to the project's other style guidelines. Linting \"Linting\" refers to the analysis of source code to detect syntax errors, potential bugs, and violations of style conventions. Linters are similar to formatters in the sense that they analyze source code and are used to ensure that code adheres to certain standards, but unlike formatters, they typically do not modify the code itself. Instead, linters typically flag issues (or \"lint errors\") for the developer to fix. Linters are particularly important for interpreted languages like Python that are dynamically typed and interpreted at runtime, because many errors that would be caught by the compiler in a statically typed language (like C or Java) are not caught until the code is actually run, by which time it may be difficult or time-consuming to fix the error. Linters can catch these kinds of errors at an early stage when they are easy for the developer to fix. An interactive example To help understand the utility of linting, let's work with a simple example. Consider the following code: def show_full_name ( first_name , last_name ): full_name = first_name + last_nam print ( f 'Your full name is { full_name } !' ) Note that there is a typo in this function: the variable last_name is misspelled as last_nam . Unfortunately, because Python is interpreted, this mistake will go unnoticed until the code is run and the show_full_name function is called. At this point, an error will occur (and the Python process will crash) because the variable last_nam is not defined. A linter, by analyzing the source code, can detect this kind of error before the code is ever run (much less shared with others or released) and flag it for the developer to fix. In Python, one popular linting tool is called ruff . Let's try it out. First, install ruff in our conda environment: mamba install -y ruff Now, copy the function above into main.py and run ruff from the command line: ruff check . You should see an output that looks like this: main.py:2:5: F841 Local variable ` full_name ` is assigned to but never used main.py:2:30: F821 Undefined name ` last_nam ` Found 2 errors. Each line corresponds to one error that ruff identified. In this case, the second error corresponds to the typo in the variable last_name . The cryptic F821 is a code that identifies the kind of error that was detected; by googling this code, we find that F821 corresponds to the \"undefined name\" error. Finally, the 2:30 indicates that the error was found on line 2 and at column 30. It's important to understand that ruff did not actually run our script main.py to detect this error. Indeed, even if it had, no error would have occured, since our script does not actually call the show_full_name function; it only defines it. Instead, ruff detected the error by analyzing the source code itself. What about the other error that ruff found? This error is actually a consequence of our typo; because we misspelled last_name , the real variable full_name --which is an argument of the show_full_name function--is never actually used. This is an example of a style convention that ruff checks for; although unused variables won't cause errors when the code is run, it is a common convention that function definitions should only include arguments that are used (or referenced) in the body of the function. This enhances readability by preventing the reader from having to determine and keep track of which arguments are and are not actually used in a given function. Here, ruff has detected that the variable full_name is never used and flagged it as a style violation. Now, fix the typo and run ruff check again. This time, you should see no output, which means that ruff found no errors in the code. This does not, unfortunately, mean that our code is perfect. There are a lot of style conventions that have been defined, and ruff can check for many of them. For example, notice that our function does not have a docstring, which most style guides require and which we might well want ruff to flag for us. Let's run ruff again, but this time we'll enable all of ruff 's rules by passing the --select ALL flag: ruff check --select ALL . You should see a minor avalanche of new errors that looks like this: warning: ` one-blank-line-before-class ` ( D203 ) and ` no-blank-line-before-class ` ( D211 ) are incompatible. Ignoring ` one-blank-line-before-class ` . warning: ` multi-line-summary-first-line ` ( D212 ) and ` multi-line-summary-second-line ` ( D213 ) are incompatible. Ignoring ` multi-line-summary-second-line ` . main.py:1:1: D100 Missing docstring in public module main.py:1:5: ANN201 Missing return type annotation for public function ` show_full_name ` main.py:1:5: D103 Missing docstring in public function main.py:1:20: ANN001 Missing type annotation for function argument ` first_name ` main.py:1:44: ANN001 Missing type annotation for function argument ` last_name ` main.py:3:5: T201 ` print ` found Found 8 errors. Notice that, among other issues, ruff has now noticed that our function (and indeed the main.py module itself) does not have a docstring. As an aside, the warning lines at the beginning of the output indicate a problem with our use of ruff : we have enabled so many lint rules that some of the rules are actually in conflict with one another, so ruff has to choose which ones to enforce. Choosing which lint rules to enforce In general, choosing the appropriate set of lint rules to enforce is a context- and project-dependent decision. As codebases grow and become more complex, or when codebases are developed by multiple people, it often makes sense to enforce increasingly strict and extensive linting rules. For our purposes at Arcadia, the default rules that ruff enforces (i.e., using ruff check alone) is a great place to start; this will catch many common typos and bugs that will cause errors at runtime (like undefined and unused variables). As we discuss below, we also have a template repository for Python projects at Arcadia that comes with reasonable default settings for ruff . Aside: linters can fix some issues automatically Many linters can automatically fix some of the issues that they detect. You can try this out with ruff by running: ruff check --fix . The --fix flag tells ruff to fix as many of the errors it finds as it can. Of course, this only goes so far, as many errors (like missing docstrings) cannot be fixed automatically. (This will probably change in the future, as LLMs become more reliable and are incorporated into linters or even used to replace them entirely.) When to format and when to lint The short answer is early and often: because formatters and linters are fast and easy to run, it is best to run them frequently and as early as possible in the development process, when errors are easy to fix. Most IDEs can be configured to run formatters automatically each time a file is saved, and linters are often integrated with IDEs as well (this is how VS Code, for example, displays squiggly red lines under undefined variables). In addition, formatting and linting are usually run automatically as part of a continuous integration (CI) pipeline . Briefly, this means that, for example, whenever a PR is opened on GitHub, the same formatting and linting tools that a developer would run locally are run remotely on the code in the PR. This ensures that the code on the main branch in the GitHub repo--which is the \"final\" version of the code that will ultimately be shared with or deployed to users--is properly formatted and passes the project's linting rules, whether or not individual developers took the time to run these tools locally. Setting up formatting and linting in a new project Because formatting and linting are such common tasks, it is convenient to develop \"templates\" that define the formatting and linting tools that should be used for all projects within an organization. These templates often also include GitHub Actions workflows to run formatting and linting automatically as part of a CI pipeline. At Arcadia, we've developed GitHub repo templates for Python projects , for Snakemake pipelines , and for R projects . These templates should allow you to start new projects with the correct formatting and linting tools already set up. Later this year, we'll have an AUG lesson about how to use these templates. Beyond formatting and linting: software architecture and design patterns While formatting, style guides, and linters can constrain many of the details of how code is written and catch many common kinds of bugs, they do not address larger questions about how code should be structured. For example, when should a class be used instead of a function? When should a large function (or class, or module) be split up into separate components? What kinds of abstractions should be used to represent a particular concept? These questions are, of course, subjective and context-dependent; they require care and experience to answer. However, there are general guidelines and patterns that can help. This is the domain of software architecture and design patterns, which will be the subject of a future AUG lesson.","title":"Introduction to code formatting, linting, and style guides"},{"location":"arcadia-users-group/20240206-intro-to-formatting-and-linting/lesson/#introduction-to-code-formatting-linting-and-style-guides","text":"This lesson explains some of the basic conventions around code formatting and style, with an emphasis on Python. It also explains how to use tools to automatically format code and check (or \"lint\") it for compliance with these style conventions. The importance of defining a consistent format and style for the code we write arises from two unfortunate realities: code is easier to write than it is to read, and code is read more often than it is written (or rewritten). Code is easier to write than read because, when we are writing it, we usually know exactly what we are trying to do and we have the context needed to understand why and how we're doing it. By contrast, neither of these two things are typically true when we are reading code: we often either don't remember or never knew exactly what the author was trying to do when they wrote the code we're reading, and the context necessary to understand the code is invariably also unknown to us or at least not fresh in our minds. This means that, when writing code, we need to have a system of rules and guidelines in place to ensure that the code we write is as easy as possible for others (either our future selves or other developers) to read and understand. This is the purpose of code formatting and style conventions. (At a higher level, this is also, in part, the purpose of design patterns and application architectures, but that is a topic for a future AUG lesson.) These conventions are particularly important for us at Arcadia, as most of our code is developed collaboratively. This necessarily means that multiple people will read it. In addition, we often intend for the code we produce to be useful and understandable to others outside of Arcadia, and part of that is ensuring our code meets high standards for readability and comprehensibility.","title":"Introduction to code formatting, linting, and style guides"},{"location":"arcadia-users-group/20240206-intro-to-formatting-and-linting/lesson/#an-introductory-example","text":"As an example to help illustrate why code formatting and style are so important, consider the following code: def qr ( a , b , c ): return - ( b - np . sqrt ( b ** 2 - 4 * a * c )) / 2 / a ,( - b - np . sqrt ( b ** 2 - 4 * a * c )) / 2 / a It is very hard to read this code, let alone understand what it does. Now consider the same code written in a more readable style: def calculate_quadratic_roots ( quadratic_coeff , linear_coeff , constant_coeff ): discriminant_square_root = np . sqrt ( linear_coeff ** 2 - 4 * quadratic_coeff * constant_coeff ) root_1 = ( - linear_coeff + discriminant_square_root ) / ( 2 * quadratic_coeff ) root_2 = ( - linear_coeff - discriminant_square_root ) / ( 2 * quadratic_coeff ) return root_1 , root_2 Notice how this code is easier to read and understand (if still somewhat opaque). This is due to several formatting and style changes. For one, spaces are used around the operators and the single long line has been broken into four separate lines, each of which does one specific thing. These changes generally makes code easier to read; they are a kind of formatting convention. In addition, the variable names are much longer and are more descriptive, which make it easier for reader to infer the meaning and intent the code; this is a kind of style convention. In this lesson, we'll discuss both formatting and style conventions, as well as tools to automatically format code and check that is complies with our style conventions.","title":"An introductory example"},{"location":"arcadia-users-group/20240206-intro-to-formatting-and-linting/lesson/#lesson-setup","text":"We'll create a Python project to demonstrate how to use the tools we'll be discussing. Create a new conda environment and activate it: mamba create -y -n aug-linting-lesson python = 3 .11 mamba activate aug-linting-lesson Create a new directory for the project: mkdir 20240206 -aug-linting-lesson cd 20240206 -aug-linting-lesson Finally, create a new file called main.py . We'll add code to this file later.","title":"Lesson setup"},{"location":"arcadia-users-group/20240206-intro-to-formatting-and-linting/lesson/#formatting-conventions","text":"In programming, \"formatting\" refers to how code is laid out or arranged on the page or screen; for example, how many spaces to use for indentation, where to put spaces around operators, where to use line breaks (and how many), whether to use single or double quotes for strings, and so on. In Python and most other languages, these questions are not settled by the syntax of the language itself, so it is up to each programmer (or project, or team, or organization) to decide for themselves (or itself) how to resolve them. Unfortunately, because most formatting decisions are a matter of personal preference, a wide range of (often strongly-held) opinions about how code should be formatted has emerged. However, there is wide agreement that code should be formatted consistently . That is, once a set of formatting rules have been chosen, they should be applied consistently throughout a given codebase. This is because inconsistent formatting makes code significantly harder to read and maintain. More subtly, inconsistent formatting also pollutes the version history of a codebase with unnecessary and superficial changes, making it harder to understand the \"real\" or substantive changes that were made to the code.","title":"Formatting conventions"},{"location":"arcadia-users-group/20240206-intro-to-formatting-and-linting/lesson/#example-breaking-up-long-lines","text":"One important formatting convention is that lines of code should not be longer than some maximum length. This is to ensure that code is readable on a wide range of screen sizes. (In Python and many other languages, a maximum line length of 80 or 100 characters is typically chosen.) The tricky part lies in determining how to break up lines that are longer than this maximum length; there are many options and no technical reason to prefer one over another. For example, consider the following code: def calculate_total_income ( gross_wages , taxable_interest , total_dividends , qualified_dividends , other_income ): total_income = gross_wages + taxable_interest + ( total_dividends - qualified_dividends ) + other_income return total_income It is clear that the function definition is too long to fit in one line, so it needs to be broken up. But where should the line breaks go? One option is to break up the long lines after each comma or operator: def calculate_total_income ( gross_wages , taxable_interest , total_dividends , qualified_dividends , other_income ): total_income = ( gross_wages + taxable_interest + ( total_dividends - qualified_dividends ) + other_income ) return total_income Another option is to use a separate line for each argument and also for the closing parentheses: def calculate_total_income ( gross_wages , taxable_interest , total_dividends , qualified_dividends , other_income ): total_income = ( gross_wages + taxable_interest + ( total_dividends - qualified_dividends ) + other_income ) return total_income Although these two versions are syntactically identical and formatted similarly, the formatting rules they obey are quite different. Even worse, a \"diff\" or line-by-line comparison of the two versions is very hard to read, since almost every line is different in some trivial way. This means that if a programmer both reformatted the code from one format to the other and, at the same time, made a substantive change to it (like renaming one of the arguments of the calculate_total_income function), it would be hard to quickly determine which lines were meaningfully changed and which were just reformatted.","title":"Example: breaking up long lines"},{"location":"arcadia-users-group/20240206-intro-to-formatting-and-linting/lesson/#code-formatters-enforce-formatting-conventions","text":"To both eliminate these kinds of formatting ambiguities and to automate the otherwise-tedious process of formatting code by hand, tools to automatically format code have been developed. These tools take files of source code as input and output the same code, but reformatted as necessary according to a pre-specified set of formatting rules. In Python, the most popular code formatting tool is called black . This tool both automatically formats code and imposes its own opinionated set of formatting rules (hence its tagline: \"any color you want\"). This eliminates the need for developers to make (or argue about) these decisions themselves. Although black 's formatting rules can take some getting used to, the benefit of eliminating almost all formatting-related decisions is usually worth the effort of adjusting to its opinions.","title":"Code formatters enforce formatting conventions"},{"location":"arcadia-users-group/20240206-intro-to-formatting-and-linting/lesson/#setting-up-black","text":"To install black in our conda environment, run: mamba install -y black Next, let's add some poorly formatted code to our toy project. Add the following code (from our previous example) to main.py : def calculate_total_income ( gross_wages , taxable_interest , total_dividends , qualified_dividends , other_income ): total_income = gross_wages + taxable_interest + ( total_dividends - qualified_dividends ) + other_income return total_income Now, from the command line, run: black . This command tells black to reformat all the Python files in the current directory (the lone dot in the command is the relative path to the current directory). By default, black considers any file ending with .py to be a Python file, and it will look for and reformat all such files in the directory path it is given (or any of that directory's subdirectories). Take a look at main.py again. You should see that the code has been reformatted according to black 's rules. Now, try simplifying the calculation of total_income by removing the two dividend-related variables from the sum, then run black again. You should see that, because the total_income line is shorter, it can fit on one line, so black automatically converts it back to a single line. There is a succinct overview of black 's formatting rules in its documentation . While it's good to be familiar with these rules, there is no need to memorize them. Indeed, the purpose of using a formatting tool like black is to eliminate the need to manually write your code according to any particular formatting conventions or even to think about formatting at all. Aside about black and ruff Since its release in 2018, black has played an influential role in standardizing code formatting across the Python ecosystem. However, it has now been reimplemented by a tool called ruff that is designed to be a fast and comprehensive tool for formatting and linting code in Python. This lesson uses black as an homage to its historical importance, but in practice, ruff is the tool we should be using for both formatting and linting.","title":"Setting up black"},{"location":"arcadia-users-group/20240206-intro-to-formatting-and-linting/lesson/#style-guides","text":"Although formatting conventions help constrain the very low-level aspects of how code is written, additional rules and conventions are required to ensure that code is both readable and understandable. These conventions are usually expressed in the form of style guides. Style guides define rules, conventions, and guidelines to which all code in a codebase should adhere. They concern both both low-level questions like how to name variables and functions, as well as higher-level considerations like documentation standards and some aspects of how code is organized or structured.","title":"Style guides"},{"location":"arcadia-users-group/20240206-intro-to-formatting-and-linting/lesson/#pep8-and-python-style-guides","text":"Many language have an official style guide. For Python, the official style guide is called PEP8 . In addition, Google's Python style guide is an important extension of PEP8 that defines additional standards, especially around nomenclature, documentation, and code organization. Most major Python projects follow PEP8, and we should strive to follow it at Arcadia as well, as it is a foundational part of ensuring our code is readable and useful to others. This is true no matter the scale of the project; it is a good practice to follow PEP8 even for small scripts and one-off analyses, as it is often hard to anticipate the lifetime of a project in advance. It is therefore prudent to assume that all of the code we write will always be read by others (inside, if not also outside, of Arcadia). As projects grow and it becomes more likely that we will need to maintain them for a long period of time, we can then readily impose additional style conventions on top of PEP8. Many of the conventions defined in PEP8 concern formatting and are automatically enforced by using black to format our code. However, PEP8 also includes style conventions that we must follow manually. Here, we'll discuss two important categories of style conventions: naming conventions and documentation conventions.","title":"PEP8 and Python style guides"},{"location":"arcadia-users-group/20240206-intro-to-formatting-and-linting/lesson/#naming-conventions","text":"Naming conventions are an important element of code style. They determine how variables, functions, classes, modules, and other objects should be named. In Python, there are some strict rules about how to name each of these things: variables, functions, and modules* should be named in lower_snake_case . class names should be UpperCamelCase . global variables should be named in ALL_CAPS_SNAKE_CASE . names cannot begin with a number or contain spaces or any special characters (except for underscores). *\"modules\" are the individual python files that are contained with a python project or package; e.g., main.py is a module. In other words, all names should be lower_snake_case except for global variables (which should be used sparingly, if at all) and class names. Note that instances of a class are variables and should be named in lower_snake_case , not UpperCamelCase . Here is an example: class ProteinSequence : def __init__ ( self , sequence ): self . sequence = sequence protein_sequence = ProteinSequence ( 'MSKGEELFTG' ) All names should be specific, descriptive, and unambiguous. When in doubt, err on the side of verbosity, and always avoid unnecessary abbreviations. Although the meaning of \"descriptive\" is obviously subjective, there are some general guidelines that all names, no matter how brief, should follow: Function names should generally begin with a verb that corresponds to what they do. Functions that return a boolean value should have a name of the form is_<something> or has_<something> , functions that calculate something calculate_<something> or if they modify an object in-place, they should be named either set_<something> or update_<something> . Variable names should not include type information (e.g., list_of_ints or title_str ), since Python is a dynamically typed language and the type of a variable can change at runtime. The same goes for functions (e.g. calculate_tm_score instead of calculate_tm_score_fn ). To express type information, type hints should be used (this is a topic for a future lesson, but if you're curious, check out this overview of type hints in Python ). Where possible, use the plural form of a noun for variables that contain a collection of things (e.g., for a list of proteins, use proteins instead of protein , protein_list , protein_set , etc). Here are some examples of good and bad variable names: # bad (too short, ambiguous) pids = [ 'P12345' , 'P23456' , 'P34567' ] # bad (includes type information) protein_id_list = [ 'P12345' , 'P23456' , 'P34567' ] # good protein_ids = [ 'P12345' , 'P23456' , 'P34567' ] # bad (ambiguous and not descriptive) def calculate ( protein1 , protein2 ): ... # bad (not lower_snake_case) def calculateTMscore ( protein1 , protein2 ): ... # good def calculate_tm_score ( protein1 , protein2 ): ... # bad (unnecessary abbreviations) usr_inpt = input ( 'Enter your name: ' ) # bad (ambiguous abbreviation) user_in = input ( 'Enter your name: ' ) # good user_input = input ( 'Enter your name: ' )","title":"Naming conventions"},{"location":"arcadia-users-group/20240206-intro-to-formatting-and-linting/lesson/#aside-about-single-letter-variable-names","text":"Although PEP8 and other style guides do not explicitly forbid the use of single-letter variable names, they are strongly discouraged in most contexts. This is because single-letter variable names are, by definition, not descriptive. They can also be quite literally ambiguous; it is often hard to visually detect the difference between i , j , l , and 1 , for example. Finally, a subtler reason to avoid single-character variable names is that they can make refactoring harder, as it may be more cumbersome to search for and rename single-character variable names than more descriptive ones. One exception to this general prohibition should be for variables defined in a very local or narrow scope such as a short for loop or in a lambda function. Here are two examples where single-letter variable names is used within a single line (which is as narrow a scope as possible): # A single-letter variable name `f` used in a list comprehension. filepaths = [ 'data_1.txt' , 'data_2.txt' , 'data_3.csv' ] txt_filepaths = [ f for f in filepaths if f . endswith ( '.txt' )] # A single-letter variable name `x` used in a lambda function. numbers = [ 1 , 2 , 3 , 4 , 5 ] squared_numbers = list ( map ( lambda x : x ** 2 , numbers )) In cases such as these, you may decide that the brevity of this code outweighs the loss of readability. However, even in these cases, it is often possible to use more descriptive variable names without sacrificing much brevity: txt_filepaths = [ filepath for filepath in filepaths if filepath . endswith ( '.txt' )] squared_numbers = list ( map ( lambda value : value ** 2 , numbers )) Another scenario in which single-letter variable names may be acceptable is when implementing mathematical formulas as they appear in a publication or as they are typically written in the literature. In these cases, it may be clearer to retain the original single-letter names rather than replacing them with more descriptive names. For example, consider the formula for the force of gravity between two objects: F = G * m_1 * m_2 / d ** 2 Here, the formula is well-known and the meaning of G , m_1 , m_2 , and d would be clear to anyone who is familiar with the formula. Using longer, more descriptive variable names in this case could make the code harder to read: force_of_gravity = gravitational_constant * mass_1 * mass_2 / distance_mass_1_mass_2 ** 2 However, if the single-letter names used in an equation are also used in other places in the code, then it is probably better to use more descriptive names throughout, as the clarity of the single-letter names heavily depends on their use in the context of a well-known or well-documented equation. More about single-letter variable names in scientific programming Unfortunately, single-letter variable names seem to be common in scientific programming. As we discussed above, this may be acceptable when they are are used in the context a well-known mathematical formula. But in many cases, their use is simply a shortcut that comes at the price of readability. Consider the following example of iterating over the pixels in a timelapse image: # Create a random timelapse image with 3 timepoints. image = np . random . rand ( 3 , 100 , 100 ) # Iterate over the pixels in the image. N_T , N_X , N_Y = image . shape for t in range ( N_T ): for x in range ( N_X ): for y in range ( N_Y ): v = image [ t , x , y ] print ( f 'The pixel intensity at time { t } and position ( { x } , { y } ) is { v } ' ) Although this code is compact, it is hard to keep track of the many single-character variables, and it will only become harder as the body of the nested for loops grows more complex. In addition, although they are not literally single characters, the names `N_X` and `N_Y` are ambiguous (and also wrongly capitalized). This version of the code, with more descriptive variable names, is much clearer: # Create a random timelapse image with 3 timepoints. image = np . random . rand ( 3 , 100 , 100 ) # Iterate over the pixels in the image. num_timepoints , num_rows , num_cols = image . shape for time_ind in range ( num_timepoints ): for row_ind in range ( num_rows ): for col_ind in range ( num_cols ): intensity = image [ time_ind , row_ind , col_ind ] print ( f 'The pixel intensity at timepoint { time_ind } ' f 'and position ( { row_ind } , { col_ind } ) is { intensity } ' )","title":"Aside about single-letter variable names"},{"location":"arcadia-users-group/20240206-intro-to-formatting-and-linting/lesson/#documentation-conventions","text":"In the context of code style guides, \"documentation\" refers to human-readable text that is embedded in the source code to explain what the code does and how it works. It is very important to define standards and conventions for documentation because it is a major way--and sometimes the only way--to ensure that code is readily understandable by others. In Python, documentation takes two forms: comments that can appear anywhere in the code and docstrings that accompany modules, classes, and functions.","title":"Documentation conventions"},{"location":"arcadia-users-group/20240206-intro-to-formatting-and-linting/lesson/#comments","text":"Comments are human-readable lines of text that can appear anywhere in the source code. In Python, they are denoted by the # character. Although they are ignored by the Python interpreter and might seem like an area where \"anything goes\", it is important to adhere to strict standards of grammar and style when writing comments, since they are often the only way to explain the purpose of a particular line or block of code. In particular, comments should be complete sentences with proper punctuation. They should be written in full English sentences and should be grammatically correct. Importantly, they should end with periods; this is not only grammatically correct but is also the only way to indicate to the reader that the comment is complete and was not accidentally truncated (or never completely written in the first place). Most importantly, comments should generally be used to explain why a particular line or block of code is doing what it is doing, not what it is doing or how it is doing it (usually, this should be apparent from the code itself). Of course, comments are also appropriate when what the code is doing is not obvious or may appear to be counter-intuitive. Ideally, these cases should be rare, particularly as a codebase matures. Finally, when a line or short block of code is known to be a temporary fix or otherwise sub-optimal, a comment is a good way to indicate this to future readers. (Needless to say, these cases should also be rare.) Here is an example of a good comment that explains why a CSV file is loaded with a particular set of parameters: # We can assume missing values are always represented by the string 'NaN' in the CSV file, # so the parameter 'na_values' is set to 'NaN'. df = pd . read_csv ( 'data.csv' , na_values = 'NaN' , keep_default_na = False ) Here is another example of a good comment that explains a non-obvious implementation detail when computing the nth Fibonacci number: def calculate_fibonacci ( n ): \"\"\" Calculate the nth Fibonacci number using naive recursion. \"\"\" # By definition, the Fibonacci sequence starts with zero and one. if n <= 0 : return 0 elif n == 1 : return 1 else : return fibonacci ( n - 1 ) + fibonacci ( n - 2 )","title":"Comments"},{"location":"arcadia-users-group/20240206-intro-to-formatting-and-linting/lesson/#things-that-comments-should-not-be-used-for","text":"There are two major ways in which comments are commonly misused. Note that avoiding these misuses is not specific to Python but is a general best practice that applies to all programming languages. The first is that, as we alluded to above, comments should not be redundant with the code. They should not simply restate what the code is doing and they should not include information that is apparent or readily inferred from the code itself. In addition to cluttering the codebase with redundant information, redundant comments also introduce a maintenance liability, as they must be manually kept in sync with the code when it changes. Even worse, when comments are not updated along with the code, the contradiction between the comment and the code will create confusion and ambiguity for future readers. Here are some examples of such redundant comments: # find the minimum value and clamp it to 0. min_value = max ( min ( values ), 0 ) # get all the .txt files in the input directory. filepaths = input_dirpath . glob ( '*.txt' ) # create the directory if it doesn't already exist. if not os . path . exists ( dirpath ): os . mkdir ( dirpath ) Note, however, that the definition of \"redundant\" is somewhat subjective and context-dependent. In particular, it depends on the reader's familiarity with the domain and context of the project and with programming in general. When we can reasonably anticipate that readers of our code will be less familiar with one of these areas, it is perfectly acceptable to apply a more relaxed definition of redundancy. The second way that comments are sometimes misused is as a way to temporarily \"disable\" code by \"commenting it out.\" While this is a common and convenient practice, it leads to various problems over time. Commented-out code is difficult to document, easy to forget about, exempt from formatting and linting checks, and over time will pollute the version history. Instead, there are several clearer and more maintainable approaches to \"disabling\" code: it can be moved into a conditional block with an appropriate condition, moved to its own file, or moved to its own branch on GitHub. The best approach depends on the nature of the code and the reason it is being disabled; but in general, short blocks of code can often be moved into a conditional block, while longer blocks of code should be moved to their own file or branch. In all cases, always first think carefully about whether or not the code in question can in fact simply be deleted; often it can be. (And in the event that it is needed later on, it can usually be recovered from the commit history of the repo on GitHub.)","title":"Things that comments should not be used for"},{"location":"arcadia-users-group/20240206-intro-to-formatting-and-linting/lesson/#docstrings","text":"Docstrings (short for \"documentation strings\") are triple-quoted strings that appear directly after the declaration of a module, class, or function and are used to describe what the module, class, or function does, what its inputs are, and what its outputs are. They are treated in a special way by the Python interpreter; they are accessible at runtime and can be used to automatically generate documentation for a Python codebase. (As an aside, many languages have a similar feature, but the term \"docstring\" is Python-specific.) Python itself does not impose any constraints on the structure or contents of docstrings, but there is both an official Python style guide for docstrings and several widely adopted conventional styles, including the Google style and the Numpy style . In general, all of these conventions are similar. For functions, they require a docstring to begin with an explanation of what the function does. This explanation should be brief and should not discuss implementation details, but should provide enough information for someone to use the function without reading its implementation. After the summary, there should follow a description of each of the inputs and outputs of the function, followed finally by any exceptions it might raise. As an example, let's revisit the example from the introduction: def calculate_roots ( quadratic_coeff , linear_coeff , constant_coeff ): discriminant_square_root = np . sqrt ( linear_coeff ** 2 - 4 * quadratic_coeff * constant_coeff ) root_1 = ( - linear_coeff + discriminant_square_root ) / ( 2 * quadratic_coeff ) root_2 = ( - linear_coeff - discriminant_square_root ) / ( 2 * quadratic_coeff ) return root_1 , root_2 Although it is possible to infer the purpose of this function from the descriptive variable names, there is still some ambiguity, especially if we lack the mathematical knowledge to interpret the equations in the code. Here is the same function with a docstring (in the Google style): def calculate_roots ( quadratic_coeff , linear_coeff , constant_coeff ): \"\"\" Calculate the real roots of a second order polynomial of the form a * x^2 + b * x + c = 0. Args: quadratic_coeff: The coefficient of the quadratic term, or `a` in the above equation. linear_coeff: The coefficient of the linear term, or `b` in the above equation. constant_coeff: The constant term, or `c` in the above equation. Returns: A tuple containing the two real roots of the polynomial. \"\"\" discriminant_square_root = np . sqrt ( linear_coeff ** 2 - 4 * quadratic_coeff * constant_coeff ) root_1 = ( - linear_coeff + discriminant_square_root ) / ( 2 * quadratic_coeff ) root_2 = ( - linear_coeff - discriminant_square_root ) / ( 2 * quadratic_coeff ) return root_1 , root_2 This version of the function is both easy to read and easy to understand; the docstring explains exactly what the function does, what its inputs are, and what its outputs are, while the implementation is transparent and easy to follow thanks to the descriptive variable names.","title":"Docstrings"},{"location":"arcadia-users-group/20240206-intro-to-formatting-and-linting/lesson/#documentation-in-code-generated-by-chatgpt","text":"Unfortunately, code generated by ChatGPT tends to include abundant redundant comments. While this may be beneficial from a pedagogical perspective, it is not conducive to generating code that can be readily incorporated into an existing codebase. When using code generated by ChatGPT, it is important to check for and remove any redundant comments before including the code in your project. This is in addition to, of course, verifying that the code itself is correct, idiomatic, and adheres to the project's other style guidelines.","title":"Documentation in code generated by ChatGPT"},{"location":"arcadia-users-group/20240206-intro-to-formatting-and-linting/lesson/#linting","text":"\"Linting\" refers to the analysis of source code to detect syntax errors, potential bugs, and violations of style conventions. Linters are similar to formatters in the sense that they analyze source code and are used to ensure that code adheres to certain standards, but unlike formatters, they typically do not modify the code itself. Instead, linters typically flag issues (or \"lint errors\") for the developer to fix. Linters are particularly important for interpreted languages like Python that are dynamically typed and interpreted at runtime, because many errors that would be caught by the compiler in a statically typed language (like C or Java) are not caught until the code is actually run, by which time it may be difficult or time-consuming to fix the error. Linters can catch these kinds of errors at an early stage when they are easy for the developer to fix.","title":"Linting"},{"location":"arcadia-users-group/20240206-intro-to-formatting-and-linting/lesson/#an-interactive-example","text":"To help understand the utility of linting, let's work with a simple example. Consider the following code: def show_full_name ( first_name , last_name ): full_name = first_name + last_nam print ( f 'Your full name is { full_name } !' ) Note that there is a typo in this function: the variable last_name is misspelled as last_nam . Unfortunately, because Python is interpreted, this mistake will go unnoticed until the code is run and the show_full_name function is called. At this point, an error will occur (and the Python process will crash) because the variable last_nam is not defined. A linter, by analyzing the source code, can detect this kind of error before the code is ever run (much less shared with others or released) and flag it for the developer to fix. In Python, one popular linting tool is called ruff . Let's try it out. First, install ruff in our conda environment: mamba install -y ruff Now, copy the function above into main.py and run ruff from the command line: ruff check . You should see an output that looks like this: main.py:2:5: F841 Local variable ` full_name ` is assigned to but never used main.py:2:30: F821 Undefined name ` last_nam ` Found 2 errors. Each line corresponds to one error that ruff identified. In this case, the second error corresponds to the typo in the variable last_name . The cryptic F821 is a code that identifies the kind of error that was detected; by googling this code, we find that F821 corresponds to the \"undefined name\" error. Finally, the 2:30 indicates that the error was found on line 2 and at column 30. It's important to understand that ruff did not actually run our script main.py to detect this error. Indeed, even if it had, no error would have occured, since our script does not actually call the show_full_name function; it only defines it. Instead, ruff detected the error by analyzing the source code itself. What about the other error that ruff found? This error is actually a consequence of our typo; because we misspelled last_name , the real variable full_name --which is an argument of the show_full_name function--is never actually used. This is an example of a style convention that ruff checks for; although unused variables won't cause errors when the code is run, it is a common convention that function definitions should only include arguments that are used (or referenced) in the body of the function. This enhances readability by preventing the reader from having to determine and keep track of which arguments are and are not actually used in a given function. Here, ruff has detected that the variable full_name is never used and flagged it as a style violation. Now, fix the typo and run ruff check again. This time, you should see no output, which means that ruff found no errors in the code. This does not, unfortunately, mean that our code is perfect. There are a lot of style conventions that have been defined, and ruff can check for many of them. For example, notice that our function does not have a docstring, which most style guides require and which we might well want ruff to flag for us. Let's run ruff again, but this time we'll enable all of ruff 's rules by passing the --select ALL flag: ruff check --select ALL . You should see a minor avalanche of new errors that looks like this: warning: ` one-blank-line-before-class ` ( D203 ) and ` no-blank-line-before-class ` ( D211 ) are incompatible. Ignoring ` one-blank-line-before-class ` . warning: ` multi-line-summary-first-line ` ( D212 ) and ` multi-line-summary-second-line ` ( D213 ) are incompatible. Ignoring ` multi-line-summary-second-line ` . main.py:1:1: D100 Missing docstring in public module main.py:1:5: ANN201 Missing return type annotation for public function ` show_full_name ` main.py:1:5: D103 Missing docstring in public function main.py:1:20: ANN001 Missing type annotation for function argument ` first_name ` main.py:1:44: ANN001 Missing type annotation for function argument ` last_name ` main.py:3:5: T201 ` print ` found Found 8 errors. Notice that, among other issues, ruff has now noticed that our function (and indeed the main.py module itself) does not have a docstring. As an aside, the warning lines at the beginning of the output indicate a problem with our use of ruff : we have enabled so many lint rules that some of the rules are actually in conflict with one another, so ruff has to choose which ones to enforce.","title":"An interactive example"},{"location":"arcadia-users-group/20240206-intro-to-formatting-and-linting/lesson/#choosing-which-lint-rules-to-enforce","text":"In general, choosing the appropriate set of lint rules to enforce is a context- and project-dependent decision. As codebases grow and become more complex, or when codebases are developed by multiple people, it often makes sense to enforce increasingly strict and extensive linting rules. For our purposes at Arcadia, the default rules that ruff enforces (i.e., using ruff check alone) is a great place to start; this will catch many common typos and bugs that will cause errors at runtime (like undefined and unused variables). As we discuss below, we also have a template repository for Python projects at Arcadia that comes with reasonable default settings for ruff .","title":"Choosing which lint rules to enforce"},{"location":"arcadia-users-group/20240206-intro-to-formatting-and-linting/lesson/#aside-linters-can-fix-some-issues-automatically","text":"Many linters can automatically fix some of the issues that they detect. You can try this out with ruff by running: ruff check --fix . The --fix flag tells ruff to fix as many of the errors it finds as it can. Of course, this only goes so far, as many errors (like missing docstrings) cannot be fixed automatically. (This will probably change in the future, as LLMs become more reliable and are incorporated into linters or even used to replace them entirely.)","title":"Aside: linters can fix some issues automatically"},{"location":"arcadia-users-group/20240206-intro-to-formatting-and-linting/lesson/#when-to-format-and-when-to-lint","text":"The short answer is early and often: because formatters and linters are fast and easy to run, it is best to run them frequently and as early as possible in the development process, when errors are easy to fix. Most IDEs can be configured to run formatters automatically each time a file is saved, and linters are often integrated with IDEs as well (this is how VS Code, for example, displays squiggly red lines under undefined variables). In addition, formatting and linting are usually run automatically as part of a continuous integration (CI) pipeline . Briefly, this means that, for example, whenever a PR is opened on GitHub, the same formatting and linting tools that a developer would run locally are run remotely on the code in the PR. This ensures that the code on the main branch in the GitHub repo--which is the \"final\" version of the code that will ultimately be shared with or deployed to users--is properly formatted and passes the project's linting rules, whether or not individual developers took the time to run these tools locally.","title":"When to format and when to lint"},{"location":"arcadia-users-group/20240206-intro-to-formatting-and-linting/lesson/#setting-up-formatting-and-linting-in-a-new-project","text":"Because formatting and linting are such common tasks, it is convenient to develop \"templates\" that define the formatting and linting tools that should be used for all projects within an organization. These templates often also include GitHub Actions workflows to run formatting and linting automatically as part of a CI pipeline. At Arcadia, we've developed GitHub repo templates for Python projects , for Snakemake pipelines , and for R projects . These templates should allow you to start new projects with the correct formatting and linting tools already set up. Later this year, we'll have an AUG lesson about how to use these templates.","title":"Setting up formatting and linting in a new project"},{"location":"arcadia-users-group/20240206-intro-to-formatting-and-linting/lesson/#beyond-formatting-and-linting-software-architecture-and-design-patterns","text":"While formatting, style guides, and linters can constrain many of the details of how code is written and catch many common kinds of bugs, they do not address larger questions about how code should be structured. For example, when should a class be used instead of a function? When should a large function (or class, or module) be split up into separate components? What kinds of abstractions should be used to represent a particular concept? These questions are, of course, subjective and context-dependent; they require care and experience to answer. However, there are general guidelines and patterns that can help. This is the domain of software architecture and design patterns, which will be the subject of a future AUG lesson.","title":"Beyond formatting and linting: software architecture and design patterns"},{"location":"arcadia-users-group/20240305-github-templates/lesson/","text":"Introduction to GitHub Templates GitHub templates provide a structured way to create new repositories, ensuring consistency, efficiency, and adherence to best practices across projects. GitHub templates are predefined repository setups that can be used to start new projects with a consistent set of files, folders, and configurations. This not only saves time but helps make sure that all projects follow Arcadia's coding and documentation standards. Templates typically include everything from README files, installation guides, contribution guidelines, and more. In this lesson, we will focus on Arcadia's python-analysis-template . Arcadia also has an r-analysis-template , a snakemake-template , and a nextflow-template . Creating a new repository using a template Choosing the Right Template Depending on the analysis type or the technology stack used in your project, select the appropriate template. For instance, use r-analysis-template for R-based analyses, python-analysis-template for Python projects, snakemake-template for workflows managed with Snakemake, and nextflow-template for Nextflow pipelines. Why isn't there a Python package template? We could create one. We haven't generated one yet because there are a ton templates out there that could be used and we haven't created many Python packages yet as an organization. If you find yourself in need of a Python template, try out an existing template and make note of what works well and what doesn't. We can then use that to generate a future organization standard template. Repository Initialization There are two ways to create a new repo with a template. Option 1 Navigate to the template you want to start from (e.x. https://github.com/Arcadia-Science/python-analysis-template). Click the green \"Use this template\" button in the upper right hand corner. This will take you to the New Repository launch screen with the \"Repository template\" section already filled out. Fill out the rest of the new repository information and click the \"Create repository\" button. This will generate a new repository that already has all of the files and folders that were in the template. Option 2 Click the \"+\" button in the upper right hand corner of any GitHub page and select \"New repository\" from the drop down menu. On the New Repository launch screen, select the template you wish to start from in the \"Repository template\" drop down menu. Fill out the rest of the new repository information and click the \"Create repository\" button. This will generate a new repository that already has all of the files and folders that were in the template. Working in branches After your new repository is created, create a new branch before you start making any changes. Do your work in branches and when you are ready to merge your changes into the main branch, open a Pull Request and request a review. For more on how to use branches, see this lesson . For more on why we think code review is important, see this blog post (note we didn't write this blog post, we just agree with a lot of the content). We recommend setting up branch protection rules that can prevent code from making it into main without a Pull Request and review. How to set up branch protection rules in your repository Branch protection rules are configured by clicking on the \"branches\" menu item on the lefthand side of the repo \"settings\" page. The following Branch protection rules should be enabled: Require a pull request before merging : this blocks anyone from pushing directly to the main branch. Require approvals: this requires that there be at least one approving review before a PR can be merged. Require status checks to pass before merging : this blocks PRs from being merged until all of the CI checks are passing. Anatomy of a template: Python analysis template We can create a new repo from a template! But what is actually in the template and what do those files do? Below we go through each file in the python-analysis-template and discuss their functions and how template users can interact with them. Documentation and re-use README.md : This README has two sections. The first is an outline for the documentation for the new repository. It includes boilerplate sections and language that we expect will be standard across most projects, as well as TODO statements where specific details need to be changed or filled in. The second section covers developer notes detailing what the files in the repository do and tips for how to use them. The second section should be deleted in repos created from the template but is included to make the template easier to understand and use. envs/dev.yml : By default, we recommend analysis repositories use conda to manage software installations and record software versions. The envs directory is where these yaml files are stored. The dev.yml file included in the template is not empty but lists the packages required by the template: python, jupyter, pip, and linting software. LICENSE : The LICENSE file specifies the terms under which the software can be freely used, modified, and shared. Technically, software must have a license for it to be re-usable by others. The default license for all of our GitHub repositories is MIT, however the license may need to change depending on the software the project uses and the code it includes. Be sure to check with legal about your license before making a GitHub repository public. .github/PULL_REQUEST_TEMPLATE.md : The .github folder controls some behavior of the GitHub repository itself. The PULL_REQUEST_TEMPLATE.md is a text document where users can provide the text that they want to appear by default each time a new PR is opened. Often, these files remind users of the list of things that need to be done before a PR is read to be reviewed. Below we show this in action. On the left is the typical screen a user is prompted with when they open a pull request in a repository without a PR template. On the right is the PR template as well as that template rendered in the pull request. Checking for accuracy, enforcing conventions, and making your life easier .gitignore : The .gitignore file lists files and directories that should be ignored by git, meaning they won't be tracked or included in the repository. This typically includes temporary files, local configuration files, and sensitive information. Using a .gitignore file helps keep the repository clean and prevents the accidental inclusion of irrelevant or sensitive files. .github/lint.yml : The .github/lint.yml file defines a GitHub Actions workflow that sets up linting for the repository. Linting automatically checks the code for stylistic errors, programming errors, and other potential issues according to predefined rules. This helps maintain code quality and consistency across the project, making it easier to read, maintain, and contribute to (see this AUG lesson for more details). .pre-commit-config.yaml : The .pre-commit-config.yaml file configures pre-commit hooks for the repository. Pre-commit hooks are scripts that run automatically on each commit to check for common issues, such as syntax errors, formatting issues, or failing tests. This setup helps catch and fix problems early in the development process, improving code quality and reducing the time spent on code reviews. .vscode/extensions.json : The .vscode/extensions.json file is a configuration file for Visual Studio Code that specifies recommended extensions for the project. This file will prompt VS Code to use ruff for linting and formatting. Makefile : The Makefile helps automate code quality and style checks, making it easier for developers to run all of these checks at once. Benefits of templates Low effort for best practices: The Python analysis template repository has 10 files. While none of these files are complicated on their own, creating each of them takes time and domain knowledge of tools and syntax. Further, if each user had to create each file on their own, there is a lot of room for error. Using a template saves this time and cuts down on the number of mistakes that users create in their repos while reinforcing best practices. Reviewed once: When a repository is started from a template, all of the code and files from that template are instantiated on the main branch. As you make your changes, reviewers only need to review the things you change, not all of the files from the template. This saves reviewers time because they don't have to review similar or idential text between many different repositories -- the content of a README.md or GitHub action workflow are reviewed when they are added to or changed in the template only. Overall, this saves reviewers time. Feedback in one place: Templates themselves are repositories, meaning they have Issues and Pull Requests. Users can report problems with a template or enhancements they'd like to see. When bugs are fixed or enhancements integrated, these changes are visible to every future project at Arcadia. This saves others from debugging the same problem you already fixed and helps keep all of our projects up to date. Some examples of this in action include: When Snakemake upgraded to version 8, the command line interface changed and a syntax update was needed to launch a workflow with conda. Keith and Mert caught this change and documented it in the Snakemake template README.md file. When I started a new snakemake workflow, I used their template documentation to launch my workflow and didn't need to spend time looking up the new command. The version of Snakemake installed in the Snakemake template causes a TypeError . I reported this issue on the template and how I fixed it so that future Snakemake workflow repositories know how to fix it. Eventually, we will incorporate this change into the template itself. Documentation and training: The templates themselves are centralized sources of best-practice documentation. Interfacing with this documentation and with the tools and conventions used in the templates trains users on those best practices. Consistency across the organization: With templates, we can achieve standardization across the organization in our package managers, directory structures, linting tools, etc. This makes it easier for someone to familiarize themselves with your work and to understand what different files or tools do. Other templates While we spent the majority of this lesson covering the python-analysis-template , there is also an r-analysis-template , a snakemake-template , and a nextflow-template . These templates are similar in spirit to the python analysis template but are tailored to different tools.","title":"Introduction to GitHub Templates"},{"location":"arcadia-users-group/20240305-github-templates/lesson/#introduction-to-github-templates","text":"GitHub templates provide a structured way to create new repositories, ensuring consistency, efficiency, and adherence to best practices across projects. GitHub templates are predefined repository setups that can be used to start new projects with a consistent set of files, folders, and configurations. This not only saves time but helps make sure that all projects follow Arcadia's coding and documentation standards. Templates typically include everything from README files, installation guides, contribution guidelines, and more. In this lesson, we will focus on Arcadia's python-analysis-template . Arcadia also has an r-analysis-template , a snakemake-template , and a nextflow-template .","title":"Introduction to GitHub Templates"},{"location":"arcadia-users-group/20240305-github-templates/lesson/#creating-a-new-repository-using-a-template","text":"","title":"Creating a new repository using a template"},{"location":"arcadia-users-group/20240305-github-templates/lesson/#choosing-the-right-template","text":"Depending on the analysis type or the technology stack used in your project, select the appropriate template. For instance, use r-analysis-template for R-based analyses, python-analysis-template for Python projects, snakemake-template for workflows managed with Snakemake, and nextflow-template for Nextflow pipelines. Why isn't there a Python package template? We could create one. We haven't generated one yet because there are a ton templates out there that could be used and we haven't created many Python packages yet as an organization. If you find yourself in need of a Python template, try out an existing template and make note of what works well and what doesn't. We can then use that to generate a future organization standard template.","title":"Choosing the Right Template"},{"location":"arcadia-users-group/20240305-github-templates/lesson/#repository-initialization","text":"There are two ways to create a new repo with a template.","title":"Repository Initialization"},{"location":"arcadia-users-group/20240305-github-templates/lesson/#option-1","text":"Navigate to the template you want to start from (e.x. https://github.com/Arcadia-Science/python-analysis-template). Click the green \"Use this template\" button in the upper right hand corner. This will take you to the New Repository launch screen with the \"Repository template\" section already filled out. Fill out the rest of the new repository information and click the \"Create repository\" button. This will generate a new repository that already has all of the files and folders that were in the template.","title":"Option 1"},{"location":"arcadia-users-group/20240305-github-templates/lesson/#option-2","text":"Click the \"+\" button in the upper right hand corner of any GitHub page and select \"New repository\" from the drop down menu. On the New Repository launch screen, select the template you wish to start from in the \"Repository template\" drop down menu. Fill out the rest of the new repository information and click the \"Create repository\" button. This will generate a new repository that already has all of the files and folders that were in the template. Working in branches After your new repository is created, create a new branch before you start making any changes. Do your work in branches and when you are ready to merge your changes into the main branch, open a Pull Request and request a review. For more on how to use branches, see this lesson . For more on why we think code review is important, see this blog post (note we didn't write this blog post, we just agree with a lot of the content). We recommend setting up branch protection rules that can prevent code from making it into main without a Pull Request and review. How to set up branch protection rules in your repository Branch protection rules are configured by clicking on the \"branches\" menu item on the lefthand side of the repo \"settings\" page. The following Branch protection rules should be enabled: Require a pull request before merging : this blocks anyone from pushing directly to the main branch. Require approvals: this requires that there be at least one approving review before a PR can be merged. Require status checks to pass before merging : this blocks PRs from being merged until all of the CI checks are passing.","title":"Option 2"},{"location":"arcadia-users-group/20240305-github-templates/lesson/#anatomy-of-a-template-python-analysis-template","text":"We can create a new repo from a template! But what is actually in the template and what do those files do? Below we go through each file in the python-analysis-template and discuss their functions and how template users can interact with them.","title":"Anatomy of a template: Python analysis template"},{"location":"arcadia-users-group/20240305-github-templates/lesson/#documentation-and-re-use","text":"README.md : This README has two sections. The first is an outline for the documentation for the new repository. It includes boilerplate sections and language that we expect will be standard across most projects, as well as TODO statements where specific details need to be changed or filled in. The second section covers developer notes detailing what the files in the repository do and tips for how to use them. The second section should be deleted in repos created from the template but is included to make the template easier to understand and use. envs/dev.yml : By default, we recommend analysis repositories use conda to manage software installations and record software versions. The envs directory is where these yaml files are stored. The dev.yml file included in the template is not empty but lists the packages required by the template: python, jupyter, pip, and linting software. LICENSE : The LICENSE file specifies the terms under which the software can be freely used, modified, and shared. Technically, software must have a license for it to be re-usable by others. The default license for all of our GitHub repositories is MIT, however the license may need to change depending on the software the project uses and the code it includes. Be sure to check with legal about your license before making a GitHub repository public. .github/PULL_REQUEST_TEMPLATE.md : The .github folder controls some behavior of the GitHub repository itself. The PULL_REQUEST_TEMPLATE.md is a text document where users can provide the text that they want to appear by default each time a new PR is opened. Often, these files remind users of the list of things that need to be done before a PR is read to be reviewed. Below we show this in action. On the left is the typical screen a user is prompted with when they open a pull request in a repository without a PR template. On the right is the PR template as well as that template rendered in the pull request.","title":"Documentation and re-use"},{"location":"arcadia-users-group/20240305-github-templates/lesson/#checking-for-accuracy-enforcing-conventions-and-making-your-life-easier","text":".gitignore : The .gitignore file lists files and directories that should be ignored by git, meaning they won't be tracked or included in the repository. This typically includes temporary files, local configuration files, and sensitive information. Using a .gitignore file helps keep the repository clean and prevents the accidental inclusion of irrelevant or sensitive files. .github/lint.yml : The .github/lint.yml file defines a GitHub Actions workflow that sets up linting for the repository. Linting automatically checks the code for stylistic errors, programming errors, and other potential issues according to predefined rules. This helps maintain code quality and consistency across the project, making it easier to read, maintain, and contribute to (see this AUG lesson for more details). .pre-commit-config.yaml : The .pre-commit-config.yaml file configures pre-commit hooks for the repository. Pre-commit hooks are scripts that run automatically on each commit to check for common issues, such as syntax errors, formatting issues, or failing tests. This setup helps catch and fix problems early in the development process, improving code quality and reducing the time spent on code reviews. .vscode/extensions.json : The .vscode/extensions.json file is a configuration file for Visual Studio Code that specifies recommended extensions for the project. This file will prompt VS Code to use ruff for linting and formatting. Makefile : The Makefile helps automate code quality and style checks, making it easier for developers to run all of these checks at once.","title":"Checking for accuracy, enforcing conventions, and making your life easier"},{"location":"arcadia-users-group/20240305-github-templates/lesson/#benefits-of-templates","text":"Low effort for best practices: The Python analysis template repository has 10 files. While none of these files are complicated on their own, creating each of them takes time and domain knowledge of tools and syntax. Further, if each user had to create each file on their own, there is a lot of room for error. Using a template saves this time and cuts down on the number of mistakes that users create in their repos while reinforcing best practices. Reviewed once: When a repository is started from a template, all of the code and files from that template are instantiated on the main branch. As you make your changes, reviewers only need to review the things you change, not all of the files from the template. This saves reviewers time because they don't have to review similar or idential text between many different repositories -- the content of a README.md or GitHub action workflow are reviewed when they are added to or changed in the template only. Overall, this saves reviewers time. Feedback in one place: Templates themselves are repositories, meaning they have Issues and Pull Requests. Users can report problems with a template or enhancements they'd like to see. When bugs are fixed or enhancements integrated, these changes are visible to every future project at Arcadia. This saves others from debugging the same problem you already fixed and helps keep all of our projects up to date. Some examples of this in action include: When Snakemake upgraded to version 8, the command line interface changed and a syntax update was needed to launch a workflow with conda. Keith and Mert caught this change and documented it in the Snakemake template README.md file. When I started a new snakemake workflow, I used their template documentation to launch my workflow and didn't need to spend time looking up the new command. The version of Snakemake installed in the Snakemake template causes a TypeError . I reported this issue on the template and how I fixed it so that future Snakemake workflow repositories know how to fix it. Eventually, we will incorporate this change into the template itself. Documentation and training: The templates themselves are centralized sources of best-practice documentation. Interfacing with this documentation and with the tools and conventions used in the templates trains users on those best practices. Consistency across the organization: With templates, we can achieve standardization across the organization in our package managers, directory structures, linting tools, etc. This makes it easier for someone to familiarize themselves with your work and to understand what different files or tools do.","title":"Benefits of templates"},{"location":"arcadia-users-group/20240305-github-templates/lesson/#other-templates","text":"While we spent the majority of this lesson covering the python-analysis-template , there is also an r-analysis-template , a snakemake-template , and a nextflow-template . These templates are similar in spirit to the python analysis template but are tailored to different tools.","title":"Other templates"},{"location":"arcadia-users-group/20240416-intro-to-ml1/make_peptides_ml_dataset/","text":"Documentation for how the dataset was made We used the environment defined in make_peptides_ml_dataset.yml to make the data set below. Originally, we downloaded the peptipedia database using the following Google Drive link: https://drive.google.com/file/d/1x3yHNl8k5teHlBI2FMgl966o51s0T8i_/view?usp=sharing. The link for this database is provided by the peptipedia GitHub repo . For persistence, we uploaded this file to this OSF repository , as well as some of the processed data created by the code below. Unzip the database. The unzipped archive contains three files, backup_sql.zip , peptipedia_csv.zip , and peptipedia_fasta.zip . Unzipping peptipedia_fasta.zip results in a FASTA file peptipedia.fasta that contains all of the peptide sequences from the peptipedia database. Unzipping peptipedia_csv.zip results in a CSV file peptipedia.csv that records the peptide id, peptide sequence, and predicted bioactivity labels. Unzipping backup_sql.zip results in a postgres SQL dump file backup_dump.sql . This file contains metadata associated with the peptide sequences. To extract the metadata to separate CSV tables, we used the python code: import re output_file_prefix = 'table_' current_table = None output_file = None with open ( 'backup_dump.sql' , 'r' ) as dump_file : for line in dump_file : # Check for COPY command start if line . startswith ( 'COPY' ): table_name = line . split ()[ 1 ] # Assumes table name is the second word current_table = table_name output_file = open ( f ' { output_file_prefix }{ current_table } .csv' , 'w' ) elif line . startswith ( '\\.' ): # End of data for the current table if output_file : output_file . close () output_file = None elif output_file : # Convert tab-separated values to comma-separated values csv_line = line . replace ( ' \\t ' , ',' ) output_file . write ( csv_line ) This code extracts the following CSV files: table_public.activity.csv table_public.activity_spectral.csv table_public.db.csv table_public.encoding.csv table_public.gene_ontology.csv table_public.index.csv table_public.patent.csv table_public.peptide.csv table_public.peptide_has_activity.csv table_public.peptide_has_db_has_index.csv table_public.peptide_has_go.csv table_public.pfam.csv Filter to peptides that are at least k in length. MMSeqs2 clustering in step 4 will fail if any of the sequences are below the k-mer length used for clustering. seqkit seq --min-len 10 -o peptipedia_minlen10.fasta peptipedia.fasta Cluster the sequences. The goal of this step is to reduce homology in peptide sequences so that the data set can be split in any way and not have pollution between the train and test sets. It uses a threshold of 80% because this has been previously shown to be sufficient for homology reduction in protein sequences ( source ). mmseqs easy-cluster peptipedia_minlen10.fasta mmseqs/peptipedia_minlen10-0.8 tmp --min-seq-id 0 .8 Filter to clusters composed of peptides that all have the same bioactivity prediction and write a TSV file. Many of the peptides that end up in clusters have multiple bioactivity predictions (e.g. antimicrobial and therapeutic). The script below filters to peptide clusters that all have at least one bioactivity label that is the same. It also adds additional metadata (GO, PFAM) from the SQL database to the data.frame of peptide information. library ( tidyverse ) library ( janitor ) # The goal of this script is to create a demo machine learning data set from peptides and their metadata. # It filters peptides in the peptipedia database to large clusters of similar peptides # (80% identity or greater) that all have the same bioactivity label. # It then joins selected peptides to their metadata. # filter to peptides clusters with many members that have the same bioactivity -------- # read in cluster information from mmseqs2 linclust clusters <- read_tsv ( \"~/Downloads/backup_peptipedia_29_03_2023/mmseqs/peptipedia_minlen10-0.8_cluster.tsv\" , col_names = c ( \"rep\" , \"member\" )) # read in peptide bioactivity information peptide_bioactivity <- read_csv ( '~/Downloads/backup_peptipedia_29_03_2023/peptipedia.csv' ) %>% clean_names () # make the data set a little bit smaller by filtering to clusters that have > 2 members # otherwise, the lines of code to find clusters with all of the same bioactivity takes too long cluster_sizes <- clusters %>% group_by ( rep ) %>% tally () %>% arrange ( desc ( n )) large_clusters <- cluster_sizes %>% filter ( n > 2 ) clusters_filtered <- clusters %>% filter ( rep %in% large_clusters $ rep ) # filter the bioactivity data set to match peptide_bioactivity_filtered <- peptide_bioactivity %>% filter ( idpeptide %in% clusters_filtered $ member ) # filter to clusters that all have the same bioactivity # (e.g. all peptides have a predicted bioactivity of \"antimicrobial\" in a given cluster; # note that most peptides have multiple bioactivity labels. # this code filters to clusters where at least one bioactivity label is the same for all peptides in the cluster.) peptide_bioactivity_with_clusters <- left_join ( clusters_filtered , peptide_bioactivity_filtered , by = c ( \"member\" = \"idpeptide\" )) peptide_bioactivity_with_clusters_long <- peptide_bioactivity_with_clusters %>% pivot_longer ( cols = starts_with ( \"allergen\" ) : last_col (), names_to = \"bioactivity\" , values_to = \"value\" ) # Find clusters where all members have a 1 for any bioactivity clusters_all_1_any_bioactivity <- peptide_bioactivity_with_clusters_long %>% group_by ( rep , bioactivity ) %>% summarise ( all_ones = all ( value == 1 ), .groups = 'drop' ) %>% filter ( all_ones ) %>% distinct ( rep , .keep_all = TRUE ) # filter to those clusters where all members have at least 1 bioactivity the same peptide_group_bioactivities <- peptide_bioactivity_with_clusters %>% filter ( rep %in% clusters_all_1_any_bioactivity $ rep ) %>% left_join ( clusters_all_1_any_bioactivity , by = \"rep\" ) %>% # select only the representative for the cluster so that there can't be pollution between test and train data no matter how it is split; # sequences will at most be < 80% similar to any other sequence in the dataset. filter ( rep == member ) %>% select ( peptipedia_peptide_id = rep , peptide_sequence = sequence , group_bioactivity = bioactivity ) # remove peptides where there are very few representatives (< 50) few_peptides_with_group_bioactivity <- peptide_group_bioactivities %>% group_by ( group_bioactivity ) %>% tally () %>% filter ( n < 50 ) peptide_group_bioactivities_filtered <- peptide_group_bioactivities %>% filter ( ! group_bioactivity %in% few_peptides_with_group_bioactivity $ group_bioactivity ) # Record all bioactivity predictions for each peptide by concatenating the bioactivity names peptide_bioactivities_concatenated <- peptide_bioactivity_with_clusters_long %>% filter ( value == 1 ) %>% filter ( rep == member ) %>% select ( peptipedia_peptide_id = rep , bioactivity , value ) %>% group_by ( peptipedia_peptide_id ) %>% summarise ( all_bioactivities = str_c ( bioactivity , collapse = \";\" ), .groups = 'drop' ) # Join this back to your original (or relevant) dataset to add the concatenated bioactivities column final_dataset <- peptide_group_bioactivities_filtered %>% left_join ( peptide_bioactivities_concatenated , by = \"peptipedia_peptide_id\" ) %>% distinct () # join with other peptide metadata ---------------------------------------- gene_ontology <- read_csv ( \"~/Downloads/backup_peptipedia_29_03_2023/table_public.gene_ontology.csv\" , col_names = c ( \"peptideid\" , \"go_id\" , \"go_description\" , \"go_category\" )) %>% select ( peptideid , go_id ) pfam <- read_csv ( \"~/Downloads/backup_peptipedia_29_03_2023/table_public.pfam.csv\" , col_names = c ( \"peptideid\" , \"pfam_id\" , \"pfam_name\" , \"pfam_type\" )) final_dataset <- final_dataset %>% left_join ( gene_ontology , by = c ( \"peptipedia_peptide_id\" = \"peptideid\" )) %>% left_join ( pfam , by = c ( \"peptipedia_peptide_id\" = \"peptideid\" )) %>% distinct () final_dataset %>% group_by ( group_bioactivity ) %>% tally () %>% arrange ( desc ( n )) %>% knitr :: kable () write_tsv ( final_dataset , \"~/Downloads/backup_peptipedia_29_03_2023/peptides_for_ml_tmp2.tsv\" ) Convert the output TSV to a FASTA file that can be used to calculate peptide characteristics (molecular weight, etc). cut -f1,2 peptides_for_ml_tmp.tsv | tail -n +2 | seqkit tab2fx > peptides_for_ml.fasta Calculate peptide characteristics (length, mw, etc.) curl https://raw.githubusercontent.com/Arcadia-Science/peptigate/main/scripts/characterize_peptides.py python characterize_peptides.py peptides_for_ml.fasta peptides_for_ml_characterized.tsv Combine peptide metadata with other information peptide_characterization <- read_tsv ( \"~/Downloads/backup_peptipedia_29_03_2023/peptides_for_ml_characterized.tsv\" ) final_dataset <- final_dataset %>% left_join ( peptide_characterization , by = c ( \"peptipedia_peptide_id\" = \"peptide_id\" )) %>% mutate ( length = nchar ( peptide_sequence )) %>% rename ( bioactivity = group_bioactivity ) write_tsv ( final_dataset , \"peptides_ml_dataset.tsv\" ) Check that the data set is predictive of the label \"group_bioactivity\". library ( caret ) set.seed ( 123 ) final_dataset <- read_tsv ( \"peptides_ml_dataset.tsv\" ) final_dataset_filtered <- final_dataset %>% select ( - peptipedia_peptide_id , - peptide_sequence , - all_bioactivities , - go_id , - pfam_id , - pfam_name , - pfam_type ) trainingIndex <- createDataPartition ( final_dataset_filtered $ bioactivity , p = . 8 , list = FALSE ) trainingData <- final_dataset_filtered [ trainingIndex , ] testData <- final_dataset_filtered [ - trainingIndex , ] control <- trainControl ( method = \"cv\" , number = 10 ) # 10-fold cross-validation model <- train ( bioactivity ~ . , data = trainingData , method = \"rf\" , trControl = control ) predictions <- predict ( model , newdata = testData ) confusionMatrix ( predictions , as.factor ( testData $ bioactivity )) The confusionMatrix() command prints the following two results to the console. The model has the following accuracy: Overall Statistics Accuracy : 0.6188 95% CI : (0.5805, 0.6559) No Information Rate : 0.2496 P-Value [Acc > NIR] : < 2.2e-16 Kappa : 0.5316 Mcnemar's Test P-Value : NA and with per class performance of: Reference Prediction amphibian_defense anti_gram antibacterial_antibiotic antimicrobial growth_factor hormone immunological_activity neurological_activity propeptide signal_peptide therapeutic amphibian_defense 0 0 0 3 0 0 0 0 0 0 0 anti_gram 0 0 0 1 0 0 0 0 0 0 0 antibacterial_antibiotic 0 0 0 0 0 0 0 0 1 3 0 antimicrobial 12 9 9 119 0 9 9 4 3 16 11 growth_factor 0 0 0 0 0 0 0 0 0 0 0 hormone 0 0 0 3 2 8 1 4 3 5 3 immunological_activity 3 0 0 10 0 3 47 3 1 1 2 neurological_activity 0 0 0 0 0 0 1 0 0 1 1 propeptide 0 0 1 9 1 1 1 1 59 10 2 signal_peptide 1 0 6 9 9 14 2 9 18 120 1 therapeutic 1 1 3 11 0 1 0 1 2 0 56 Statistics by Class: Class: amphibian_defense Class: anti_gram Class: antibacterial_antibiotic Class: antimicrobial Class: growth_factor Class: hormone Class: immunological_activity Class: neurological_activity Class: propeptide Class: signal_peptide Class: therapeutic Sensitivity 0.000000 0.000000 0.000000 0.7212 0.00000 0.22222 0.77049 0.000000 0.67816 0.7692 0.73684 Specificity 0.995342 0.998464 0.993769 0.8347 1.00000 0.96640 0.96167 0.995305 0.95470 0.8634 0.96581 Pos Pred Value 0.000000 0.000000 0.000000 0.5920 NaN 0.27586 0.67143 0.000000 0.69412 0.6349 0.73684 Neg Pred Value 0.974164 0.984848 0.971081 0.9000 0.98185 0.95570 0.97631 0.966565 0.95139 0.9237 0.96581 Prevalence 0.025719 0.015129 0.028744 0.2496 0.01815 0.05446 0.09228 0.033283 0.13162 0.2360 0.11498 Detection Rate 0.000000 0.000000 0.000000 0.1800 0.00000 0.01210 0.07110 0.000000 0.08926 0.1815 0.08472 Detection Prevalence 0.004539 0.001513 0.006051 0.3041 0.00000 0.04387 0.10590 0.004539 0.12859 0.2859 0.11498 Balanced Accuracy 0.497671 0.499232 0.496885 0.7779 0.50000 0.59431 0.86608 0.497653 0.81643 0.8163 0.85133 Note that the accuracy is low-ish by design. We wanted a teaching data set we can demonstrate how to improve performance through tuning or by using different models or analysis techniques. Documentation of data set contents Column descriptions peptipedia_peptide_id : peptide id of the peptide in the peptipedia database. All peptides were representative sequences in mmseqs2 clustering (80% threshold). peptide_sequence : peptide sequence. bioactivity : bioactivity label for the peptide. This was derived by taking all bioactivity labels for all peptides in a cluster and selecting only clusters where all peptides had at least one label that was assigned to all peptides. We kept the first of such labels and use this as the \"bioactivity\" label for the peptide. all_bioactivities : all predicted bioactivities for a given peptide. go_id : gene ontology ID. pfam_id : PFAM (protein family) ID. pfam_name : name of PFAM protein family. pfam_type : type of PFAM protein family. aliphatic_index : relative volume occupied by aliphatic side chains (Alanine, Valine, Isoleucine, and Leucine). boman_index : The potential interaction index proposed by Boman (2003) is an index computed by averaging the solubility values for all residues in a sequence. It can be used to give an overall estimate of the potential of a peptide to bind to membranes or other proteins. A value greater than 2.48 indicates that a protein has high binding potential. charge : the theoretical net charge of a peptide sequence based on the Henderson-Hasselbach equation. Computed at a pH of 7 with the Lehninger pKscale. hydrophobicity : average of the hydrophobicity values of each residue using one of the 39 scales from different sources. instability_index : predicts the stability of a protein based on its dipeptide composition. A protein whose instability index is smaller than 40 is predicted as stable, a value above 40 predicts that the protein may be unstable. isoelectric_point : the pH at which a particular molecule or surface carries no net electrical charge. molecular_weight : the sum of the mass of each amino acid in the peptide sequence. pd1_residue_volume : physical descriptor related to the peptide residue volume. pd2_hydrophilicity : physical descriptor related to the peptide hydrophilicity. z1_lipophilicity : Zscale quantifying the lipophilicity of the peptide. z2_steric_bulk_or_polarizability : Zscale modeling steric properties like steric bulk and polarizability. z3_polarity_or_charge : Zscale quantifying electronic properties like polarity and charge. z4_electronegativity_etc : Zscale relating to electronegativity, heat of formation, electrophilicity and hardness. z5_electronegativity_etc : Zscale relating to electronegativity, heat of formation, electrophilicity and hardness. length : length in amino acids of the peptide. Potential future additions or improvements We could make the data set creation script into a Snakemake workflow to better document how it was generated. We could BLAST each peptide sequence to add more metadata (peptide function, gene name, species/taxonomy, number of hits, etc). This might be helpful especially if we need more character columns.","title":"Make peptides ml dataset"},{"location":"arcadia-users-group/20240416-intro-to-ml1/make_peptides_ml_dataset/#documentation-for-how-the-dataset-was-made","text":"We used the environment defined in make_peptides_ml_dataset.yml to make the data set below. Originally, we downloaded the peptipedia database using the following Google Drive link: https://drive.google.com/file/d/1x3yHNl8k5teHlBI2FMgl966o51s0T8i_/view?usp=sharing. The link for this database is provided by the peptipedia GitHub repo . For persistence, we uploaded this file to this OSF repository , as well as some of the processed data created by the code below. Unzip the database. The unzipped archive contains three files, backup_sql.zip , peptipedia_csv.zip , and peptipedia_fasta.zip . Unzipping peptipedia_fasta.zip results in a FASTA file peptipedia.fasta that contains all of the peptide sequences from the peptipedia database. Unzipping peptipedia_csv.zip results in a CSV file peptipedia.csv that records the peptide id, peptide sequence, and predicted bioactivity labels. Unzipping backup_sql.zip results in a postgres SQL dump file backup_dump.sql . This file contains metadata associated with the peptide sequences. To extract the metadata to separate CSV tables, we used the python code: import re output_file_prefix = 'table_' current_table = None output_file = None with open ( 'backup_dump.sql' , 'r' ) as dump_file : for line in dump_file : # Check for COPY command start if line . startswith ( 'COPY' ): table_name = line . split ()[ 1 ] # Assumes table name is the second word current_table = table_name output_file = open ( f ' { output_file_prefix }{ current_table } .csv' , 'w' ) elif line . startswith ( '\\.' ): # End of data for the current table if output_file : output_file . close () output_file = None elif output_file : # Convert tab-separated values to comma-separated values csv_line = line . replace ( ' \\t ' , ',' ) output_file . write ( csv_line ) This code extracts the following CSV files: table_public.activity.csv table_public.activity_spectral.csv table_public.db.csv table_public.encoding.csv table_public.gene_ontology.csv table_public.index.csv table_public.patent.csv table_public.peptide.csv table_public.peptide_has_activity.csv table_public.peptide_has_db_has_index.csv table_public.peptide_has_go.csv table_public.pfam.csv Filter to peptides that are at least k in length. MMSeqs2 clustering in step 4 will fail if any of the sequences are below the k-mer length used for clustering. seqkit seq --min-len 10 -o peptipedia_minlen10.fasta peptipedia.fasta Cluster the sequences. The goal of this step is to reduce homology in peptide sequences so that the data set can be split in any way and not have pollution between the train and test sets. It uses a threshold of 80% because this has been previously shown to be sufficient for homology reduction in protein sequences ( source ). mmseqs easy-cluster peptipedia_minlen10.fasta mmseqs/peptipedia_minlen10-0.8 tmp --min-seq-id 0 .8 Filter to clusters composed of peptides that all have the same bioactivity prediction and write a TSV file. Many of the peptides that end up in clusters have multiple bioactivity predictions (e.g. antimicrobial and therapeutic). The script below filters to peptide clusters that all have at least one bioactivity label that is the same. It also adds additional metadata (GO, PFAM) from the SQL database to the data.frame of peptide information. library ( tidyverse ) library ( janitor ) # The goal of this script is to create a demo machine learning data set from peptides and their metadata. # It filters peptides in the peptipedia database to large clusters of similar peptides # (80% identity or greater) that all have the same bioactivity label. # It then joins selected peptides to their metadata. # filter to peptides clusters with many members that have the same bioactivity -------- # read in cluster information from mmseqs2 linclust clusters <- read_tsv ( \"~/Downloads/backup_peptipedia_29_03_2023/mmseqs/peptipedia_minlen10-0.8_cluster.tsv\" , col_names = c ( \"rep\" , \"member\" )) # read in peptide bioactivity information peptide_bioactivity <- read_csv ( '~/Downloads/backup_peptipedia_29_03_2023/peptipedia.csv' ) %>% clean_names () # make the data set a little bit smaller by filtering to clusters that have > 2 members # otherwise, the lines of code to find clusters with all of the same bioactivity takes too long cluster_sizes <- clusters %>% group_by ( rep ) %>% tally () %>% arrange ( desc ( n )) large_clusters <- cluster_sizes %>% filter ( n > 2 ) clusters_filtered <- clusters %>% filter ( rep %in% large_clusters $ rep ) # filter the bioactivity data set to match peptide_bioactivity_filtered <- peptide_bioactivity %>% filter ( idpeptide %in% clusters_filtered $ member ) # filter to clusters that all have the same bioactivity # (e.g. all peptides have a predicted bioactivity of \"antimicrobial\" in a given cluster; # note that most peptides have multiple bioactivity labels. # this code filters to clusters where at least one bioactivity label is the same for all peptides in the cluster.) peptide_bioactivity_with_clusters <- left_join ( clusters_filtered , peptide_bioactivity_filtered , by = c ( \"member\" = \"idpeptide\" )) peptide_bioactivity_with_clusters_long <- peptide_bioactivity_with_clusters %>% pivot_longer ( cols = starts_with ( \"allergen\" ) : last_col (), names_to = \"bioactivity\" , values_to = \"value\" ) # Find clusters where all members have a 1 for any bioactivity clusters_all_1_any_bioactivity <- peptide_bioactivity_with_clusters_long %>% group_by ( rep , bioactivity ) %>% summarise ( all_ones = all ( value == 1 ), .groups = 'drop' ) %>% filter ( all_ones ) %>% distinct ( rep , .keep_all = TRUE ) # filter to those clusters where all members have at least 1 bioactivity the same peptide_group_bioactivities <- peptide_bioactivity_with_clusters %>% filter ( rep %in% clusters_all_1_any_bioactivity $ rep ) %>% left_join ( clusters_all_1_any_bioactivity , by = \"rep\" ) %>% # select only the representative for the cluster so that there can't be pollution between test and train data no matter how it is split; # sequences will at most be < 80% similar to any other sequence in the dataset. filter ( rep == member ) %>% select ( peptipedia_peptide_id = rep , peptide_sequence = sequence , group_bioactivity = bioactivity ) # remove peptides where there are very few representatives (< 50) few_peptides_with_group_bioactivity <- peptide_group_bioactivities %>% group_by ( group_bioactivity ) %>% tally () %>% filter ( n < 50 ) peptide_group_bioactivities_filtered <- peptide_group_bioactivities %>% filter ( ! group_bioactivity %in% few_peptides_with_group_bioactivity $ group_bioactivity ) # Record all bioactivity predictions for each peptide by concatenating the bioactivity names peptide_bioactivities_concatenated <- peptide_bioactivity_with_clusters_long %>% filter ( value == 1 ) %>% filter ( rep == member ) %>% select ( peptipedia_peptide_id = rep , bioactivity , value ) %>% group_by ( peptipedia_peptide_id ) %>% summarise ( all_bioactivities = str_c ( bioactivity , collapse = \";\" ), .groups = 'drop' ) # Join this back to your original (or relevant) dataset to add the concatenated bioactivities column final_dataset <- peptide_group_bioactivities_filtered %>% left_join ( peptide_bioactivities_concatenated , by = \"peptipedia_peptide_id\" ) %>% distinct () # join with other peptide metadata ---------------------------------------- gene_ontology <- read_csv ( \"~/Downloads/backup_peptipedia_29_03_2023/table_public.gene_ontology.csv\" , col_names = c ( \"peptideid\" , \"go_id\" , \"go_description\" , \"go_category\" )) %>% select ( peptideid , go_id ) pfam <- read_csv ( \"~/Downloads/backup_peptipedia_29_03_2023/table_public.pfam.csv\" , col_names = c ( \"peptideid\" , \"pfam_id\" , \"pfam_name\" , \"pfam_type\" )) final_dataset <- final_dataset %>% left_join ( gene_ontology , by = c ( \"peptipedia_peptide_id\" = \"peptideid\" )) %>% left_join ( pfam , by = c ( \"peptipedia_peptide_id\" = \"peptideid\" )) %>% distinct () final_dataset %>% group_by ( group_bioactivity ) %>% tally () %>% arrange ( desc ( n )) %>% knitr :: kable () write_tsv ( final_dataset , \"~/Downloads/backup_peptipedia_29_03_2023/peptides_for_ml_tmp2.tsv\" ) Convert the output TSV to a FASTA file that can be used to calculate peptide characteristics (molecular weight, etc). cut -f1,2 peptides_for_ml_tmp.tsv | tail -n +2 | seqkit tab2fx > peptides_for_ml.fasta Calculate peptide characteristics (length, mw, etc.) curl https://raw.githubusercontent.com/Arcadia-Science/peptigate/main/scripts/characterize_peptides.py python characterize_peptides.py peptides_for_ml.fasta peptides_for_ml_characterized.tsv Combine peptide metadata with other information peptide_characterization <- read_tsv ( \"~/Downloads/backup_peptipedia_29_03_2023/peptides_for_ml_characterized.tsv\" ) final_dataset <- final_dataset %>% left_join ( peptide_characterization , by = c ( \"peptipedia_peptide_id\" = \"peptide_id\" )) %>% mutate ( length = nchar ( peptide_sequence )) %>% rename ( bioactivity = group_bioactivity ) write_tsv ( final_dataset , \"peptides_ml_dataset.tsv\" ) Check that the data set is predictive of the label \"group_bioactivity\". library ( caret ) set.seed ( 123 ) final_dataset <- read_tsv ( \"peptides_ml_dataset.tsv\" ) final_dataset_filtered <- final_dataset %>% select ( - peptipedia_peptide_id , - peptide_sequence , - all_bioactivities , - go_id , - pfam_id , - pfam_name , - pfam_type ) trainingIndex <- createDataPartition ( final_dataset_filtered $ bioactivity , p = . 8 , list = FALSE ) trainingData <- final_dataset_filtered [ trainingIndex , ] testData <- final_dataset_filtered [ - trainingIndex , ] control <- trainControl ( method = \"cv\" , number = 10 ) # 10-fold cross-validation model <- train ( bioactivity ~ . , data = trainingData , method = \"rf\" , trControl = control ) predictions <- predict ( model , newdata = testData ) confusionMatrix ( predictions , as.factor ( testData $ bioactivity )) The confusionMatrix() command prints the following two results to the console. The model has the following accuracy: Overall Statistics Accuracy : 0.6188 95% CI : (0.5805, 0.6559) No Information Rate : 0.2496 P-Value [Acc > NIR] : < 2.2e-16 Kappa : 0.5316 Mcnemar's Test P-Value : NA and with per class performance of: Reference Prediction amphibian_defense anti_gram antibacterial_antibiotic antimicrobial growth_factor hormone immunological_activity neurological_activity propeptide signal_peptide therapeutic amphibian_defense 0 0 0 3 0 0 0 0 0 0 0 anti_gram 0 0 0 1 0 0 0 0 0 0 0 antibacterial_antibiotic 0 0 0 0 0 0 0 0 1 3 0 antimicrobial 12 9 9 119 0 9 9 4 3 16 11 growth_factor 0 0 0 0 0 0 0 0 0 0 0 hormone 0 0 0 3 2 8 1 4 3 5 3 immunological_activity 3 0 0 10 0 3 47 3 1 1 2 neurological_activity 0 0 0 0 0 0 1 0 0 1 1 propeptide 0 0 1 9 1 1 1 1 59 10 2 signal_peptide 1 0 6 9 9 14 2 9 18 120 1 therapeutic 1 1 3 11 0 1 0 1 2 0 56 Statistics by Class: Class: amphibian_defense Class: anti_gram Class: antibacterial_antibiotic Class: antimicrobial Class: growth_factor Class: hormone Class: immunological_activity Class: neurological_activity Class: propeptide Class: signal_peptide Class: therapeutic Sensitivity 0.000000 0.000000 0.000000 0.7212 0.00000 0.22222 0.77049 0.000000 0.67816 0.7692 0.73684 Specificity 0.995342 0.998464 0.993769 0.8347 1.00000 0.96640 0.96167 0.995305 0.95470 0.8634 0.96581 Pos Pred Value 0.000000 0.000000 0.000000 0.5920 NaN 0.27586 0.67143 0.000000 0.69412 0.6349 0.73684 Neg Pred Value 0.974164 0.984848 0.971081 0.9000 0.98185 0.95570 0.97631 0.966565 0.95139 0.9237 0.96581 Prevalence 0.025719 0.015129 0.028744 0.2496 0.01815 0.05446 0.09228 0.033283 0.13162 0.2360 0.11498 Detection Rate 0.000000 0.000000 0.000000 0.1800 0.00000 0.01210 0.07110 0.000000 0.08926 0.1815 0.08472 Detection Prevalence 0.004539 0.001513 0.006051 0.3041 0.00000 0.04387 0.10590 0.004539 0.12859 0.2859 0.11498 Balanced Accuracy 0.497671 0.499232 0.496885 0.7779 0.50000 0.59431 0.86608 0.497653 0.81643 0.8163 0.85133 Note that the accuracy is low-ish by design. We wanted a teaching data set we can demonstrate how to improve performance through tuning or by using different models or analysis techniques.","title":"Documentation for how the dataset was made"},{"location":"arcadia-users-group/20240416-intro-to-ml1/make_peptides_ml_dataset/#documentation-of-data-set-contents","text":"","title":"Documentation of data set contents"},{"location":"arcadia-users-group/20240416-intro-to-ml1/make_peptides_ml_dataset/#column-descriptions","text":"peptipedia_peptide_id : peptide id of the peptide in the peptipedia database. All peptides were representative sequences in mmseqs2 clustering (80% threshold). peptide_sequence : peptide sequence. bioactivity : bioactivity label for the peptide. This was derived by taking all bioactivity labels for all peptides in a cluster and selecting only clusters where all peptides had at least one label that was assigned to all peptides. We kept the first of such labels and use this as the \"bioactivity\" label for the peptide. all_bioactivities : all predicted bioactivities for a given peptide. go_id : gene ontology ID. pfam_id : PFAM (protein family) ID. pfam_name : name of PFAM protein family. pfam_type : type of PFAM protein family. aliphatic_index : relative volume occupied by aliphatic side chains (Alanine, Valine, Isoleucine, and Leucine). boman_index : The potential interaction index proposed by Boman (2003) is an index computed by averaging the solubility values for all residues in a sequence. It can be used to give an overall estimate of the potential of a peptide to bind to membranes or other proteins. A value greater than 2.48 indicates that a protein has high binding potential. charge : the theoretical net charge of a peptide sequence based on the Henderson-Hasselbach equation. Computed at a pH of 7 with the Lehninger pKscale. hydrophobicity : average of the hydrophobicity values of each residue using one of the 39 scales from different sources. instability_index : predicts the stability of a protein based on its dipeptide composition. A protein whose instability index is smaller than 40 is predicted as stable, a value above 40 predicts that the protein may be unstable. isoelectric_point : the pH at which a particular molecule or surface carries no net electrical charge. molecular_weight : the sum of the mass of each amino acid in the peptide sequence. pd1_residue_volume : physical descriptor related to the peptide residue volume. pd2_hydrophilicity : physical descriptor related to the peptide hydrophilicity. z1_lipophilicity : Zscale quantifying the lipophilicity of the peptide. z2_steric_bulk_or_polarizability : Zscale modeling steric properties like steric bulk and polarizability. z3_polarity_or_charge : Zscale quantifying electronic properties like polarity and charge. z4_electronegativity_etc : Zscale relating to electronegativity, heat of formation, electrophilicity and hardness. z5_electronegativity_etc : Zscale relating to electronegativity, heat of formation, electrophilicity and hardness. length : length in amino acids of the peptide.","title":"Column descriptions"},{"location":"arcadia-users-group/20240416-intro-to-ml1/make_peptides_ml_dataset/#potential-future-additions-or-improvements","text":"We could make the data set creation script into a Snakemake workflow to better document how it was generated. We could BLAST each peptide sequence to add more metadata (peptide function, gene name, species/taxonomy, number of hits, etc). This might be helpful especially if we need more character columns.","title":"Potential future additions or improvements"},{"location":"arcadia-users-group/20240730-arcadia-pycolor/lesson/","text":"Using arcadia-pycolor to make figures in Python arcadia-pycolor is a Python package that provides a set of color palettes and convenience functions to style plots so that they are compatible with Arcadia's style guide. This notebook provides a quick introduction to arcadia_pycolor and how to use it to style matplotlib and seaborn plots so that they comply with the Arcadia style guide, largely following the Quickstart Guide . We'll cover the following topics: Installing arcadia-pycolor Configuring matplotlib defaults Accessing and using colors, palettes, and gradients Using arcadia-pycolor to style plots 1. Install the arcadia-pycolor package We've designed arcadia-pycolor as a Python package that is distributed via PyPI. This means it can be installed using pip just like any other Python package. In a virtual environment of your choice, run the following command in your terminal: pip install arcadia-pycolor Requirements arcadia-pycolor requires Python >=3.9 and has the following dependencies (which are detected and installed automatically when you install arcadia-pycolor using pip ): - matplotlib (version 3.7 and above, except version 3.8.0) - colorspacious (version 1.1.2 and above) Additionally, arcadia-pycolor checks whether you have the \"Suisse Int'l\" family of fonts installed on your system. If these fonts aren't installed, arcadia-pycolor will default to \"Arial\" . To download the \"Suisse Int'l\" font family, check out the Brand Assets page in Notion. The package can then be imported in notebooks or scripts using the following command: import arcadia_pycolor as apc 2. Set the default matplotlib styles The package provides a function called apc.mpl.setup that sets the default matplotlib styles to match the Arcadia style guide. This function only needs to be called once, ideally at the beginning of the notebook or script. The styles it sets will automatically apply to all plots in the notebook or script. What does apc.mpl.setup do? Sets the default font to \"Suisse Int'l\" (or \"Arial\" if not installed). Changes default axis, axis label, legend, and color cycler styles to match the Arcadia style. Registers Arcadia's colors, palettes, and gradients with matplotlib . import matplotlib as mpl import matplotlib.pyplot as plt def example_plot ( title : str = \"Example plot\" ): plt . figure ( figsize = ( 5 , 3.5 )) plt . plot ([ 3 , 1 , 4 , 1 , 4 ], label = \"group a\" ) plt . plot ([ 2 , 7 , 1 , 4 , 2 ], label = \"group b\" ) plt . xlabel ( \"time\" ) plt . ylabel ( \"value\" ) plt . legend ( title = \"category\" ) plt . title ( title ) # Plot using default rcParams example_plot ( \"Using default rcParams\" ) plt . show () # Plot using apc.mpl.setup() apc . mpl . setup () example_plot ( \"Using arcadia-pycolor\" ) plt . show () Fontconfig warning: ignoring UTF-8: not a valid region tag Some aspects of the style guide can only be applied to individual plots. The apc.mpl.style_plot function can be used to apply these styles to a single plot. This function takes a Matplotlib Axes object as input. If an Axes object is not passed to style_plot , the function will style the current plot (internally, style_plot uses plt.gca() to get the \"current\" Axes object). By default, the style_plot function capitalizes the x- and y-axis labels and styles the legend, if one exists. In addition, it has a few optional arguments that can be used to customize the styling of the x- and y-axis tick labels: monospaced_axes sets the tick labels of the x- and/or y-axis to a monospaced font. categorical_axes adjusts the x- and/or y-axis styles to be more readable when the axis represents a categorical variable. colorbar_exists tells the function to style the colorbar, if one exists. example_plot ( \"Using mpl.style_plot\" ) apc . mpl . style_plot ( monospaced_axes = \"both\" ) plt . show () 3. Using Arcadia colors The Arcadia style guide defines sets of colors called \"color palettes\" that should be used in all figures. The arcadia_pycolor package provides easy access to both individual colors and to pre-defined palettes and gradients. Within arcadia-pycolor , all of Arcadia's named colors are available as variables. For example, you can access and preview the color aegean as follows. When the cell is evaluated, it will output the name and hex code of the color alongside a swatch showing what the color looks like: apc . aegean You can pass colors directly to matplotlib functions to style plots. Named colors are also registered to matplotlib using the prefix \"apc:\" . For example, \"apc:aegean\" is a valid color argument for matplotlib functions. import matplotlib.pyplot as plt import arcadia_pycolor as apc plt . plot ([ 1 , 2 , 3 ], [ 4 , 5 , 6 ], color = apc . rose ) plt . plot ([ 1 , 2 , 3 ], [ 8 , 10 , 12 ], color = \"apc:aegean\" ) plt . xlabel ( \"time\" ) plt . ylabel ( \"value\" ) apc . mpl . style_plot ( monospaced_axes = \"both\" ) plt . show () Using color palettes Individual colors are organized into groups called \"palettes.\" The palettes themselves have names and are accessible as attributes of the apc.palettes module. For example, we can rewrite the previous example to use the first two colors in the \"primary\" palette: plt . plot ([ 1 , 2 , 3 ], [ 4 , 5 , 6 ], color = apc . palettes . primary . colors [ 0 ]) plt . plot ([ 1 , 2 , 3 ], [ 8 , 10 , 12 ], color = apc . palettes . secondary . colors [ 1 ]) plt . xlabel ( \"time\" ) plt . ylabel ( \"value\" ) apc . mpl . style_plot ( monospaced_axes = \"both\" ) plt . show () To see all of the colors in a palette, evaluate the palette object in a notebook cell. This outputs a list of color swatches with the names and hex codes of the colors in the palette: apc . palettes . primary Using color gradients The Arcadia style guide also defines continuous color gradients that can be used in plots like heatmaps. These gradients are accessible as attributes of the apc.gradients module. To use a gradient in a matplotlib or seaborn plot, you can convert it to a matplotlib colormap using the to_mpl_cmap method. For example, to use the \"blues\" gradient in a heatmap: import numpy as np import seaborn as sns data = np . random . rand ( 10 , 10 ) sns . heatmap ( data , square = True , cmap = apc . gradients . blues . to_mpl_cmap ()) apc . mpl . style_plot ( categorical_axes = \"both\" , monospaced_axes = \"both\" , colorbar_exists = True ) plt . show () Gradients are also registered to matplotlib using the prefix \"apc:\" . For example, \"apc:reds\" is a valid colormap argument for matplotlib or seaborn functions: sns . heatmap ( data , square = True , cmap = \"apc:reds\" ) apc . mpl . style_plot ( categorical_axes = \"both\" , monospaced_axes = \"both\" , colorbar_exists = True ) plt . show () Just like palettes, gradients can be visualized by evaluating a gradient object in a Jupyter notebook cell. This outputs a gradient swatch showing the colors in the gradient. apc . gradients . viridis 4. More resources You can find more resources and examples in the arcadia-pycolor documentation .","title":"Using `arcadia-pycolor` to make figures in Python"},{"location":"arcadia-users-group/20240730-arcadia-pycolor/lesson/#using-arcadia-pycolor-to-make-figures-in-python","text":"arcadia-pycolor is a Python package that provides a set of color palettes and convenience functions to style plots so that they are compatible with Arcadia's style guide. This notebook provides a quick introduction to arcadia_pycolor and how to use it to style matplotlib and seaborn plots so that they comply with the Arcadia style guide, largely following the Quickstart Guide . We'll cover the following topics: Installing arcadia-pycolor Configuring matplotlib defaults Accessing and using colors, palettes, and gradients Using arcadia-pycolor to style plots","title":"Using arcadia-pycolor to make figures in Python"},{"location":"arcadia-users-group/20240730-arcadia-pycolor/lesson/#1-install-the-arcadia-pycolor-package","text":"We've designed arcadia-pycolor as a Python package that is distributed via PyPI. This means it can be installed using pip just like any other Python package. In a virtual environment of your choice, run the following command in your terminal: pip install arcadia-pycolor","title":"1. Install the arcadia-pycolor package"},{"location":"arcadia-users-group/20240730-arcadia-pycolor/lesson/#requirements","text":"arcadia-pycolor requires Python >=3.9 and has the following dependencies (which are detected and installed automatically when you install arcadia-pycolor using pip ): - matplotlib (version 3.7 and above, except version 3.8.0) - colorspacious (version 1.1.2 and above) Additionally, arcadia-pycolor checks whether you have the \"Suisse Int'l\" family of fonts installed on your system. If these fonts aren't installed, arcadia-pycolor will default to \"Arial\" . To download the \"Suisse Int'l\" font family, check out the Brand Assets page in Notion. The package can then be imported in notebooks or scripts using the following command: import arcadia_pycolor as apc","title":"Requirements"},{"location":"arcadia-users-group/20240730-arcadia-pycolor/lesson/#2-set-the-default-matplotlib-styles","text":"The package provides a function called apc.mpl.setup that sets the default matplotlib styles to match the Arcadia style guide. This function only needs to be called once, ideally at the beginning of the notebook or script. The styles it sets will automatically apply to all plots in the notebook or script.","title":"2. Set the default matplotlib styles"},{"location":"arcadia-users-group/20240730-arcadia-pycolor/lesson/#what-does-apcmplsetup-do","text":"Sets the default font to \"Suisse Int'l\" (or \"Arial\" if not installed). Changes default axis, axis label, legend, and color cycler styles to match the Arcadia style. Registers Arcadia's colors, palettes, and gradients with matplotlib . import matplotlib as mpl import matplotlib.pyplot as plt def example_plot ( title : str = \"Example plot\" ): plt . figure ( figsize = ( 5 , 3.5 )) plt . plot ([ 3 , 1 , 4 , 1 , 4 ], label = \"group a\" ) plt . plot ([ 2 , 7 , 1 , 4 , 2 ], label = \"group b\" ) plt . xlabel ( \"time\" ) plt . ylabel ( \"value\" ) plt . legend ( title = \"category\" ) plt . title ( title ) # Plot using default rcParams example_plot ( \"Using default rcParams\" ) plt . show () # Plot using apc.mpl.setup() apc . mpl . setup () example_plot ( \"Using arcadia-pycolor\" ) plt . show () Fontconfig warning: ignoring UTF-8: not a valid region tag Some aspects of the style guide can only be applied to individual plots. The apc.mpl.style_plot function can be used to apply these styles to a single plot. This function takes a Matplotlib Axes object as input. If an Axes object is not passed to style_plot , the function will style the current plot (internally, style_plot uses plt.gca() to get the \"current\" Axes object). By default, the style_plot function capitalizes the x- and y-axis labels and styles the legend, if one exists. In addition, it has a few optional arguments that can be used to customize the styling of the x- and y-axis tick labels: monospaced_axes sets the tick labels of the x- and/or y-axis to a monospaced font. categorical_axes adjusts the x- and/or y-axis styles to be more readable when the axis represents a categorical variable. colorbar_exists tells the function to style the colorbar, if one exists. example_plot ( \"Using mpl.style_plot\" ) apc . mpl . style_plot ( monospaced_axes = \"both\" ) plt . show ()","title":"What does apc.mpl.setup do?"},{"location":"arcadia-users-group/20240730-arcadia-pycolor/lesson/#3-using-arcadia-colors","text":"The Arcadia style guide defines sets of colors called \"color palettes\" that should be used in all figures. The arcadia_pycolor package provides easy access to both individual colors and to pre-defined palettes and gradients. Within arcadia-pycolor , all of Arcadia's named colors are available as variables. For example, you can access and preview the color aegean as follows. When the cell is evaluated, it will output the name and hex code of the color alongside a swatch showing what the color looks like: apc . aegean You can pass colors directly to matplotlib functions to style plots. Named colors are also registered to matplotlib using the prefix \"apc:\" . For example, \"apc:aegean\" is a valid color argument for matplotlib functions. import matplotlib.pyplot as plt import arcadia_pycolor as apc plt . plot ([ 1 , 2 , 3 ], [ 4 , 5 , 6 ], color = apc . rose ) plt . plot ([ 1 , 2 , 3 ], [ 8 , 10 , 12 ], color = \"apc:aegean\" ) plt . xlabel ( \"time\" ) plt . ylabel ( \"value\" ) apc . mpl . style_plot ( monospaced_axes = \"both\" ) plt . show ()","title":"3. Using Arcadia colors"},{"location":"arcadia-users-group/20240730-arcadia-pycolor/lesson/#using-color-palettes","text":"Individual colors are organized into groups called \"palettes.\" The palettes themselves have names and are accessible as attributes of the apc.palettes module. For example, we can rewrite the previous example to use the first two colors in the \"primary\" palette: plt . plot ([ 1 , 2 , 3 ], [ 4 , 5 , 6 ], color = apc . palettes . primary . colors [ 0 ]) plt . plot ([ 1 , 2 , 3 ], [ 8 , 10 , 12 ], color = apc . palettes . secondary . colors [ 1 ]) plt . xlabel ( \"time\" ) plt . ylabel ( \"value\" ) apc . mpl . style_plot ( monospaced_axes = \"both\" ) plt . show () To see all of the colors in a palette, evaluate the palette object in a notebook cell. This outputs a list of color swatches with the names and hex codes of the colors in the palette: apc . palettes . primary","title":"Using color palettes"},{"location":"arcadia-users-group/20240730-arcadia-pycolor/lesson/#using-color-gradients","text":"The Arcadia style guide also defines continuous color gradients that can be used in plots like heatmaps. These gradients are accessible as attributes of the apc.gradients module. To use a gradient in a matplotlib or seaborn plot, you can convert it to a matplotlib colormap using the to_mpl_cmap method. For example, to use the \"blues\" gradient in a heatmap: import numpy as np import seaborn as sns data = np . random . rand ( 10 , 10 ) sns . heatmap ( data , square = True , cmap = apc . gradients . blues . to_mpl_cmap ()) apc . mpl . style_plot ( categorical_axes = \"both\" , monospaced_axes = \"both\" , colorbar_exists = True ) plt . show () Gradients are also registered to matplotlib using the prefix \"apc:\" . For example, \"apc:reds\" is a valid colormap argument for matplotlib or seaborn functions: sns . heatmap ( data , square = True , cmap = \"apc:reds\" ) apc . mpl . style_plot ( categorical_axes = \"both\" , monospaced_axes = \"both\" , colorbar_exists = True ) plt . show () Just like palettes, gradients can be visualized by evaluating a gradient object in a Jupyter notebook cell. This outputs a gradient swatch showing the colors in the gradient. apc . gradients . viridis","title":"Using color gradients"},{"location":"arcadia-users-group/20240730-arcadia-pycolor/lesson/#4-more-resources","text":"You can find more resources and examples in the arcadia-pycolor documentation .","title":"4. More resources"},{"location":"arcadia-users-group/20240730-arcadiathemer/20240730-arcadiathemer/","text":"Using arcadiathemeR to make figures in R arcadiathemeR is an R package that provides functions to create ggplot2 -style figures that (mostly) adhere to the Arcadia Science style guide. This lesson provides a quick introduction to arcadiathemeR and how to use it to style ggplot2 -style plots, which largely follows the package README documentation . We\u2019ll cover the following topics: Installing arcadiathemeR Using arcadiathemeR to layer onto a ggplot2 plot Accessing and using colors, palettes, and gradients Saving plots Prerequisites and Installation You need R installed, and arcadiathemeR requires at least R version >= 4.0 (the package was built and tested with version 4.3.1). Install R for Mac OS X here . You can use RStudio or a Jupyter notebook for creating plots. Install RStudio for Mac OSx here or Jupyter in a conda environment following these instructions . To use the custom fonts you need to download the TTF formatted font files and place in the Users/YOURUSERNAME/Library/Fonts/ directory. You can also double click on the fonts to install them using FontBook. Check out the Arcadia Science Brand Assets page in Notion to find these. This should only need to be performed once even if the package is updated over time. You must download the TTF formatted files to be compatible with arcadiathemeR . If you want to download both the TTF and OTF formatted files to use both arcadiathemeR and the arcadia-pycolor python package that\u2019s fine - there will just be duplicate selections for each font in Illustrator. Install arcadiathemeR with the remotes package: # install.packages(\"remotes\") remotes::install_github(\"Arcadia-Science/arcadiathemeR\") Load the package to use in your scripts with library(arcadiathemeR) . When you first load the package in a new R session it will print a message about whether the custom fonts have been accessed and loaded correctly. Layering onto an existing ggplot2 plot There are two main functions you will use to layer onto an existing ggplot2 plot to create plots adhering to the Arcadia style guide and access palettes. These are the theme_arcadia and scale functions. The particular scale function differs on whether you use color or fill to access colors. Here is how you use these functions without changing the default arguments: library ( ggplot2 ) library ( arcadiathemeR ) #> Loading Suisse fonts... #> All custom fonts 'Suisse Int'l, Suisse Int'l Semi Bold, Suisse Int'l Medium, Suisse Int'l Mono' are successfully loaded. ggplot ( data = mtcars , aes ( x = hp , y = mpg , color = as.factor ( cyl ))) + geom_point () + theme_arcadia () + scale_color_arcadia () If you want to change the color palette used, you can access this with the palette_name argument in the scale function: ggplot ( data = mtcars , aes ( x = hp , y = mpg , color = as.factor ( cyl ))) + geom_point () + theme_arcadia () + scale_color_arcadia ( palette_name = \"primary\" ) We have specific font specifications for whether the represented data is categorical or numerical. In the next plot where the x-axis is different categories, you can see where the font isn\u2019t specified correctly: ggplot ( data = diamonds , aes ( x = cut , fill = cut )) + geom_bar () + theme_arcadia () + scale_fill_arcadia () To use the correct font type, you can specify what type of data you have on each axis by using the x_axis_type or y_axis_type arguments in the theme_arcadia function: ggplot ( data = diamonds , aes ( x = cut , fill = cut )) + geom_bar () + theme_arcadia ( x_axis_type = \"categorical\" ) + scale_fill_arcadia () In addition to specifying which palette to use in the scale function, you can also reverse the colors of the scale used with reverse=TRUE . You can also use other ggplot2 or theme specifications on top of these functions, such as moving the position of the legend or modifying the scales to remove whitespace between the axis lines and the bars: ggplot ( data = diamonds , aes ( x = cut , fill = cut )) + geom_bar () + theme_arcadia ( x_axis_type = \"categorical\" ) + scale_fill_arcadia ( palette_name = \"secondary\" , reverse = TRUE ) + scale_y_continuous ( expand = c ( 0 , 0 )) + # removes whitespace between axis and bars theme ( legend.position = \"bottom\" ) In addition to reversing the order of the colors used in the palette, you can select different indices of colors from the palettes within the scale function with the start and end arguments: ggplot ( mtcars , aes ( x = hp , fill = as.factor ( cyl ))) + geom_density ( alpha = 0.8 , linewidth = 0 ) + # remove border line from filled-in density plots theme_arcadia () + scale_fill_arcadia ( palette_name = \"blue_shades\" , start = 2 , end = 5 ) + scale_y_continuous ( expand = c ( 0 , 0 )) + scale_x_continuous ( expand = c ( 0 , 0 )) # remove whitespace between both axes and the plot The scale functions are used to specify which palettes to use. The gradient functions are used to access the gradient palettes, which work in the same way as the scale functions using color or fill : ggplot ( data = mtcars , aes ( x = hp , y = mpg , color = hp )) + geom_point ( size = 2.5 ) + theme_arcadia () + gradient_color_arcadia ( palette_name = \"lisafrank\" ) There are also single color gradients available in additions to the gradient palettes that are useful for heatmap plots. You can also remove the background color with background=FALSE , which is recommended when exporting plots, which is described below. library ( reshape2 ) # heatmap of correlation matrix data ( iris ) iris_data <- iris [, 1 : 4 ] cor_matrix <- cor ( iris_data ) melted_cor_matrix <- ( melt ( cor_matrix )) ggplot ( melted_cor_matrix , aes ( x = Var1 , y = Var2 , fill = value )) + geom_tile () + theme_arcadia ( x_axis_type = \"categorical\" , y_axis_type = \"categorical\" , background = FALSE ) + gradient_fill_arcadia ( palette_name = \"reds\" ) + theme ( axis.text.x = element_text ( angle = 45 , hjust = 1 ), legend.position = \"top\" , axis.line = element_blank ()) + labs ( x = \"\" , y = \"\" ) + scale_y_discrete ( expand = c ( 0 , 0 )) + scale_x_discrete ( expand = c ( 0 , 0 )) Accessing palettes and specific colors To view all the color palette options and the individual hex codes that comprise each palette, you can view these with show_arcadia_palettes : show_arcadia_palettes () #> $primary #> [1] \"#5088C5\" \"#F28360\" \"#3B9886\" \"#F7B846\" \"#7A77AB\" \"#F898AE\" \"#73B5E3\" #> [8] \"#FFB984\" \"#F5E4BE\" \"#BABEE0\" \"#97CD78\" \"#C85152\" #> #> $secondary #> [1] \"#C6E7F4\" \"#F8C5C1\" \"#DBD1C3\" \"#B6C8D4\" \"#B5BEA4\" \"#DA9085\" \"#8A99AD\" #> [8] \"#EDE0D6\" #> #> $primary_ordered #> [1] \"#5088C5\" \"#F28360\" \"#F7B846\" \"#97CD78\" \"#7A77AB\" \"#F898AE\" \"#3B9886\" #> [8] \"#C85152\" \"#73B5E3\" \"#FFB984\" \"#F5E4BE\" \"#BABEE0\" #> #> $secondary_ordered #> [1] \"#C6E7F4\" \"#F8C5C1\" \"#DBD1C3\" \"#B5BEA4\" \"#B6C8D4\" \"#DA9085\" \"#EDE0D6\" #> [8] \"#8A99AD\" #> #> $neutrals #> [1] \"#FFFFFF\" \"#EBEDE8\" \"#BAB0A8\" \"#8F8885\" \"#43413F\" \"#484B50\" \"#292928\" #> [8] \"#09090A\" \"#596F74\" #> #> $blue_shades #> [1] \"#C6E7F4\" \"#73B5E3\" \"#5088C5\" \"#2B65A1\" \"#094468\" #> #> $orange_shades #> [1] \"#FFCFAF\" \"#FFB984\" \"#F28360\" \"#C85152\" \"#9E3F41\" #> #> $yellow_shades #> [1] \"#F5E4BE\" \"#FFD364\" \"#F7B846\" \"#D68D22\" \"#A85E28\" #> #> $purple_shades #> [1] \"#DCDFEF\" \"#BABEE0\" \"#7A77AB\" \"#54448C\" \"#341E60\" #> #> $teal_shades #> [1] \"#C3E2DB\" \"#6FBCAD\" \"#3B9886\" \"#2A6B5E\" \"#09473E\" #> #> $pink_shades #> [1] \"#FFE3D4\" \"#F8C5C1\" \"#F898AE\" \"#E2718F\" \"#C04C70\" #> #> $warm_gray_shades #> [1] \"#EDE6DA\" \"#DBD1C3\" \"#BAB0A8\" \"#8F8885\" \"#635C5A\" #> #> $cool_gray_shades #> [1] \"#E6EAED\" \"#CAD4DB\" \"#ABBAC4\" \"#8A99AD\" \"#687787\" To view gradient options and colors, use show_arcadia_gradients . The gradients also show the positions for each individual color in the gradient. You can use these lists of hex codes to create custom or reordered sets of palettes. show_arcadia_gradients () #> $magma #> $magma$colors #> [1] \"#341E60\" \"#54448C\" \"#A96789\" \"#E9A482\" \"#F5DFB2\" #> #> $magma$positions #> [1] 0.000 0.217 0.498 0.799 1.000 #> #> #> $verde #> $verde$colors #> [1] \"#09473E\" \"#4E7F72\" \"#FFCC7B\" \"#FFE3D4\" #> #> $verde$positions #> [1] 0.000 0.357 0.909 1.000 #> #> #> $viridis #> $viridis$colors #> [1] \"#282A49\" \"#5088C5\" \"#97CD78\" \"#FFFDBD\" #> #> $viridis$positions #> [1] 0.000 0.468 0.746 1.000 #> #> #> $wine #> $wine$colors #> [1] \"#52180A\" \"#C85152\" \"#FFB984\" \"#F8F4F1\" #> #> $wine$positions #> [1] 0.000 0.451 0.828 1.000 #> #> #> $lisafrank #> $lisafrank$colors #> [1] \"#09473E\" \"#5088C5\" \"#BABEE0\" \"#F4CAE3\" #> #> $lisafrank$positions #> [1] 0.000 0.484 0.862 1.000 #> #> #> $sunset #> $sunset$colors #> [1] \"#4D2500\" \"#A85E28\" \"#E9A482\" \"#FFCC7B\" \"#FFE3D4\" #> #> $sunset$positions #> [1] 0.000 0.407 0.767 0.915 1.000 #> #> #> $oranges #> $oranges$colors #> [1] \"#964222\" \"#FFB984\" \"#F8F4F1\" #> #> $oranges$positions #> [1] 0.000 0.761 1.000 #> #> #> $sages #> $sages$colors #> [1] \"#2A6B5E\" \"#B5BEA4\" \"#F7FBEF\" #> #> $sages$positions #> [1] 0.000 0.641 1.000 #> #> #> $orangesage #> $orangesage$colors #> [1] \"#964222\" \"#FFB984\" \"#F8F4F1\" \"#F7FBEF\" \"#B5BEA4\" \"#2A6B5E\" #> #> $orangesage$positions #> [1] 0.000 0.761 1.000 1.000 0.641 0.000 #> #> #> $reds #> $reds$colors #> [1] \"#9E3F41\" \"#C85152\" \"#FFF3F4\" #> #> $reds$positions #> [1] 0.000 0.212 1.000 #> #> #> $blues #> $blues$colors #> [1] \"#2B65A1\" \"#5088C5\" \"#F4FBFF\" #> #> $blues$positions #> [1] 0.000 0.254 1.000 #> #> #> $redblue #> $redblue$colors #> [1] \"#9E3F41\" \"#C85152\" \"#FFF3F4\" \"#F4FBFF\" \"#5088C5\" \"#2B65A1\" #> #> $redblue$positions #> [1] 0.000 0.212 1.000 1.000 0.254 0.000 #> #> #> $purples #> $purples$colors #> [1] \"#6862AB\" \"#7A77AB\" \"#FCF7FF\" #> #> $purples$positions #> [1] 0.000 0.144 1.000 #> #> #> $greens #> $greens$colors #> [1] \"#47784A\" \"#97CD78\" \"#F7FBEF\" #> #> $greens$positions #> [1] 0.000 0.622 1.000 #> #> #> $purplegreen #> $purplegreen$colors #> [1] \"#6862AB\" \"#7A77AB\" \"#FCF7FF\" \"#F7FBEF\" \"#97CD78\" \"#47784A\" #> #> $purplegreen$positions #> [1] 0.000 0.144 1.000 1.000 0.622 0.000 Exporting plots To save plots, we have a custom save_arcadia_plot function built on top of ggsave that helps you export plots that adhere to our size guidelines and can be used with the Illustrator templates. The different plot size options are \"full_wide\", \"float_wide\", \"half_square\", \"full_square\" , or \"float_square\" . These panel sizes adhere to the panel sizes available in the Illustrator templates. If you want to save plots in different formats (such as PNG or JPG) or sizes that aren\u2019t available through this convenience function, you can still use the regular ggsave function. Additionally for the background to be transparent in exported plots you need to set the background argument to FALSE in the theme_arcadia function: plot <- ggplot ( data = diamonds , aes ( x = cut , fill = cut )) + geom_bar () + theme_arcadia ( x_axis_type = \"categorical\" , background = FALSE ) + scale_fill_arcadia ( palette_name = \"secondary\" , reverse = TRUE ) + scale_y_continuous ( expand = c ( 0 , 0 )) + # removes whitespace between axis and bars theme ( legend.position = \"bottom\" ) save_arcadia_plot ( \"figures/arcadia-plot.pdf\" , plot , panel_size = \"full_square\" ) Resources This lesson follows the examples given in the arcadiathemeR documentation . Open an issue in the repo to request new features or report bugs as you come across them!","title":"Using `arcadiathemeR` to make figures in R"},{"location":"arcadia-users-group/20240730-arcadiathemer/20240730-arcadiathemer/#using-arcadiathemer-to-make-figures-in-r","text":"arcadiathemeR is an R package that provides functions to create ggplot2 -style figures that (mostly) adhere to the Arcadia Science style guide. This lesson provides a quick introduction to arcadiathemeR and how to use it to style ggplot2 -style plots, which largely follows the package README documentation . We\u2019ll cover the following topics: Installing arcadiathemeR Using arcadiathemeR to layer onto a ggplot2 plot Accessing and using colors, palettes, and gradients Saving plots","title":"Using arcadiathemeR to make figures in R"},{"location":"arcadia-users-group/20240730-arcadiathemer/20240730-arcadiathemer/#prerequisites-and-installation","text":"You need R installed, and arcadiathemeR requires at least R version >= 4.0 (the package was built and tested with version 4.3.1). Install R for Mac OS X here . You can use RStudio or a Jupyter notebook for creating plots. Install RStudio for Mac OSx here or Jupyter in a conda environment following these instructions . To use the custom fonts you need to download the TTF formatted font files and place in the Users/YOURUSERNAME/Library/Fonts/ directory. You can also double click on the fonts to install them using FontBook. Check out the Arcadia Science Brand Assets page in Notion to find these. This should only need to be performed once even if the package is updated over time. You must download the TTF formatted files to be compatible with arcadiathemeR . If you want to download both the TTF and OTF formatted files to use both arcadiathemeR and the arcadia-pycolor python package that\u2019s fine - there will just be duplicate selections for each font in Illustrator. Install arcadiathemeR with the remotes package: # install.packages(\"remotes\") remotes::install_github(\"Arcadia-Science/arcadiathemeR\") Load the package to use in your scripts with library(arcadiathemeR) . When you first load the package in a new R session it will print a message about whether the custom fonts have been accessed and loaded correctly.","title":"Prerequisites and Installation"},{"location":"arcadia-users-group/20240730-arcadiathemer/20240730-arcadiathemer/#layering-onto-an-existing-ggplot2-plot","text":"There are two main functions you will use to layer onto an existing ggplot2 plot to create plots adhering to the Arcadia style guide and access palettes. These are the theme_arcadia and scale functions. The particular scale function differs on whether you use color or fill to access colors. Here is how you use these functions without changing the default arguments: library ( ggplot2 ) library ( arcadiathemeR ) #> Loading Suisse fonts... #> All custom fonts 'Suisse Int'l, Suisse Int'l Semi Bold, Suisse Int'l Medium, Suisse Int'l Mono' are successfully loaded. ggplot ( data = mtcars , aes ( x = hp , y = mpg , color = as.factor ( cyl ))) + geom_point () + theme_arcadia () + scale_color_arcadia () If you want to change the color palette used, you can access this with the palette_name argument in the scale function: ggplot ( data = mtcars , aes ( x = hp , y = mpg , color = as.factor ( cyl ))) + geom_point () + theme_arcadia () + scale_color_arcadia ( palette_name = \"primary\" ) We have specific font specifications for whether the represented data is categorical or numerical. In the next plot where the x-axis is different categories, you can see where the font isn\u2019t specified correctly: ggplot ( data = diamonds , aes ( x = cut , fill = cut )) + geom_bar () + theme_arcadia () + scale_fill_arcadia () To use the correct font type, you can specify what type of data you have on each axis by using the x_axis_type or y_axis_type arguments in the theme_arcadia function: ggplot ( data = diamonds , aes ( x = cut , fill = cut )) + geom_bar () + theme_arcadia ( x_axis_type = \"categorical\" ) + scale_fill_arcadia () In addition to specifying which palette to use in the scale function, you can also reverse the colors of the scale used with reverse=TRUE . You can also use other ggplot2 or theme specifications on top of these functions, such as moving the position of the legend or modifying the scales to remove whitespace between the axis lines and the bars: ggplot ( data = diamonds , aes ( x = cut , fill = cut )) + geom_bar () + theme_arcadia ( x_axis_type = \"categorical\" ) + scale_fill_arcadia ( palette_name = \"secondary\" , reverse = TRUE ) + scale_y_continuous ( expand = c ( 0 , 0 )) + # removes whitespace between axis and bars theme ( legend.position = \"bottom\" ) In addition to reversing the order of the colors used in the palette, you can select different indices of colors from the palettes within the scale function with the start and end arguments: ggplot ( mtcars , aes ( x = hp , fill = as.factor ( cyl ))) + geom_density ( alpha = 0.8 , linewidth = 0 ) + # remove border line from filled-in density plots theme_arcadia () + scale_fill_arcadia ( palette_name = \"blue_shades\" , start = 2 , end = 5 ) + scale_y_continuous ( expand = c ( 0 , 0 )) + scale_x_continuous ( expand = c ( 0 , 0 )) # remove whitespace between both axes and the plot The scale functions are used to specify which palettes to use. The gradient functions are used to access the gradient palettes, which work in the same way as the scale functions using color or fill : ggplot ( data = mtcars , aes ( x = hp , y = mpg , color = hp )) + geom_point ( size = 2.5 ) + theme_arcadia () + gradient_color_arcadia ( palette_name = \"lisafrank\" ) There are also single color gradients available in additions to the gradient palettes that are useful for heatmap plots. You can also remove the background color with background=FALSE , which is recommended when exporting plots, which is described below. library ( reshape2 ) # heatmap of correlation matrix data ( iris ) iris_data <- iris [, 1 : 4 ] cor_matrix <- cor ( iris_data ) melted_cor_matrix <- ( melt ( cor_matrix )) ggplot ( melted_cor_matrix , aes ( x = Var1 , y = Var2 , fill = value )) + geom_tile () + theme_arcadia ( x_axis_type = \"categorical\" , y_axis_type = \"categorical\" , background = FALSE ) + gradient_fill_arcadia ( palette_name = \"reds\" ) + theme ( axis.text.x = element_text ( angle = 45 , hjust = 1 ), legend.position = \"top\" , axis.line = element_blank ()) + labs ( x = \"\" , y = \"\" ) + scale_y_discrete ( expand = c ( 0 , 0 )) + scale_x_discrete ( expand = c ( 0 , 0 ))","title":"Layering onto an existing ggplot2 plot"},{"location":"arcadia-users-group/20240730-arcadiathemer/20240730-arcadiathemer/#accessing-palettes-and-specific-colors","text":"To view all the color palette options and the individual hex codes that comprise each palette, you can view these with show_arcadia_palettes : show_arcadia_palettes () #> $primary #> [1] \"#5088C5\" \"#F28360\" \"#3B9886\" \"#F7B846\" \"#7A77AB\" \"#F898AE\" \"#73B5E3\" #> [8] \"#FFB984\" \"#F5E4BE\" \"#BABEE0\" \"#97CD78\" \"#C85152\" #> #> $secondary #> [1] \"#C6E7F4\" \"#F8C5C1\" \"#DBD1C3\" \"#B6C8D4\" \"#B5BEA4\" \"#DA9085\" \"#8A99AD\" #> [8] \"#EDE0D6\" #> #> $primary_ordered #> [1] \"#5088C5\" \"#F28360\" \"#F7B846\" \"#97CD78\" \"#7A77AB\" \"#F898AE\" \"#3B9886\" #> [8] \"#C85152\" \"#73B5E3\" \"#FFB984\" \"#F5E4BE\" \"#BABEE0\" #> #> $secondary_ordered #> [1] \"#C6E7F4\" \"#F8C5C1\" \"#DBD1C3\" \"#B5BEA4\" \"#B6C8D4\" \"#DA9085\" \"#EDE0D6\" #> [8] \"#8A99AD\" #> #> $neutrals #> [1] \"#FFFFFF\" \"#EBEDE8\" \"#BAB0A8\" \"#8F8885\" \"#43413F\" \"#484B50\" \"#292928\" #> [8] \"#09090A\" \"#596F74\" #> #> $blue_shades #> [1] \"#C6E7F4\" \"#73B5E3\" \"#5088C5\" \"#2B65A1\" \"#094468\" #> #> $orange_shades #> [1] \"#FFCFAF\" \"#FFB984\" \"#F28360\" \"#C85152\" \"#9E3F41\" #> #> $yellow_shades #> [1] \"#F5E4BE\" \"#FFD364\" \"#F7B846\" \"#D68D22\" \"#A85E28\" #> #> $purple_shades #> [1] \"#DCDFEF\" \"#BABEE0\" \"#7A77AB\" \"#54448C\" \"#341E60\" #> #> $teal_shades #> [1] \"#C3E2DB\" \"#6FBCAD\" \"#3B9886\" \"#2A6B5E\" \"#09473E\" #> #> $pink_shades #> [1] \"#FFE3D4\" \"#F8C5C1\" \"#F898AE\" \"#E2718F\" \"#C04C70\" #> #> $warm_gray_shades #> [1] \"#EDE6DA\" \"#DBD1C3\" \"#BAB0A8\" \"#8F8885\" \"#635C5A\" #> #> $cool_gray_shades #> [1] \"#E6EAED\" \"#CAD4DB\" \"#ABBAC4\" \"#8A99AD\" \"#687787\" To view gradient options and colors, use show_arcadia_gradients . The gradients also show the positions for each individual color in the gradient. You can use these lists of hex codes to create custom or reordered sets of palettes. show_arcadia_gradients () #> $magma #> $magma$colors #> [1] \"#341E60\" \"#54448C\" \"#A96789\" \"#E9A482\" \"#F5DFB2\" #> #> $magma$positions #> [1] 0.000 0.217 0.498 0.799 1.000 #> #> #> $verde #> $verde$colors #> [1] \"#09473E\" \"#4E7F72\" \"#FFCC7B\" \"#FFE3D4\" #> #> $verde$positions #> [1] 0.000 0.357 0.909 1.000 #> #> #> $viridis #> $viridis$colors #> [1] \"#282A49\" \"#5088C5\" \"#97CD78\" \"#FFFDBD\" #> #> $viridis$positions #> [1] 0.000 0.468 0.746 1.000 #> #> #> $wine #> $wine$colors #> [1] \"#52180A\" \"#C85152\" \"#FFB984\" \"#F8F4F1\" #> #> $wine$positions #> [1] 0.000 0.451 0.828 1.000 #> #> #> $lisafrank #> $lisafrank$colors #> [1] \"#09473E\" \"#5088C5\" \"#BABEE0\" \"#F4CAE3\" #> #> $lisafrank$positions #> [1] 0.000 0.484 0.862 1.000 #> #> #> $sunset #> $sunset$colors #> [1] \"#4D2500\" \"#A85E28\" \"#E9A482\" \"#FFCC7B\" \"#FFE3D4\" #> #> $sunset$positions #> [1] 0.000 0.407 0.767 0.915 1.000 #> #> #> $oranges #> $oranges$colors #> [1] \"#964222\" \"#FFB984\" \"#F8F4F1\" #> #> $oranges$positions #> [1] 0.000 0.761 1.000 #> #> #> $sages #> $sages$colors #> [1] \"#2A6B5E\" \"#B5BEA4\" \"#F7FBEF\" #> #> $sages$positions #> [1] 0.000 0.641 1.000 #> #> #> $orangesage #> $orangesage$colors #> [1] \"#964222\" \"#FFB984\" \"#F8F4F1\" \"#F7FBEF\" \"#B5BEA4\" \"#2A6B5E\" #> #> $orangesage$positions #> [1] 0.000 0.761 1.000 1.000 0.641 0.000 #> #> #> $reds #> $reds$colors #> [1] \"#9E3F41\" \"#C85152\" \"#FFF3F4\" #> #> $reds$positions #> [1] 0.000 0.212 1.000 #> #> #> $blues #> $blues$colors #> [1] \"#2B65A1\" \"#5088C5\" \"#F4FBFF\" #> #> $blues$positions #> [1] 0.000 0.254 1.000 #> #> #> $redblue #> $redblue$colors #> [1] \"#9E3F41\" \"#C85152\" \"#FFF3F4\" \"#F4FBFF\" \"#5088C5\" \"#2B65A1\" #> #> $redblue$positions #> [1] 0.000 0.212 1.000 1.000 0.254 0.000 #> #> #> $purples #> $purples$colors #> [1] \"#6862AB\" \"#7A77AB\" \"#FCF7FF\" #> #> $purples$positions #> [1] 0.000 0.144 1.000 #> #> #> $greens #> $greens$colors #> [1] \"#47784A\" \"#97CD78\" \"#F7FBEF\" #> #> $greens$positions #> [1] 0.000 0.622 1.000 #> #> #> $purplegreen #> $purplegreen$colors #> [1] \"#6862AB\" \"#7A77AB\" \"#FCF7FF\" \"#F7FBEF\" \"#97CD78\" \"#47784A\" #> #> $purplegreen$positions #> [1] 0.000 0.144 1.000 1.000 0.622 0.000","title":"Accessing palettes and specific colors"},{"location":"arcadia-users-group/20240730-arcadiathemer/20240730-arcadiathemer/#exporting-plots","text":"To save plots, we have a custom save_arcadia_plot function built on top of ggsave that helps you export plots that adhere to our size guidelines and can be used with the Illustrator templates. The different plot size options are \"full_wide\", \"float_wide\", \"half_square\", \"full_square\" , or \"float_square\" . These panel sizes adhere to the panel sizes available in the Illustrator templates. If you want to save plots in different formats (such as PNG or JPG) or sizes that aren\u2019t available through this convenience function, you can still use the regular ggsave function. Additionally for the background to be transparent in exported plots you need to set the background argument to FALSE in the theme_arcadia function: plot <- ggplot ( data = diamonds , aes ( x = cut , fill = cut )) + geom_bar () + theme_arcadia ( x_axis_type = \"categorical\" , background = FALSE ) + scale_fill_arcadia ( palette_name = \"secondary\" , reverse = TRUE ) + scale_y_continuous ( expand = c ( 0 , 0 )) + # removes whitespace between axis and bars theme ( legend.position = \"bottom\" ) save_arcadia_plot ( \"figures/arcadia-plot.pdf\" , plot , panel_size = \"full_square\" )","title":"Exporting plots"},{"location":"arcadia-users-group/20240730-arcadiathemer/20240730-arcadiathemer/#resources","text":"This lesson follows the examples given in the arcadiathemeR documentation . Open an issue in the repo to request new features or report bugs as you come across them!","title":"Resources"},{"location":"workshops/overview/","text":"Arcadia Workshops This is the landing page for workshops hosted at Arcadia Science. Workshops are longer hands-on trainings that focus on building specific skill sets. See below for lesson materials for workshops that have been held at Arcadia. Date Tutorial Length Description Recording Oct 6, 2023 Git & GitHub for Hacktoberfest optional This lesson is an optional refresher for participants in our Hacktoberfest mini-hackathon. It covers how to make contributions to repositories that you don't own via forks and pull requests. Sept 20, 2022 Introduction to Git & GitHub 2 hours This workshop introduces the Git version control system and how to use GitHub for collaboration. It covers git clone , git add , git commit , git push , git pull , git checkout , git branch , git status , git restore , git revert , pull requests, and code review. recording","title":"Workshops"},{"location":"workshops/overview/#arcadia-workshops","text":"This is the landing page for workshops hosted at Arcadia Science. Workshops are longer hands-on trainings that focus on building specific skill sets. See below for lesson materials for workshops that have been held at Arcadia. Date Tutorial Length Description Recording Oct 6, 2023 Git & GitHub for Hacktoberfest optional This lesson is an optional refresher for participants in our Hacktoberfest mini-hackathon. It covers how to make contributions to repositories that you don't own via forks and pull requests. Sept 20, 2022 Introduction to Git & GitHub 2 hours This workshop introduces the Git version control system and how to use GitHub for collaboration. It covers git clone , git add , git commit , git push , git pull , git checkout , git branch , git status , git restore , git revert , pull requests, and code review. recording","title":"Arcadia Workshops"},{"location":"workshops/20220920-intro-to-git-and-github/lesson/","text":"Hands on introduction to Git and GitHub Version control 'Piled Higher and Deeper' by Jorge Cham www.phdcomics.com Version control keeps track of changes to a file over time. While the document above is technically under version control, using file names can be chaotic and unreliable especially when files are passed between multiple people working on different computers. Figure 6 in DOI 10.1093/gigascience/giaa140 www.academic.oup.com/gigascience Version control systems work by tracking incremental differences -- delineated by commits -- in files and keeping a history of those changes. This allows you to return to return to previous versions at any time. The file name doesn't change and only one version of a file is displayed at a time, but the full history is accessible. This is similar to the version control that occurs in programs like Google Docs or Microsoft Word. Version control with Git and GitHub Git is a free , distributed version control system. The complete history of your project is safely stored in a hidden .git repository. A repository is like a project folder or directory on your computer. It contains all of the files and folders for a project as well as the history of those files. Changes to files are recorded as commits . A commit is marked by the user when they're ready to check in a set of changes to a file. You can always go back to a previous commit. GitHub is an internet service for hosting files that are under git version control. You can think of it like social media for code and small text files. GitHub houses repositories in a central locations. Repositories on GitHub are referred to as remote repositories. Users can create a local copy of a repository by cloning (downloading) it to their computer, or to a computer in the cloud that they have access to. You don't need GitHub to use the Git version control system. By using the Git command line interface, you can use version control locally without ever pushing your files to a remote repository on GitHub. You also don't need to know the Git command line interface to take advantage of the version control system: Git is well-integrated into GitHub, so if you know the basic terminology and definitions, you can still version control your project through GitHub alone. However, both Git and GitHub are most powerful when combined together. This workshop will teach you the basics of Git and GitHub so you can leverage both in your research. Setting up Signing up for a GitHub account You will need a GitHub account for today's workshop. If you already have one, you can use your current username (e.g. there is no need to create an Arcadia-specific GitHub username). If you don't, sign up at github.com . Usernames are public so choose accordingly. Accessing a Unix shell & Git For this lesson, we need to have access to a Unix shell. If you're not sure how to open a terminal on your computer, see these instructions . Many computers come with Git installed. Once you have a Unix shell open, run the following command to see if git is installed: which git You should see something like: /usr/bin/git If you don't, you'll need to install git . If Git is not already installed on your computer, see these instructions . First time setup and configuration Whenever you use Git for the first time on a computer, there are a few things you need to set up. Unless you need to change something, these commands only need to be used the first time you use Git on a computer. First, set your name and email: git config --global user.name \"My Name\" git config --global user.email \"myemail@gmail.com\" Your email and user name is recorded with every commit. This helps ensure integrity and authenticity of the history. Most people keep their email public, but if you are concerned about privacy, check GitHub's tips to hide your email . Next, we'll set up a key pair. GitHub recently discontinued password authentication . Cryptographic keys are a convenient and secure way to authenticate without having to use passwords and are an authentication method still supported by GitHub. They consist of a pair of files called the public and private keys: the public part can be shared with whoever you'd like to authenticate with (in our case, GitHub), and the private part is kept \"secret\" on your machine. The private key should never be shared in an unencrypted way with anyone -- this includes over email, slack, etc. If your private key is accidentally shared, you should stop using it for authentication and create a new key pair. Things that are encrypted with the public key can be be decrypted with the private key, but it is computationally intractable (ie, it would take on the order of thousands of years) to determine a private key from a public key. You can read more about it here . cd ~ mkdir -p .ssh cd .ssh ssh-keygen The ssh-keygen program will prompt you to input a key file name (below, we use 20220920-github-workshop ) and a passphrase. It's ok to leave the passphrase blank; if you put in a passphrase, you'll need to remember it and type it every time you use the key file. ssh-keygen Generating public/private rsa key pair. Enter file in which to save the key (/home/jovyan/.ssh/id_rsa): 20220920-github-workshop Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in 20220920-github-workshop. Your public key has been saved in 20220920-github-workshop.pub. The key fingerprint is: SHA265: The key's randomart image is: Next, we'll check the key files permissions. Permissions control who has access to a file on your computer, and key files need to have very restricted permissions. ls -lah We see: total 16K drwxr-xr-x 2 jovyan jovyan 4.0K Sep 9 20:21 . drwxr-xr-x 1 jovyan jovyan 4.0K Sep 9 20:20 .. -rw------- 1 jovyan jovyan 1.7K Sep 9 20:21 20220920-github-workshop -rw-r--r-- 1 jovyan jovyan 445 Sep 9 20:21 20220920-github-workshop.pub We are the only user who has read access to our private key file, so our permissions are fine. When we ran ls , we saw that the ssh-keygen program generated two files. The first file 20220920-github-workshop is the private key file and should never be shared. The second file 20220920-github-workshop.pub is the public key file that we'll upload to GitHub. We can tell it's the public key file because it ends in .pub . Next, we need to get our public key file uploaded to GitHub so we can use it for authentication. GitHub will need the text in the public key file. You can view it by running cat : cat 20220920-github-workshop.pub Your public key file text should look something like this: ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCtrKIjBDjfAt3sIfHOPKEE/RkcuPAfdl0xO7M+CBQNWuYqST2bW20yRFu4lpCyNuz7uG12DgIVMmLMdlfGlGjJpj/B/f3FUw6XxIaAzQIfYtg+Q+qJ0M2GSaooeEoKi9lgiJsR69fwAoVPgI0kyyA4253F/SLVD/QpWMQgcN5m43tPztc9vp1Lt5u8PZmZJUBMyMolOgtvRUYDKx7MRb9nWO/Rmzeibj96hLEm8GHiERRGDpHK1BOryiq2jI9C2+o3ujj+SWCeqRVTdBp7raSzhwPWCsLpNRX1MNQ9t+807eDV8pUDnJ6gfHZndcQ23k+OMwhTdhPk74drz2k5X+/h jovyan@jupyter-arcadia-2dscience-2dtional-2dtraining-2dgi36ozk6 To upload the key file text, navigate to GitHub and click on settings. Then use the menu on the left hand side of the page to navigate to the SSH and GPG keys tab. Once there, select New SSH key . Give your key a descriptive name (such as 20220920-github-workshop ) and then paste in the contents of your public key file to text editor. The very last thing we need to do is tell our computers which key file to use when we want to authenticate with GitHub. We do this by creating a config file in our .ssh directory. We'll use nano to do this. Type the following contents into your config file and save it as config . nano config Host github.com User git HostName ssh.github.com IdentityFile ~/.ssh/20220920-github-workshop Follow the command prompts in the bottom ribbon of nano to exit. To confirm that you have set up authentication properly, run the following command: ssh -i ~/.ssh/20220920-github-workshop -T git@github.com First, you'll see a message like: The authenticity of host 'ssh.github.com (192.30.255.122)' can't be established. ECDSA key fingerprint is SHA256:p2QAMXNIC1TJYWeIOttrVc98/R1BUFWu3/LiyKgUfQM. Are you sure you want to continue connecting (yes/no/[fingerprint])? Type yes and hit enter. Then, you should see: Hi username! You've successfully authenticated, but GitHub does not provide shell access. This may be all of the set up you need to do. However, if you continue to have issues, you may need to load your key into the SSH agent with: ssh-add ~/.ssh/20220920-github-workshop Getting started with version control Now that we've established a secure connection between our computers and GitHub, it's time to learn how to start a version controlled repository and add files to it. We'll start by creating a repository on GitHub. Creating your first repository on GitHub Follow the steps below to create your first repository. Navigate to GitHub . Click on the plus sign in the upper right of the screen. Select \"New repository\" from the drop-down menu. On the \"Create a new repository\" page, choose a name for your repository. For this workshop, name the repository 2022-git-workshop . A repository can have any valid directory name, but putting the year at the beginning is a good practice because it makes it clear when the repo was created. Check the box \"Add a README file\". Click the green \"Create repository\" button at the bottom of the page. This will create a new repository and take you to the repository's landing page on GitHub. Cloning a repository from remote to local Cloning is the process of copying an existing Git repository from a remote location (here, on GitHub) to your local computer (here, on our binder instances). To clone the repository, click the green \"Code\" button in the top right hand corner of the repository screen. This creates a drop down menu with clone options. We'll select the SSH tab because we configured an ssh key pair. Once you select the tab, copy the path that starts with git@gitub.com: . Then, navigate to your terminal and use the command below to clone the repository. Remember to substitute out the username/URL with your own URL that we copied. cd ~ git clone git@github.com:your_username/2022-git-workshop cd 2022-git-workshop The basic Git workflow Figure by Allison Horst. www.twitter.com/allison_horst We can now make changes to our files in our local repository. The basic Git workflow begins when you communicate to Git about changes you've made to your files. Once you like a set of changes you\u2019ve made you can tell Git about them by using the command git add . This stages the changes you have made. Staging a file tells Git that you're ready to commit those files -- it's a way of telling Git which files you're ready to commit. Next, you bake them into the branch using git commit . When you\u2019re ready, you can communicate those changes back to GitHub using git push . This will push your the changes that are in your local repository up to the remote repository. Let's try this workflow out. Throughout this process, we'll use the command git status to track our progress through the workflow. git status displays the state of a working direcotry and the staging area. git status We'll use the echo command to create a new file, notes.txt . ls echo \"some interesting notes\" > notes.txt ls Take a look at the contents of your notes.txt file: less notes.txt And run git status to see how creating a new file changes the output of that command: git status Once you have made changes in your repository, you need to tell Git to start tracking the file. The command git add adds the files to the \"staging area\", meaning Git is now tracking the changes. git add notes.txt After adding this file, we see our output of git status changes. git status The text associated with our file is now green because the file is staged. When you've made all of the changes to a file that represent a unit of changes, you can use git commit to create a snapshot of the file in your Git version history. git commit -m \"start notes file\" git status The -m flag is the message that is associated with that commit. The message should be short and descriptive so that you or someone looking at your code could quickly determine what changes took place from one commit to the next. What constitutes a unit of changes worthy of running git commit ? That depends on the project and the person, but think about returning to this code six months in the future. What set of changes would make it most easy to return to an earlier version of document? Committing a file bakes changes into your local repository. To communicate that changes back up to your remote repository, use git push . git push Challenge : Add today's date to the README.md text file, stage the changes in those files, commit them to version history, and push them up to your remote repository. Challenge solution You can make changes to your README.md file however you choose -- using a text editor on your system (TextEdit, BBEdit, VSCode, Sublime), using a text editor in your terminal, or by using a shell redirect. This solution demonstrates how to add text using echo and redirects >> . echo \"20220920\" >> README.md git add README.md git commit -m \"add date to readme\" git push Working on branches When you want to make changes to files in a repository, it\u2019s best practice to do it in a branch . A branch is an independent line of development of files in a repository. You can think of it like making a copy of a document and making changes to that copy only: the original stays the same, but the copy diverges with new content. In a branch, all files are copied and can be changed. Branches are particularly powerful for collaborating. They allow multiple people to work on the same code base and seamless integrate changes. By default, you start on the main branch. You can see which branch you're on locally using the git branch command. git branch Let's create a branch and make changes to it. We can create a new local branch using git checkout -b . git checkout tells git we want to switch which branch we're currently on, while the -b flag tells git that we're creating a new branch. git checkout -b my-first-branch Now, if we run git branch we'll see we're on the branch my-first-branch . git branch To go back to the main branch, we can use the git checkout command without the flag -b . git checkout main Branch names can be used to convey information that can help you and others keep track of what you're doing. For example, you can prepend your branch names with your initials and a slash to make it clear that you were the one who created the branch, and you can specify a name that is associated with the file changes made on the branch: <your initials>/<brief description of the code change> . Example: ter/git-workshop . Let's practice making a branch using these conventions. In our branch, we'll update our README.md file. Use the code below to create a new branch, but remember to substitute out your-initials for your initials. git checkout -b your-initials/update-readme. Whenever you create a new branch, it branches off from the branch you're currently in when you make the new branch. In this case, we started in main , so our branch will branch off of from here. Next, let's add some changes to our README.md file and run git status . echo \"adding more text to the README file\" >> README.md git status Stage the changes with git add : git add README.md And then create a new commit with git commit : git commit -m \"update readme w more text\" Run git push : git push This gives us an error message! fatal: The current branch tmp has no upstream branch. To push the current branch and set the remote as upstream, use git push --set-upstream origin ter/update-readme While we created our branch locally, we haven't created the same branch remotely. We need to tell git where to put our changes in the remote repository. Luckily, the error message points us to the code we need to run to set the remote branch. git push --set-upstream origin ter/update-readme In the above command, origin is shorthand for the remote repository that a project was cloned from. To view what this value is, you can run git remote -v . If we navigate to our repositories on GitHub, we now see a yellow banner wtih our branch name and a green button inviting us to \"Compare & pull request\". Integrating changes into main using pull requests We just created changes to our README.md file in a branch. To integrate this changes formally back into the main branch , you can open a pull request . Pull requests create a line-by-line comparison between the original code and the new code and generate an interface for inline and overall comments and feedback. Pull requests are how changes are reviewed before they\u2019re integrated back into the main branch. To open a pull request, click the green button \"Compare & pull request\". This will take you to a page where you can draft your pull request. You should update the title of your pull request to reflect the changes you made in your branch in plain language. Then you should add a few sentence description of those changes. Once you've done that, click the \"Create pull request\" button. The pull request interface has a lot of features that make collaboration and code review straight forward. The main pull request page supports many important features: Each commit and the descriptive message your provided for them are displayed. Conversations about the file changes can take place via comments. You can convert your pull request to a draft if you're not done adding to it yet, or request review from other GitHub users. The main pull request page also has a tab for \"Files changed\". This provides a succint view of every line changed in every file, showing you the new content in green and the deleted content in red. The \"Files changed\" interface is the most useful interface for code review. Code review is when someone who didn't write the code but who has domain expertise relevant to the code reads and reviews the code being added or removed by a pull request. Code review a) encourages you to write better documentation and cleaner code as you go because you know someone will be reading it soon, b) gives a collaborator an opportunity to comment on and improve your work, and c) increases accountability and visibility of your work. There is no one-size-fits-all approach for code review, but these are some things to keep an eye out for when you're reviewing someone's code (modified from here ): Bugs/Potential bugs Repetitive code Code saying one thing, documentation saying another Off-by-one errors Making sure each function does one thing only Lack of tests and sanity checks for what different parts are doing Magic numbers (a number hardcoded in the script) Unclear, messy code Bad variable/method names Inconsistent indentation The order of the different steps Too much on one line Lack of comments and signposting Fragile and non-reusable code Software or software versions not documented Tailor-made and manual steps Only works with the given data For more on code review, see this lesson . From the \"Files changed\" tab, you can see all of the changes suggested in a pull request. You can use in-line comments and suggestions or holistic comments on changes. When you're finished with your review, you can submit it as a set of comments, approved changes, or requesting further changes. Once a pull request is approved , the changes are merged into the main branch. Challenge Open a pull request with the changes you made to your README.md . Request a review from the person sitting next to you. After your PR has been reviewed, merge the changes into your main branch. Pulling changes from the remote repository back to the local repostory After we merged our pull requests, our main branches in our remote repositories on GitHub now contain content that our To get the merged changes back to your local branch, you can run git pull in your local repository. git pull This will pull in all of the changes that are on GitHub but not in our local repo. Challenge Checkout the main branch and run git pull . What happens? Challenge solution git checkout main git pull You should see a message that ends with: Already up to date. This happens because we already pulled in all of the remote changes to our local repository, so there is nothing left to do. Now that all of our changes to our README.md are in our local and remote branch, we can safely delete our branch. git branch git branch -d ter/update-readme While you don't have to delete a branch when you're finished with it, deleting branches you're done with helps keep your project tidy and helps yourself and others not get confused when you revisit work in the future. Bringing the whole process together Figure by Allison Horst. www.twitter.com/allison_horst This figure gives an overview of the entire Git & GitHub ecosystem. It provides a succinct review of the concepts we've covered today. GitHub has integrated enough features that many of these steps can be orchestrated completely on GitHub without needing to clone a repository to your local machine. However, this mental model is still helpful: you can create a branch, make edits to a text file and commit them, open a pull request, and merge the pull request all from the GitHub online interface. You do not need to learn the Git CLI to experience the joys and benefits of GitHub and to contribute to projects that live there. Undoing Changes Confusingly, git has four different subcommands for undoing things: restore revert reset checkout We'll cover these commands below. Restoring a file to its state at the previous commit The git restore command is useful if you want to undo changes you haven't committed yet. Open up the README.md file in a text editor and make some changes. For instance, let's add the line This is a mistake to the end of the file. Then save and close the file. Next, check the repository status: git status As expected, git tells us there are changes to the README.md file: On branch main Changes not staged for commit: (use \"git add <file>...\" to update what will be committed) (use \"git restore <file>...\" to discard changes in working directory) modified: README.md no changes added to commit (use \"git add\" and/or \"git commit -a\") What if now you decide the changes were a mistake and you don't want to keep them? The git restore command restores a file in the working directory to the version of the file in the most recent commit. Let's try out restoring the README.md file: git restore README.md Now check the status again: git status The status shows that there are no longer any new changes to the README.md file: On branch main nothing to commit, working tree clean There is no way to undo git restore , so be careful when you're using it. In older versions of git, the command to restore a file in the working directory was git checkout -- FILENAME . You may occasionally see people mention this command online, and it still works in recent versions of git. However, the git restore command is preferable because it disambiguates what you're trying to do ( git checkout is also used for other things unrelated to restoring files). Restoring the staging area With a different set of arguments, you can also use the git restore command to remove changes from the staging area. Open up the README.md file in a text editor again and make some more changes. This time let's add the line Git is tough to the end of the file. As usual, save and close the file. Next, add the changes to the staging area: git add README.md Then check the repository status with git status : Changes to be committed: (use \"git restore --staged <file>...\" to unstage) modified: README.md git tells us our changes to README.md are staged. Now suppose you decide you want to unstage the changes to README.md . This removes the changes from the staging area, but not from the working directory. Unstaging changes is especially useful when you're working with multiple files and accidentally add a file you don't want to commit yet. To unstage a file, use git restore with the --staged argument: git restore --staged README.md Then check the status again: Changes not staged for commit: (use \"git add <file>...\" to update what will be committed) (use \"git restore <file>...\" to discard changes in working directory) modified: README.md no changes added to commit (use \"git add\" and/or \"git commit -a\") The changes to README.md are still in the working directory, but no longer in the staging area. In older versions of git, the command to restore a file in the staging area was git reset FILENAME . You may occasionally see people mention this command online, and it still works in recent versions of git. However, the git restore command is preferable because it disambiguates what you're trying to do ( git reset is also used for other things unrelated to restoring the staging area). Reverting a commit The git revert command is useful if you want to undo changes that you've already committed. Once a commit is in your repository's history, it generally stays there forever. There is a way to delete commits, but you should really only delete commits if you accidentally commit sensitive data . Instead, the idiomatic way to undo a commit is to create a new commit that reverses the changes. Let's start by editing the README.md file again and making a commit. Add the line This is a big mistake. to the end of the file. Then run: git add README.md git commit -m \"This commit is a mistake.\" Now suppose you decide the commit is a mistake, and want to undo it. First, you need to find the ID of the commit you want to undo. The git log command opens a scrolling list of all commits in the repository's history and their IDs. Run the command: git log You can exit the log by typing q . In the log, locate the the mistaken commit, and copy or remember the first 5 digits of its ID. Git is smart enough that it can generally recognize a commit from the first few digits of its ID, and will tell you if it needs more digits for disambiguation. In my repo, the ID of the commit starts with e01d1 . In your repo, the commit will likely have a different ID. Next, run this command, replacing the ID with the ID you copied: git revert e01d1 --no-edit The --no-edit flag tells git revert to generate the commit message for the new commit automatically. Without the flag, git revert will prompt you to enter a commit message. Now inspect the file with nano or cat . You should see that the changes from your bad commit are gone. If you look in the log with git log , you'll also see a new commit to revert the changes of a previous commit. Challenge : The goal of this challenge is to make a bad commit and then revert it. Work through these steps: Change a file in your repository. add and commit the changes. Find the ID of the commit from step 2 in the repository history. revert the commit from step 2. Challenge solution Change a file like `README.md`. Then run: git add README.md git commit -m \"update README\" To find the ID of the commit, run: git log Then revert the commit: git revert 07f61 --no-edit Creating a branch from a previous commit Another strategy to revert to a prior state is to create a new branch that starts at the state of a previous commit. This has the advantage of keeping a branch with the work you did on it, but allows you to continue to work in a different direction from a previous commit. To do this, you can use git log like above to identify which commit you want your new branch to start out at. Once you have the commit ID, run the following: git checkout -b name-of-new-branch e01d1 You'll now be on a new branch that starts from the commit you indicated. To check, run git branch . GitHub Goodies Issues GitHub issues started as a way to record problems with software and have since evolved into generalized project planning tools . Issues are a great way to keep track of to do items, have asynchronous conversations relevant to a repository, or otherwise deposit information relevant to a repository. Releases Releases bundle and deliver iterations of your project. When your entire repository reaches a consensus point, you can use GitHub to compress that version of files together. Releases integrate with zenodo to archive your repo and give it a DOI. .gitignore A .gitignore file is a hidden text file in you repository that you can use to \"ignore\" files that you don't want under version control. This is handy for really large data files that you don't want to add to your repo. To use a .gitignore file, create a .gitignore text file in the main directory of your repo and add paths to files that you would like to ignore. The .gitignore file accepts regular expressions (e.g. *.fastq.gz to ignore any file that ends in .fastq.gz ). Links to other goodies Archive a repository that is no long in use Continuous integration with GitHub Actions Summary Terms introduced: Term Definition Git a version control system GitHub an internet hosting service for version control using Git repository a collection of files and folders associated with a project that is under Git version control local accessed from your system; the computer you're working on remote stored on a remote computer; GitHub commit a snapshot of your files and folders at a given time branch an independent line of development in a repository pull request a mechanism to suggest changes from a branch be integrated back into the main branch review the process of reviewing changes in a branch through a pull request issue a tracking tool integrated into a GitHub repository Commands introduced: Command Function git clone copies a Git repository from remote to local git add adds new or changed files in your working directory to the Git staging area git commit creates a commit of your repository git push uploads all local branch commits to the corresponding remote branch git checkout switches between local branches, or creates a new branch with the -b flag git branch reports what branch you're currently on and what other local branches exist git pull updates current local working branch and all of the remote tracking branches git restore restores a file to its state at the last commit or remove a file from the staging area git revert reverts changes that have already been committed","title":"Hands on introduction to Git and GitHub"},{"location":"workshops/20220920-intro-to-git-and-github/lesson/#hands-on-introduction-to-git-and-github","text":"","title":"Hands on introduction to Git and GitHub"},{"location":"workshops/20220920-intro-to-git-and-github/lesson/#version-control","text":"'Piled Higher and Deeper' by Jorge Cham www.phdcomics.com Version control keeps track of changes to a file over time. While the document above is technically under version control, using file names can be chaotic and unreliable especially when files are passed between multiple people working on different computers. Figure 6 in DOI 10.1093/gigascience/giaa140 www.academic.oup.com/gigascience Version control systems work by tracking incremental differences -- delineated by commits -- in files and keeping a history of those changes. This allows you to return to return to previous versions at any time. The file name doesn't change and only one version of a file is displayed at a time, but the full history is accessible. This is similar to the version control that occurs in programs like Google Docs or Microsoft Word.","title":"Version control"},{"location":"workshops/20220920-intro-to-git-and-github/lesson/#version-control-with-git-and-github","text":"Git is a free , distributed version control system. The complete history of your project is safely stored in a hidden .git repository. A repository is like a project folder or directory on your computer. It contains all of the files and folders for a project as well as the history of those files. Changes to files are recorded as commits . A commit is marked by the user when they're ready to check in a set of changes to a file. You can always go back to a previous commit. GitHub is an internet service for hosting files that are under git version control. You can think of it like social media for code and small text files. GitHub houses repositories in a central locations. Repositories on GitHub are referred to as remote repositories. Users can create a local copy of a repository by cloning (downloading) it to their computer, or to a computer in the cloud that they have access to. You don't need GitHub to use the Git version control system. By using the Git command line interface, you can use version control locally without ever pushing your files to a remote repository on GitHub. You also don't need to know the Git command line interface to take advantage of the version control system: Git is well-integrated into GitHub, so if you know the basic terminology and definitions, you can still version control your project through GitHub alone. However, both Git and GitHub are most powerful when combined together. This workshop will teach you the basics of Git and GitHub so you can leverage both in your research.","title":"Version control with Git and GitHub"},{"location":"workshops/20220920-intro-to-git-and-github/lesson/#setting-up","text":"","title":"Setting up"},{"location":"workshops/20220920-intro-to-git-and-github/lesson/#signing-up-for-a-github-account","text":"You will need a GitHub account for today's workshop. If you already have one, you can use your current username (e.g. there is no need to create an Arcadia-specific GitHub username). If you don't, sign up at github.com . Usernames are public so choose accordingly.","title":"Signing up for a GitHub account"},{"location":"workshops/20220920-intro-to-git-and-github/lesson/#accessing-a-unix-shell-git","text":"For this lesson, we need to have access to a Unix shell. If you're not sure how to open a terminal on your computer, see these instructions . Many computers come with Git installed. Once you have a Unix shell open, run the following command to see if git is installed: which git You should see something like: /usr/bin/git If you don't, you'll need to install git . If Git is not already installed on your computer, see these instructions .","title":"Accessing a Unix shell &amp; Git"},{"location":"workshops/20220920-intro-to-git-and-github/lesson/#first-time-setup-and-configuration","text":"Whenever you use Git for the first time on a computer, there are a few things you need to set up. Unless you need to change something, these commands only need to be used the first time you use Git on a computer. First, set your name and email: git config --global user.name \"My Name\" git config --global user.email \"myemail@gmail.com\" Your email and user name is recorded with every commit. This helps ensure integrity and authenticity of the history. Most people keep their email public, but if you are concerned about privacy, check GitHub's tips to hide your email . Next, we'll set up a key pair. GitHub recently discontinued password authentication . Cryptographic keys are a convenient and secure way to authenticate without having to use passwords and are an authentication method still supported by GitHub. They consist of a pair of files called the public and private keys: the public part can be shared with whoever you'd like to authenticate with (in our case, GitHub), and the private part is kept \"secret\" on your machine. The private key should never be shared in an unencrypted way with anyone -- this includes over email, slack, etc. If your private key is accidentally shared, you should stop using it for authentication and create a new key pair. Things that are encrypted with the public key can be be decrypted with the private key, but it is computationally intractable (ie, it would take on the order of thousands of years) to determine a private key from a public key. You can read more about it here . cd ~ mkdir -p .ssh cd .ssh ssh-keygen The ssh-keygen program will prompt you to input a key file name (below, we use 20220920-github-workshop ) and a passphrase. It's ok to leave the passphrase blank; if you put in a passphrase, you'll need to remember it and type it every time you use the key file. ssh-keygen Generating public/private rsa key pair. Enter file in which to save the key (/home/jovyan/.ssh/id_rsa): 20220920-github-workshop Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in 20220920-github-workshop. Your public key has been saved in 20220920-github-workshop.pub. The key fingerprint is: SHA265: The key's randomart image is: Next, we'll check the key files permissions. Permissions control who has access to a file on your computer, and key files need to have very restricted permissions. ls -lah We see: total 16K drwxr-xr-x 2 jovyan jovyan 4.0K Sep 9 20:21 . drwxr-xr-x 1 jovyan jovyan 4.0K Sep 9 20:20 .. -rw------- 1 jovyan jovyan 1.7K Sep 9 20:21 20220920-github-workshop -rw-r--r-- 1 jovyan jovyan 445 Sep 9 20:21 20220920-github-workshop.pub We are the only user who has read access to our private key file, so our permissions are fine. When we ran ls , we saw that the ssh-keygen program generated two files. The first file 20220920-github-workshop is the private key file and should never be shared. The second file 20220920-github-workshop.pub is the public key file that we'll upload to GitHub. We can tell it's the public key file because it ends in .pub . Next, we need to get our public key file uploaded to GitHub so we can use it for authentication. GitHub will need the text in the public key file. You can view it by running cat : cat 20220920-github-workshop.pub Your public key file text should look something like this: ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCtrKIjBDjfAt3sIfHOPKEE/RkcuPAfdl0xO7M+CBQNWuYqST2bW20yRFu4lpCyNuz7uG12DgIVMmLMdlfGlGjJpj/B/f3FUw6XxIaAzQIfYtg+Q+qJ0M2GSaooeEoKi9lgiJsR69fwAoVPgI0kyyA4253F/SLVD/QpWMQgcN5m43tPztc9vp1Lt5u8PZmZJUBMyMolOgtvRUYDKx7MRb9nWO/Rmzeibj96hLEm8GHiERRGDpHK1BOryiq2jI9C2+o3ujj+SWCeqRVTdBp7raSzhwPWCsLpNRX1MNQ9t+807eDV8pUDnJ6gfHZndcQ23k+OMwhTdhPk74drz2k5X+/h jovyan@jupyter-arcadia-2dscience-2dtional-2dtraining-2dgi36ozk6 To upload the key file text, navigate to GitHub and click on settings. Then use the menu on the left hand side of the page to navigate to the SSH and GPG keys tab. Once there, select New SSH key . Give your key a descriptive name (such as 20220920-github-workshop ) and then paste in the contents of your public key file to text editor. The very last thing we need to do is tell our computers which key file to use when we want to authenticate with GitHub. We do this by creating a config file in our .ssh directory. We'll use nano to do this. Type the following contents into your config file and save it as config . nano config Host github.com User git HostName ssh.github.com IdentityFile ~/.ssh/20220920-github-workshop Follow the command prompts in the bottom ribbon of nano to exit. To confirm that you have set up authentication properly, run the following command: ssh -i ~/.ssh/20220920-github-workshop -T git@github.com First, you'll see a message like: The authenticity of host 'ssh.github.com (192.30.255.122)' can't be established. ECDSA key fingerprint is SHA256:p2QAMXNIC1TJYWeIOttrVc98/R1BUFWu3/LiyKgUfQM. Are you sure you want to continue connecting (yes/no/[fingerprint])? Type yes and hit enter. Then, you should see: Hi username! You've successfully authenticated, but GitHub does not provide shell access. This may be all of the set up you need to do. However, if you continue to have issues, you may need to load your key into the SSH agent with: ssh-add ~/.ssh/20220920-github-workshop","title":"First time setup and configuration"},{"location":"workshops/20220920-intro-to-git-and-github/lesson/#getting-started-with-version-control","text":"Now that we've established a secure connection between our computers and GitHub, it's time to learn how to start a version controlled repository and add files to it. We'll start by creating a repository on GitHub.","title":"Getting started with version control"},{"location":"workshops/20220920-intro-to-git-and-github/lesson/#creating-your-first-repository-on-github","text":"Follow the steps below to create your first repository. Navigate to GitHub . Click on the plus sign in the upper right of the screen. Select \"New repository\" from the drop-down menu. On the \"Create a new repository\" page, choose a name for your repository. For this workshop, name the repository 2022-git-workshop . A repository can have any valid directory name, but putting the year at the beginning is a good practice because it makes it clear when the repo was created. Check the box \"Add a README file\". Click the green \"Create repository\" button at the bottom of the page. This will create a new repository and take you to the repository's landing page on GitHub.","title":"Creating your first repository on GitHub"},{"location":"workshops/20220920-intro-to-git-and-github/lesson/#cloning-a-repository-from-remote-to-local","text":"Cloning is the process of copying an existing Git repository from a remote location (here, on GitHub) to your local computer (here, on our binder instances). To clone the repository, click the green \"Code\" button in the top right hand corner of the repository screen. This creates a drop down menu with clone options. We'll select the SSH tab because we configured an ssh key pair. Once you select the tab, copy the path that starts with git@gitub.com: . Then, navigate to your terminal and use the command below to clone the repository. Remember to substitute out the username/URL with your own URL that we copied. cd ~ git clone git@github.com:your_username/2022-git-workshop cd 2022-git-workshop","title":"Cloning a repository from remote to local"},{"location":"workshops/20220920-intro-to-git-and-github/lesson/#the-basic-git-workflow","text":"Figure by Allison Horst. www.twitter.com/allison_horst We can now make changes to our files in our local repository. The basic Git workflow begins when you communicate to Git about changes you've made to your files. Once you like a set of changes you\u2019ve made you can tell Git about them by using the command git add . This stages the changes you have made. Staging a file tells Git that you're ready to commit those files -- it's a way of telling Git which files you're ready to commit. Next, you bake them into the branch using git commit . When you\u2019re ready, you can communicate those changes back to GitHub using git push . This will push your the changes that are in your local repository up to the remote repository. Let's try this workflow out. Throughout this process, we'll use the command git status to track our progress through the workflow. git status displays the state of a working direcotry and the staging area. git status We'll use the echo command to create a new file, notes.txt . ls echo \"some interesting notes\" > notes.txt ls Take a look at the contents of your notes.txt file: less notes.txt And run git status to see how creating a new file changes the output of that command: git status Once you have made changes in your repository, you need to tell Git to start tracking the file. The command git add adds the files to the \"staging area\", meaning Git is now tracking the changes. git add notes.txt After adding this file, we see our output of git status changes. git status The text associated with our file is now green because the file is staged. When you've made all of the changes to a file that represent a unit of changes, you can use git commit to create a snapshot of the file in your Git version history. git commit -m \"start notes file\" git status The -m flag is the message that is associated with that commit. The message should be short and descriptive so that you or someone looking at your code could quickly determine what changes took place from one commit to the next. What constitutes a unit of changes worthy of running git commit ? That depends on the project and the person, but think about returning to this code six months in the future. What set of changes would make it most easy to return to an earlier version of document? Committing a file bakes changes into your local repository. To communicate that changes back up to your remote repository, use git push . git push Challenge : Add today's date to the README.md text file, stage the changes in those files, commit them to version history, and push them up to your remote repository. Challenge solution You can make changes to your README.md file however you choose -- using a text editor on your system (TextEdit, BBEdit, VSCode, Sublime), using a text editor in your terminal, or by using a shell redirect. This solution demonstrates how to add text using echo and redirects >> . echo \"20220920\" >> README.md git add README.md git commit -m \"add date to readme\" git push","title":"The basic Git workflow"},{"location":"workshops/20220920-intro-to-git-and-github/lesson/#working-on-branches","text":"When you want to make changes to files in a repository, it\u2019s best practice to do it in a branch . A branch is an independent line of development of files in a repository. You can think of it like making a copy of a document and making changes to that copy only: the original stays the same, but the copy diverges with new content. In a branch, all files are copied and can be changed. Branches are particularly powerful for collaborating. They allow multiple people to work on the same code base and seamless integrate changes. By default, you start on the main branch. You can see which branch you're on locally using the git branch command. git branch Let's create a branch and make changes to it. We can create a new local branch using git checkout -b . git checkout tells git we want to switch which branch we're currently on, while the -b flag tells git that we're creating a new branch. git checkout -b my-first-branch Now, if we run git branch we'll see we're on the branch my-first-branch . git branch To go back to the main branch, we can use the git checkout command without the flag -b . git checkout main Branch names can be used to convey information that can help you and others keep track of what you're doing. For example, you can prepend your branch names with your initials and a slash to make it clear that you were the one who created the branch, and you can specify a name that is associated with the file changes made on the branch: <your initials>/<brief description of the code change> . Example: ter/git-workshop . Let's practice making a branch using these conventions. In our branch, we'll update our README.md file. Use the code below to create a new branch, but remember to substitute out your-initials for your initials. git checkout -b your-initials/update-readme. Whenever you create a new branch, it branches off from the branch you're currently in when you make the new branch. In this case, we started in main , so our branch will branch off of from here. Next, let's add some changes to our README.md file and run git status . echo \"adding more text to the README file\" >> README.md git status Stage the changes with git add : git add README.md And then create a new commit with git commit : git commit -m \"update readme w more text\" Run git push : git push This gives us an error message! fatal: The current branch tmp has no upstream branch. To push the current branch and set the remote as upstream, use git push --set-upstream origin ter/update-readme While we created our branch locally, we haven't created the same branch remotely. We need to tell git where to put our changes in the remote repository. Luckily, the error message points us to the code we need to run to set the remote branch. git push --set-upstream origin ter/update-readme In the above command, origin is shorthand for the remote repository that a project was cloned from. To view what this value is, you can run git remote -v . If we navigate to our repositories on GitHub, we now see a yellow banner wtih our branch name and a green button inviting us to \"Compare & pull request\".","title":"Working on branches"},{"location":"workshops/20220920-intro-to-git-and-github/lesson/#integrating-changes-into-main-using-pull-requests","text":"We just created changes to our README.md file in a branch. To integrate this changes formally back into the main branch , you can open a pull request . Pull requests create a line-by-line comparison between the original code and the new code and generate an interface for inline and overall comments and feedback. Pull requests are how changes are reviewed before they\u2019re integrated back into the main branch. To open a pull request, click the green button \"Compare & pull request\". This will take you to a page where you can draft your pull request. You should update the title of your pull request to reflect the changes you made in your branch in plain language. Then you should add a few sentence description of those changes. Once you've done that, click the \"Create pull request\" button. The pull request interface has a lot of features that make collaboration and code review straight forward. The main pull request page supports many important features: Each commit and the descriptive message your provided for them are displayed. Conversations about the file changes can take place via comments. You can convert your pull request to a draft if you're not done adding to it yet, or request review from other GitHub users. The main pull request page also has a tab for \"Files changed\". This provides a succint view of every line changed in every file, showing you the new content in green and the deleted content in red. The \"Files changed\" interface is the most useful interface for code review. Code review is when someone who didn't write the code but who has domain expertise relevant to the code reads and reviews the code being added or removed by a pull request. Code review a) encourages you to write better documentation and cleaner code as you go because you know someone will be reading it soon, b) gives a collaborator an opportunity to comment on and improve your work, and c) increases accountability and visibility of your work. There is no one-size-fits-all approach for code review, but these are some things to keep an eye out for when you're reviewing someone's code (modified from here ): Bugs/Potential bugs Repetitive code Code saying one thing, documentation saying another Off-by-one errors Making sure each function does one thing only Lack of tests and sanity checks for what different parts are doing Magic numbers (a number hardcoded in the script) Unclear, messy code Bad variable/method names Inconsistent indentation The order of the different steps Too much on one line Lack of comments and signposting Fragile and non-reusable code Software or software versions not documented Tailor-made and manual steps Only works with the given data For more on code review, see this lesson . From the \"Files changed\" tab, you can see all of the changes suggested in a pull request. You can use in-line comments and suggestions or holistic comments on changes. When you're finished with your review, you can submit it as a set of comments, approved changes, or requesting further changes. Once a pull request is approved , the changes are merged into the main branch. Challenge Open a pull request with the changes you made to your README.md . Request a review from the person sitting next to you. After your PR has been reviewed, merge the changes into your main branch.","title":"Integrating changes into main using pull requests"},{"location":"workshops/20220920-intro-to-git-and-github/lesson/#pulling-changes-from-the-remote-repository-back-to-the-local-repostory","text":"After we merged our pull requests, our main branches in our remote repositories on GitHub now contain content that our To get the merged changes back to your local branch, you can run git pull in your local repository. git pull This will pull in all of the changes that are on GitHub but not in our local repo. Challenge Checkout the main branch and run git pull . What happens? Challenge solution git checkout main git pull You should see a message that ends with: Already up to date. This happens because we already pulled in all of the remote changes to our local repository, so there is nothing left to do. Now that all of our changes to our README.md are in our local and remote branch, we can safely delete our branch. git branch git branch -d ter/update-readme While you don't have to delete a branch when you're finished with it, deleting branches you're done with helps keep your project tidy and helps yourself and others not get confused when you revisit work in the future.","title":"Pulling changes from the remote repository back to the local repostory"},{"location":"workshops/20220920-intro-to-git-and-github/lesson/#bringing-the-whole-process-together","text":"Figure by Allison Horst. www.twitter.com/allison_horst This figure gives an overview of the entire Git & GitHub ecosystem. It provides a succinct review of the concepts we've covered today. GitHub has integrated enough features that many of these steps can be orchestrated completely on GitHub without needing to clone a repository to your local machine. However, this mental model is still helpful: you can create a branch, make edits to a text file and commit them, open a pull request, and merge the pull request all from the GitHub online interface. You do not need to learn the Git CLI to experience the joys and benefits of GitHub and to contribute to projects that live there.","title":"Bringing the whole process together"},{"location":"workshops/20220920-intro-to-git-and-github/lesson/#undoing-changes","text":"Confusingly, git has four different subcommands for undoing things: restore revert reset checkout We'll cover these commands below.","title":"Undoing Changes"},{"location":"workshops/20220920-intro-to-git-and-github/lesson/#restoring-a-file-to-its-state-at-the-previous-commit","text":"The git restore command is useful if you want to undo changes you haven't committed yet. Open up the README.md file in a text editor and make some changes. For instance, let's add the line This is a mistake to the end of the file. Then save and close the file. Next, check the repository status: git status As expected, git tells us there are changes to the README.md file: On branch main Changes not staged for commit: (use \"git add <file>...\" to update what will be committed) (use \"git restore <file>...\" to discard changes in working directory) modified: README.md no changes added to commit (use \"git add\" and/or \"git commit -a\") What if now you decide the changes were a mistake and you don't want to keep them? The git restore command restores a file in the working directory to the version of the file in the most recent commit. Let's try out restoring the README.md file: git restore README.md Now check the status again: git status The status shows that there are no longer any new changes to the README.md file: On branch main nothing to commit, working tree clean There is no way to undo git restore , so be careful when you're using it. In older versions of git, the command to restore a file in the working directory was git checkout -- FILENAME . You may occasionally see people mention this command online, and it still works in recent versions of git. However, the git restore command is preferable because it disambiguates what you're trying to do ( git checkout is also used for other things unrelated to restoring files).","title":"Restoring a file to its state at the previous commit"},{"location":"workshops/20220920-intro-to-git-and-github/lesson/#restoring-the-staging-area","text":"With a different set of arguments, you can also use the git restore command to remove changes from the staging area. Open up the README.md file in a text editor again and make some more changes. This time let's add the line Git is tough to the end of the file. As usual, save and close the file. Next, add the changes to the staging area: git add README.md Then check the repository status with git status : Changes to be committed: (use \"git restore --staged <file>...\" to unstage) modified: README.md git tells us our changes to README.md are staged. Now suppose you decide you want to unstage the changes to README.md . This removes the changes from the staging area, but not from the working directory. Unstaging changes is especially useful when you're working with multiple files and accidentally add a file you don't want to commit yet. To unstage a file, use git restore with the --staged argument: git restore --staged README.md Then check the status again: Changes not staged for commit: (use \"git add <file>...\" to update what will be committed) (use \"git restore <file>...\" to discard changes in working directory) modified: README.md no changes added to commit (use \"git add\" and/or \"git commit -a\") The changes to README.md are still in the working directory, but no longer in the staging area. In older versions of git, the command to restore a file in the staging area was git reset FILENAME . You may occasionally see people mention this command online, and it still works in recent versions of git. However, the git restore command is preferable because it disambiguates what you're trying to do ( git reset is also used for other things unrelated to restoring the staging area).","title":"Restoring the staging area"},{"location":"workshops/20220920-intro-to-git-and-github/lesson/#reverting-a-commit","text":"The git revert command is useful if you want to undo changes that you've already committed. Once a commit is in your repository's history, it generally stays there forever. There is a way to delete commits, but you should really only delete commits if you accidentally commit sensitive data . Instead, the idiomatic way to undo a commit is to create a new commit that reverses the changes. Let's start by editing the README.md file again and making a commit. Add the line This is a big mistake. to the end of the file. Then run: git add README.md git commit -m \"This commit is a mistake.\" Now suppose you decide the commit is a mistake, and want to undo it. First, you need to find the ID of the commit you want to undo. The git log command opens a scrolling list of all commits in the repository's history and their IDs. Run the command: git log You can exit the log by typing q . In the log, locate the the mistaken commit, and copy or remember the first 5 digits of its ID. Git is smart enough that it can generally recognize a commit from the first few digits of its ID, and will tell you if it needs more digits for disambiguation. In my repo, the ID of the commit starts with e01d1 . In your repo, the commit will likely have a different ID. Next, run this command, replacing the ID with the ID you copied: git revert e01d1 --no-edit The --no-edit flag tells git revert to generate the commit message for the new commit automatically. Without the flag, git revert will prompt you to enter a commit message. Now inspect the file with nano or cat . You should see that the changes from your bad commit are gone. If you look in the log with git log , you'll also see a new commit to revert the changes of a previous commit. Challenge : The goal of this challenge is to make a bad commit and then revert it. Work through these steps: Change a file in your repository. add and commit the changes. Find the ID of the commit from step 2 in the repository history. revert the commit from step 2. Challenge solution Change a file like `README.md`. Then run: git add README.md git commit -m \"update README\" To find the ID of the commit, run: git log Then revert the commit: git revert 07f61 --no-edit","title":"Reverting a commit"},{"location":"workshops/20220920-intro-to-git-and-github/lesson/#creating-a-branch-from-a-previous-commit","text":"Another strategy to revert to a prior state is to create a new branch that starts at the state of a previous commit. This has the advantage of keeping a branch with the work you did on it, but allows you to continue to work in a different direction from a previous commit. To do this, you can use git log like above to identify which commit you want your new branch to start out at. Once you have the commit ID, run the following: git checkout -b name-of-new-branch e01d1 You'll now be on a new branch that starts from the commit you indicated. To check, run git branch .","title":"Creating a branch from a previous commit"},{"location":"workshops/20220920-intro-to-git-and-github/lesson/#github-goodies","text":"","title":"GitHub Goodies"},{"location":"workshops/20220920-intro-to-git-and-github/lesson/#issues","text":"GitHub issues started as a way to record problems with software and have since evolved into generalized project planning tools . Issues are a great way to keep track of to do items, have asynchronous conversations relevant to a repository, or otherwise deposit information relevant to a repository.","title":"Issues"},{"location":"workshops/20220920-intro-to-git-and-github/lesson/#releases","text":"Releases bundle and deliver iterations of your project. When your entire repository reaches a consensus point, you can use GitHub to compress that version of files together. Releases integrate with zenodo to archive your repo and give it a DOI.","title":"Releases"},{"location":"workshops/20220920-intro-to-git-and-github/lesson/#gitignore","text":"A .gitignore file is a hidden text file in you repository that you can use to \"ignore\" files that you don't want under version control. This is handy for really large data files that you don't want to add to your repo. To use a .gitignore file, create a .gitignore text file in the main directory of your repo and add paths to files that you would like to ignore. The .gitignore file accepts regular expressions (e.g. *.fastq.gz to ignore any file that ends in .fastq.gz ).","title":".gitignore"},{"location":"workshops/20220920-intro-to-git-and-github/lesson/#links-to-other-goodies","text":"Archive a repository that is no long in use Continuous integration with GitHub Actions","title":"Links to other goodies"},{"location":"workshops/20220920-intro-to-git-and-github/lesson/#summary","text":"","title":"Summary"},{"location":"workshops/20231006-github-hacktoberfest/lesson/","text":"Using Git and GitHub to Contribute to Repositories You Don't Have Write Access To Arcadia is participating in Hacktoberfest 2023 . Our goal is to give back to the open source software community that we use every day in our own scientific work. As part of this, we are hosting a mini-hackathon to create a space for and support others in making contribution to open source software libraries. We've curated a list of bioinformatics (or adjacent) software repos that are accepting contributions. The purpose of the lesson below is to outline how to contribute to a GitHub repository that you don't own (e.g., you don't have write access to). While we will cover this in more depth below, we want to highlight how important it is to only contribute to projects that are open to contributions and to abide by community- or project-specific guidelines when doing so. Every contribution creates additional work for maintainers, many of whom are volunteers or who have responsibilities outside of software maintenance. Approaching your contribution with this in mind ensures that suggestions you make are more likely to be a net benefit for the project to which you're contributing. Respect, gratitude, and openness to feedback also go a long way toward fostering a fruitful collaboration. tl;dr Select a repository to contribute to and read the contribution guide. Fork your repository of interest to your own GitHub account (click the Fork button in the top right of the repo page). Navigate to your fork. Clone your fork down to your local machine: git clone https://github.com/your-username/repository-name.git Create a branch: git checkout -b branch-name Make your changes. Add your changes to be tracked by git: git add file-changed.txt Commit your changes: git commit -m 'short explanation of changes i made' Push your changes: git push origin branch-name Create a new pull request from your forked repository (click the Pull request banner and then the New Pull Request button located at the top of your repo). Describe your changes following the guidelines in the contribution guide or the PR template. Wait for your PR to be reviewed and make any requested changes. Setting up If you need to: sign up for a GitHub account install git locally configure global git settings create an ssh key to authenticate without a password Navigate to the Setting Up Instructions in the Intro to Git and GitHub lesson and follow along with the tutorial. Finding repositories that accept contributions & work that needs to be done Searching for repositories that are open to external contributions Not every open source repository is in a position to accept work from new contributors. Contributing to a repository that isn't accepting these types of contributions at best leads to unintegrated pull requests and at worse creates additional work and headache for maintainers. To avoid these situations, look out for signals that a repository is accepting contributions from new contributors. Good signals include: The repository has contributor guidelines. The repository uses issues to keep track of work that needs to be completed and labels issues to make it clear where contributions are welcome. The maintainers of the repository have recently engaged with issues, pull requests, or have recently updated the repository. The repository is marked with a hacktoberfest topic. Since Hacktoberfest is in its 10th year and repository topics have been around for three years, the tag may be from previous years so look for other signals that the repo is still accepting contributions. You can also search for the hacktoberfest2023 tag instead to get recent labels. We've curated a list of bioinformatics (or adjacent) software repos that are accepting contributions either for hacktoberfest or otherwise. You can start from this list or search GitHub topics to find a repository you want to contribute to. For the hands on portions in the rest of this lesson, we will practice making a contribution using the Arcadia-Science/2023-hackathon-practice repository. Contributor guidelines The Contributor Guidelines for a repository outline how the project would like to receive contributions. This is a very important document to read and to continually reference as you're making your contribution to make sure you're following the preferences of the community or project you're contributing to. Each project is a little different, so make sure you read this document closely for each project you plan to contribute to. In the Arcadia-Science/2023-hackathon-practice repository, the contributor guide is located in the file CONTRIBUTING.md and it is linked to from the README.md . Finding work that needs to be completed: issues, pull requests, and repo searches The contributor guidelines document often details how new contributors can find work that needs to be done. Often times, projects use GitHub Issues to document work that needs to be completed. When a project is accepting external contributions, they often use issue labels to highlight work that can be done by anyone. Common labels include \"good first issue,\" \"good second issue,\" and \"help wanted,\" but these labels can vary a lot by project. Issues can also be used to document work that is being worked on by someone else. These can be issues all on their own or responses to issues that already exist. Similarly, a pull request may declare work that is in progress or has already been contributed. It's a good idea to search open issues and pull requests, as well as the repository in general, to make sure the work you want to do hasn't already been undertaken by another contributor. Narrating the work that you will complete To make sure that others know what work you plan to do, it's usually a good idea to narrate this work somewhere in the repository. Check the contributor guidelines first to see what the recommended strategy to use is. A common strategy is to respond with a comment to an open issue asking if the work still needs to be done or to state that you plan to work on it. According to the contributor guide in the Arcadia-Science/2023-hackathon-practice repository, the maintainers ask that you post an issue declaring the change that you will make the repo. The guide also states that if you plan to add a file that the file should be added in the practice folder. Practice opening an issue. First, navigate to the issues tab. Then, open a new issue by selecting the green \"New issue\" button. Describe that you will add a file to the practice folder called my-first-file- , your initials, and then .txt ( my-first-file-ter.txt ). Workflow for making changes on a repository you don't own Once you have identified a repository you want to contribute to and the work that you plan on doing, it's time to start making actual changes. Below, we cover how to make changes in a way that will allow you to contribute those changes back to the main code base when you are finished with your work. Forking the repository When you don't have write access to a repository, you need to make a copy of the repo into a user name or organization that you do have write access to. This process is called forking or creating a fork for a repo. You do this by clicking the fork button in the upper right hand corner of a repository. Select the new owner (for this lesson, yourself), and then select the green \"Create fork\" button. You should now have a copy of the repository in your own GitHub account. Cloning the fork from remote to local Cloning is the process of copying an existing Git repository from a remote location (here, on GitHub) to your local computer. Navigate to your fork of the repository you're working with (GitHub did this for you after forking if you kept the same tab open). The URL should start with your GitHub user name. To clone the repository, click the green \"Code\" button in the top right hand corner of the repository screen. This creates a drop down menu with clone options. We'll select the SSH tab because we configured an ssh key pair. Once you select the tab, copy the path that starts with git@gitub.com: . Then, navigate to your terminal and use the command below to clone the repository. Remember to substitute out the username/URL with your own URL that we copied. cd ~ # this will clone the repo to your home directory. Feel free to put it somewhere else if you prefer. git clone git@github.com:your_username/2023-hacktathon-practice cd 2023-hackathon-practice Making changes to the code: working on a branch When you want to make changes to files in a repository, it\u2019s best practice to do it in a branch . A branch is an independent line of development of files in a repository. You can think of it like making a copy of a document and making changes to that copy only: the original stays the same, but the copy diverges with new content. In a branch, all files are copied and can be changed. Branches are particularly powerful for collaborating. They allow multiple people to work on the same code base and seamless integrate changes. Because we'll be contributing to someone else's code base, it's best to do so on a branch. By default, you start on the main branch (some older repositories will start from master ). You can see which branch you're on locally using the git branch command. git branch Let's create a branch and make changes to it. We can create a new local branch using git checkout -b . git checkout tells git we want to switch which branch we're currently on, while the -b flag tells git that we're creating a new branch. git checkout -b ter/my-feature-branch Branch names can be used to convey information that can help you and others keep track of what you're doing. For example, you can prepend your branch names with your initials and a slash to make it clear that you were the one who created the branch, and you can specify a name that is associated with the file changes made on the branch: <your initials>/<brief description of the code change> . Now, if we run git branch we'll see we're on the branch ter/my-feature-branch . git branch To go back to the main branch, we can use the git checkout command without the flag -b . git checkout main and to go back to our feature branch, we can use git checkout again. git checkout ter/my-feature-branch Whenever you create a new branch, it branches off from the branch you're currently in when you make the new branch. In this case, we started in main , so our branch will branch off of from here. Making changes to the code: adding, committing, and pushing We can now make changes to our files in our local repository. The basic Git workflow begins when you communicate to Git about changes you've made to your files. Once you like a set of changes you\u2019ve made you can tell Git about them by using the command git add . This stages the changes you have made. Staging a file tells Git that you're ready to commit those files -- it's a way of telling Git which files you're ready to commit. Next, you bake them into the branch using git commit . When you\u2019re ready, you can communicate those changes back to GitHub using git push . This will push your the changes that are in your local repository up to the remote repository. Let's try this workflow out. Throughout this process, we'll use the command git status to track our progress through the workflow. git status displays the state of a working direcotry and the staging area. git status We'll use the echo command to create a new file matching the name of the file we said we'd make in our issue. ls echo \"creating my first file\" > practice/my-first-file-ter.txt ls Take a look at the contents of your practice/my-first-file-ter.txt file: less practice/my-first-file-ter.txt And run git status to see how creating a new file changes the output of that command: git status Once you have made changes in your repository, you need to tell Git to start tracking the file. The command git add adds the files to the \"staging area\", meaning Git is now tracking the changes. git add practice/my-first-file-ter.txt After adding this file, we see our output of git status changes. git status The text associated with our file is now green because the file is staged. When you've made all of the changes to a file that represent a unit of changes, you can use git commit to create a snapshot of the file in your Git version history. git commit -m \"start my first file\" git status The -m flag is the message that is associated with that commit. The message should be short and descriptive so that you or someone looking at your code could quickly determine what changes took place from one commit to the next. What constitutes a unit of changes worthy of running git commit ? That depends on the project and the person, but think about returning to this code six months in the future. What set of changes would make it most easy to return to an earlier version of document? Committing a file bakes changes into your local repository. To communicate that changes back up to your remote repository, use git push . git push origin ter/my-feature-branch In the above command, origin is shorthand for the remote repository that a project was cloned from. To view what this value is, you can run git remote -v . Contributing changes Pull requests are used to contribute changes from your fork back to the main repository. Before opening a pull request, re-read the contributing guidelines and make sure to fulfill all of the requirements requested therein. This will often include that all test should be passing locally. For more on testing, see this Carpentries Incubator lesson on testing in python. To create a new pull request, you can click the yellow banner with the green button, \"Compare & pull request.\" This will launch a page that suggests where the changes are coming from and where they're being integrated into. Pay special attention on this page to make sure you select the appropriate branches and forks here. Use the title and dialogue box to annotate what your PR accomplishes. Be sure to link to the issue you're addressing using a # . Re-read the contributor guidelines or follow the PR template to make sure you're including all required information. The opened PR looks something like this In some repositories, opening a PR may launch continuous integration with GitHub Actions . These will appear as banners of tasks. When they pass successfully you'll see a green check mark. Generally, all tests need to pass before a PR is merged. After opening your PR, a maintainer will typically review the PR and request or make changes. Summary Terms introduced: Term Definition Git a version control system GitHub an internet hosting service for version control using Git repository a collection of files and folders associated with a project that is under Git version control local accessed from your system; the computer you're working on remote stored on a remote computer; GitHub commit a snapshot of your files and folders at a given time branch an independent line of development in a repository pull request a mechanism to suggest changes from a branch be integrated back into the main branch review the process of reviewing changes in a branch through a pull request issue a tracking tool integrated into a GitHub repository Commands introduced: Command Function git clone copies a Git repository from remote to local git status summarizes the current status of the Git repository, including the branch you are on and any changes on that branch git add adds new or changed files in your working directory to the Git staging area git commit creates a commit of your repository git push uploads all local branch commits to the corresponding remote branch git checkout switches between local branches, or creates a new branch with the -b flag git branch reports what branch you're currently on and what other local branches exist git pull updates current local working branch and all of the remote tracking branches","title":"Using Git and GitHub to Contribute to Repositories You Don't Have Write Access To"},{"location":"workshops/20231006-github-hacktoberfest/lesson/#using-git-and-github-to-contribute-to-repositories-you-dont-have-write-access-to","text":"Arcadia is participating in Hacktoberfest 2023 . Our goal is to give back to the open source software community that we use every day in our own scientific work. As part of this, we are hosting a mini-hackathon to create a space for and support others in making contribution to open source software libraries. We've curated a list of bioinformatics (or adjacent) software repos that are accepting contributions. The purpose of the lesson below is to outline how to contribute to a GitHub repository that you don't own (e.g., you don't have write access to). While we will cover this in more depth below, we want to highlight how important it is to only contribute to projects that are open to contributions and to abide by community- or project-specific guidelines when doing so. Every contribution creates additional work for maintainers, many of whom are volunteers or who have responsibilities outside of software maintenance. Approaching your contribution with this in mind ensures that suggestions you make are more likely to be a net benefit for the project to which you're contributing. Respect, gratitude, and openness to feedback also go a long way toward fostering a fruitful collaboration.","title":"Using Git and GitHub to Contribute to Repositories You Don't Have Write Access To"},{"location":"workshops/20231006-github-hacktoberfest/lesson/#tldr","text":"Select a repository to contribute to and read the contribution guide. Fork your repository of interest to your own GitHub account (click the Fork button in the top right of the repo page). Navigate to your fork. Clone your fork down to your local machine: git clone https://github.com/your-username/repository-name.git Create a branch: git checkout -b branch-name Make your changes. Add your changes to be tracked by git: git add file-changed.txt Commit your changes: git commit -m 'short explanation of changes i made' Push your changes: git push origin branch-name Create a new pull request from your forked repository (click the Pull request banner and then the New Pull Request button located at the top of your repo). Describe your changes following the guidelines in the contribution guide or the PR template. Wait for your PR to be reviewed and make any requested changes.","title":"tl;dr"},{"location":"workshops/20231006-github-hacktoberfest/lesson/#setting-up","text":"If you need to: sign up for a GitHub account install git locally configure global git settings create an ssh key to authenticate without a password Navigate to the Setting Up Instructions in the Intro to Git and GitHub lesson and follow along with the tutorial.","title":"Setting up"},{"location":"workshops/20231006-github-hacktoberfest/lesson/#finding-repositories-that-accept-contributions-work-that-needs-to-be-done","text":"","title":"Finding repositories that accept contributions &amp; work that needs to be done"},{"location":"workshops/20231006-github-hacktoberfest/lesson/#searching-for-repositories-that-are-open-to-external-contributions","text":"Not every open source repository is in a position to accept work from new contributors. Contributing to a repository that isn't accepting these types of contributions at best leads to unintegrated pull requests and at worse creates additional work and headache for maintainers. To avoid these situations, look out for signals that a repository is accepting contributions from new contributors. Good signals include: The repository has contributor guidelines. The repository uses issues to keep track of work that needs to be completed and labels issues to make it clear where contributions are welcome. The maintainers of the repository have recently engaged with issues, pull requests, or have recently updated the repository. The repository is marked with a hacktoberfest topic. Since Hacktoberfest is in its 10th year and repository topics have been around for three years, the tag may be from previous years so look for other signals that the repo is still accepting contributions. You can also search for the hacktoberfest2023 tag instead to get recent labels. We've curated a list of bioinformatics (or adjacent) software repos that are accepting contributions either for hacktoberfest or otherwise. You can start from this list or search GitHub topics to find a repository you want to contribute to. For the hands on portions in the rest of this lesson, we will practice making a contribution using the Arcadia-Science/2023-hackathon-practice repository.","title":"Searching for repositories that are open to external contributions"},{"location":"workshops/20231006-github-hacktoberfest/lesson/#contributor-guidelines","text":"The Contributor Guidelines for a repository outline how the project would like to receive contributions. This is a very important document to read and to continually reference as you're making your contribution to make sure you're following the preferences of the community or project you're contributing to. Each project is a little different, so make sure you read this document closely for each project you plan to contribute to. In the Arcadia-Science/2023-hackathon-practice repository, the contributor guide is located in the file CONTRIBUTING.md and it is linked to from the README.md .","title":"Contributor guidelines"},{"location":"workshops/20231006-github-hacktoberfest/lesson/#finding-work-that-needs-to-be-completed-issues-pull-requests-and-repo-searches","text":"The contributor guidelines document often details how new contributors can find work that needs to be done. Often times, projects use GitHub Issues to document work that needs to be completed. When a project is accepting external contributions, they often use issue labels to highlight work that can be done by anyone. Common labels include \"good first issue,\" \"good second issue,\" and \"help wanted,\" but these labels can vary a lot by project. Issues can also be used to document work that is being worked on by someone else. These can be issues all on their own or responses to issues that already exist. Similarly, a pull request may declare work that is in progress or has already been contributed. It's a good idea to search open issues and pull requests, as well as the repository in general, to make sure the work you want to do hasn't already been undertaken by another contributor.","title":"Finding work that needs to be completed: issues, pull requests, and repo searches"},{"location":"workshops/20231006-github-hacktoberfest/lesson/#narrating-the-work-that-you-will-complete","text":"To make sure that others know what work you plan to do, it's usually a good idea to narrate this work somewhere in the repository. Check the contributor guidelines first to see what the recommended strategy to use is. A common strategy is to respond with a comment to an open issue asking if the work still needs to be done or to state that you plan to work on it. According to the contributor guide in the Arcadia-Science/2023-hackathon-practice repository, the maintainers ask that you post an issue declaring the change that you will make the repo. The guide also states that if you plan to add a file that the file should be added in the practice folder. Practice opening an issue. First, navigate to the issues tab. Then, open a new issue by selecting the green \"New issue\" button. Describe that you will add a file to the practice folder called my-first-file- , your initials, and then .txt ( my-first-file-ter.txt ).","title":"Narrating the work that you will complete"},{"location":"workshops/20231006-github-hacktoberfest/lesson/#workflow-for-making-changes-on-a-repository-you-dont-own","text":"Once you have identified a repository you want to contribute to and the work that you plan on doing, it's time to start making actual changes. Below, we cover how to make changes in a way that will allow you to contribute those changes back to the main code base when you are finished with your work.","title":"Workflow for making changes on a repository you don't own"},{"location":"workshops/20231006-github-hacktoberfest/lesson/#forking-the-repository","text":"When you don't have write access to a repository, you need to make a copy of the repo into a user name or organization that you do have write access to. This process is called forking or creating a fork for a repo. You do this by clicking the fork button in the upper right hand corner of a repository. Select the new owner (for this lesson, yourself), and then select the green \"Create fork\" button. You should now have a copy of the repository in your own GitHub account.","title":"Forking the repository"},{"location":"workshops/20231006-github-hacktoberfest/lesson/#cloning-the-fork-from-remote-to-local","text":"Cloning is the process of copying an existing Git repository from a remote location (here, on GitHub) to your local computer. Navigate to your fork of the repository you're working with (GitHub did this for you after forking if you kept the same tab open). The URL should start with your GitHub user name. To clone the repository, click the green \"Code\" button in the top right hand corner of the repository screen. This creates a drop down menu with clone options. We'll select the SSH tab because we configured an ssh key pair. Once you select the tab, copy the path that starts with git@gitub.com: . Then, navigate to your terminal and use the command below to clone the repository. Remember to substitute out the username/URL with your own URL that we copied. cd ~ # this will clone the repo to your home directory. Feel free to put it somewhere else if you prefer. git clone git@github.com:your_username/2023-hacktathon-practice cd 2023-hackathon-practice","title":"Cloning the fork from remote to local"},{"location":"workshops/20231006-github-hacktoberfest/lesson/#making-changes-to-the-code-working-on-a-branch","text":"When you want to make changes to files in a repository, it\u2019s best practice to do it in a branch . A branch is an independent line of development of files in a repository. You can think of it like making a copy of a document and making changes to that copy only: the original stays the same, but the copy diverges with new content. In a branch, all files are copied and can be changed. Branches are particularly powerful for collaborating. They allow multiple people to work on the same code base and seamless integrate changes. Because we'll be contributing to someone else's code base, it's best to do so on a branch. By default, you start on the main branch (some older repositories will start from master ). You can see which branch you're on locally using the git branch command. git branch Let's create a branch and make changes to it. We can create a new local branch using git checkout -b . git checkout tells git we want to switch which branch we're currently on, while the -b flag tells git that we're creating a new branch. git checkout -b ter/my-feature-branch Branch names can be used to convey information that can help you and others keep track of what you're doing. For example, you can prepend your branch names with your initials and a slash to make it clear that you were the one who created the branch, and you can specify a name that is associated with the file changes made on the branch: <your initials>/<brief description of the code change> . Now, if we run git branch we'll see we're on the branch ter/my-feature-branch . git branch To go back to the main branch, we can use the git checkout command without the flag -b . git checkout main and to go back to our feature branch, we can use git checkout again. git checkout ter/my-feature-branch Whenever you create a new branch, it branches off from the branch you're currently in when you make the new branch. In this case, we started in main , so our branch will branch off of from here.","title":"Making changes to the code: working on a branch"},{"location":"workshops/20231006-github-hacktoberfest/lesson/#making-changes-to-the-code-adding-committing-and-pushing","text":"We can now make changes to our files in our local repository. The basic Git workflow begins when you communicate to Git about changes you've made to your files. Once you like a set of changes you\u2019ve made you can tell Git about them by using the command git add . This stages the changes you have made. Staging a file tells Git that you're ready to commit those files -- it's a way of telling Git which files you're ready to commit. Next, you bake them into the branch using git commit . When you\u2019re ready, you can communicate those changes back to GitHub using git push . This will push your the changes that are in your local repository up to the remote repository. Let's try this workflow out. Throughout this process, we'll use the command git status to track our progress through the workflow. git status displays the state of a working direcotry and the staging area. git status We'll use the echo command to create a new file matching the name of the file we said we'd make in our issue. ls echo \"creating my first file\" > practice/my-first-file-ter.txt ls Take a look at the contents of your practice/my-first-file-ter.txt file: less practice/my-first-file-ter.txt And run git status to see how creating a new file changes the output of that command: git status Once you have made changes in your repository, you need to tell Git to start tracking the file. The command git add adds the files to the \"staging area\", meaning Git is now tracking the changes. git add practice/my-first-file-ter.txt After adding this file, we see our output of git status changes. git status The text associated with our file is now green because the file is staged. When you've made all of the changes to a file that represent a unit of changes, you can use git commit to create a snapshot of the file in your Git version history. git commit -m \"start my first file\" git status The -m flag is the message that is associated with that commit. The message should be short and descriptive so that you or someone looking at your code could quickly determine what changes took place from one commit to the next. What constitutes a unit of changes worthy of running git commit ? That depends on the project and the person, but think about returning to this code six months in the future. What set of changes would make it most easy to return to an earlier version of document? Committing a file bakes changes into your local repository. To communicate that changes back up to your remote repository, use git push . git push origin ter/my-feature-branch In the above command, origin is shorthand for the remote repository that a project was cloned from. To view what this value is, you can run git remote -v .","title":"Making changes to the code: adding, committing, and pushing"},{"location":"workshops/20231006-github-hacktoberfest/lesson/#contributing-changes","text":"Pull requests are used to contribute changes from your fork back to the main repository. Before opening a pull request, re-read the contributing guidelines and make sure to fulfill all of the requirements requested therein. This will often include that all test should be passing locally. For more on testing, see this Carpentries Incubator lesson on testing in python. To create a new pull request, you can click the yellow banner with the green button, \"Compare & pull request.\" This will launch a page that suggests where the changes are coming from and where they're being integrated into. Pay special attention on this page to make sure you select the appropriate branches and forks here. Use the title and dialogue box to annotate what your PR accomplishes. Be sure to link to the issue you're addressing using a # . Re-read the contributor guidelines or follow the PR template to make sure you're including all required information. The opened PR looks something like this In some repositories, opening a PR may launch continuous integration with GitHub Actions . These will appear as banners of tasks. When they pass successfully you'll see a green check mark. Generally, all tests need to pass before a PR is merged. After opening your PR, a maintainer will typically review the PR and request or make changes.","title":"Contributing changes"},{"location":"workshops/20231006-github-hacktoberfest/lesson/#summary","text":"","title":"Summary"}]}