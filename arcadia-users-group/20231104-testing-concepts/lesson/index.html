
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="http://arcadia-science.github.io/arcadia-computational-training/arcadia-users-group/20231104-testing-concepts/lesson/">
      
      <link rel="icon" href="../../../assets/Arcadia_LogoOnly-XL-K.png">
      <meta name="generator" content="mkdocs-1.4.0, mkdocs-material-8.5.6">
    
    
      
        <title>Testing concepts and terminology - Arcadia Science Computational Training</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.20d9efc8.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.cbb835fc.min.css">
        
          
          
          <meta name="theme-color" content="#000000">
        
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="black" data-md-color-accent="light-blue">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#testing-concepts-and-terminology" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="Arcadia Science Computational Training" class="md-header__button md-logo" aria-label="Arcadia Science Computational Training" data-md-component="logo">
      
  <img src="../../../assets/logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Arcadia Science Computational Training
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Testing concepts and terminology
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/arcadia-science/arcadia-computational-training" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    arcadia-computational-training
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Arcadia Science Computational Training" class="md-nav__button md-logo" aria-label="Arcadia Science Computational Training" data-md-component="logo">
      
  <img src="../../../assets/logo.png" alt="logo">

    </a>
    Arcadia Science Computational Training
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/arcadia-science/arcadia-computational-training" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    arcadia-computational-training
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        Computational Training Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../overview/" class="md-nav__link">
        Arcadia Users Group
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../workshops/overview/" class="md-nav__link">
        Workshops
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../CONTRIBUTING/" class="md-nav__link">
        Contribute
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#lesson-setup" class="md-nav__link">
    Lesson setup
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#an-introduction-to-testing-concepts" class="md-nav__link">
    An introduction to testing concepts
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#assertions" class="md-nav__link">
    Assertions
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#exceptions" class="md-nav__link">
    Exceptions
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#unit-tests" class="md-nav__link">
    Unit tests
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#using-the-test-framework-pytest" class="md-nav__link">
    Using the test framework pytest
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#testing-for-expected-exceptions" class="md-nav__link">
    Testing for expected exceptions
  </a>
  
    <nav class="md-nav" aria-label="Testing for expected exceptions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#challenge-1-altering-functions-to-pass-all-tests" class="md-nav__link">
    Challenge 1: Altering functions to pass all tests
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#parametrized-tests" class="md-nav__link">
    Parametrized tests
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#integration-tests" class="md-nav__link">
    Integration tests
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#regression-tests" class="md-nav__link">
    Regression tests
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#continuous-integration" class="md-nav__link">
    Continuous integration
  </a>
  
    <nav class="md-nav" aria-label="Continuous integration">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#set-up-a-mean-git-repository-on-github" class="md-nav__link">
    Set Up a Mean Git Repository on GitHub
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#github-actions" class="md-nav__link">
    GitHub Actions
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#triggering-ci" class="md-nav__link">
    Triggering CI
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="testing-concepts-and-terminology">Testing concepts and terminology</h1>
<blockquote>
<p>Note that this lesson has been modified from <a href="https://github.com/carpentries-incubator">The Carpentries Incubator</a> lesson on <a href="https://carpentries-incubator.github.io/python-testing/">Python Testing</a>.
Parts are reproduced in full, but the major changes were included to shorten the lesson to 60 minutes.</p>
</blockquote>
<p>Software testing is a critical practice in the development process aimed at ensuring that a program functions as intended and does not break when changes are made.
It encompasses a range of activities designed to evaluate the correctness, performance, and usability among other aspects of a software application.</p>
<p>At its core, software testing seeks to verify that your software does what it is supposed to do, handles various cases gracefully, and remains stable even as changes are introduced over time.
It is the safety net that catches bugs or errors before they reach the end-users, and it's what gives developers the confidence to continue improving their code.</p>
<p>Everyone engages in software testing to some extent, often informally.
This could be as simple as running the software to see what happens, or doing some exploratory testing as new code is written or old code is modified.
Such testing might involve printing or plotting the outputs of your code as you work.</p>
<p>However, informal testing has its limitations.
It's manual, sporadic, and can become unmanageable, especially as the codebase grows and evolves.
This is where systematic testing comes into play.</p>
<p>Systematic testing takes the informal testing behaviors and codifies them, allowing for automation.
This automation enables tests to be run quickly and repeatedly across the entire codebase, ensuring that every part of the software is validated every time a change is made.
Without automation, it's nearly impossible to ensure that a change in one part of the software hasn't introduced a new bug elsewhere.</p>
<p>This lesson introduces the concepts of automated testing and provides guidance on how to utilize Python constructs to start writing tests.
Although we focus on Python, many of the higher-level concepts discussed are applicable to writing tests in other programming languages as well.</p>
<details>
 <summary>Implementing tests in other languages</summary>
 While this lesson uses Python, almost all programming languages have robust (and often loved) packages dedicated to testing.
 In R, testing is orchestrated with the <a href="https://testthat.r-lib.org/">testthat package</a>.
 The <a href="https://r-pkgs.org">R packages</a> guide has a chapter dedicated to <a href=https://r-pkgs.org/testing-basics.html>testing</a>.
 Other programming languages, such as Rust, have built-in testing features.
</details>

<h2 id="lesson-setup">Lesson setup</h2>
<p>This lesson will take advantage of the skills we've learned in many previous lessons.
We'll use <a href="https://training.arcadiascience.com/arcadia-users-group/20221024-jupyter-notebooks/lesson/">Jupyter Notebooks</a>, <a href="https://training.arcadiascience.com/workshops/20220920-intro-to-git-and-github/lesson/">GitHub</a>, <a href="https://training.arcadiascience.com/arcadia-users-group/20221017-conda/lesson/">conda</a>, and <a href="https://training.arcadiascience.com/arcadia-users-group/20230228-intro-to-python-1/lesson/">Python</a>.
This is more overhead than we typically strive for in a lesson, but we hope that it's a chance to practice these skills to achieve a new goal.
In the future, we may provide a <a href="https://www.gitpod.io/">GitPod</a> environment for learners to use while working through this lesson, however if possible we would prefer to empower users to start implementing tests on their own computers using their own setup.</p>
<p>To start this lesson, we'll begin by creating a conda environment that has the tools we'll need.</p>
<div class="highlight"><pre><span></span><code>mamba create -n augtest jupyter pytest
conda activate augtest
</code></pre></div>
<p>We'll also create a folder to help us stay organized.
<div class="highlight"><pre><span></span><code>mkdir 20231114-aug-testing-lesson
cd 20231114-aug-testing-lesson
</code></pre></div></p>
<p>Once our environment is activated, start a Jupyter notebook</p>
<div class="highlight"><pre><span></span><code>jupyter notebook
</code></pre></div>
<p>Now we can start learning about testing!</p>
<h2 id="an-introduction-to-testing-concepts">An introduction to testing concepts</h2>
<p>There are many ways to test software, such as assertions, exceptions, unit tests, integration tests, and regression tests.</p>
<ul>
<li><strong>Exceptions and assertions</strong>: While writing code, <code>exceptions</code> and <code>assertions</code> can be added to sound an alarm as runtime problems come up.
These kinds of tests, are embedded in the software itself and handle, as their name implies, exceptional cases rather than the norm.</li>
<li><strong>Unit tests</strong>: Unit tests investigate the behavior of units of code (such as functions, classes, or data structures).
By validating each software unit across the valid range of its input and output parameters, tracking down unexpected behavior that may appear when the units are combined is made vastly simpler.
Some examples of things a unit test might test include functions, individual Snakemake rules, a process in Nextflow, or a cell in a Jupyter notebook.</li>
<li><strong>Integration tests</strong>: Integration tests check that various pieces of the software work together as expected.
Some examples of things an integration test might test include a set of Snakemake rule or Nextflow processes or the execution of an entire Jupyter notebook.</li>
<li><strong>Regression tests</strong>: Regression tests defend against new bugs, or regressions, which might appear due to new software and updates. Regression tests can also refer to tests for decreases in performance (run time, memory usage, etc.) or in the quality of some output (the resolution of a rendered graph, accuracy of a set of predictions, etc.).</li>
</ul>
<p>While each of these types of tests has a different definition, in practice there isn't always a firm delineation between each type.</p>
<h2 id="assertions">Assertions</h2>
<p>Perhaps the simplest test we can use in Python is to directly test, using an <code>if</code> statement, whether some condition is <code>True</code>, and exit the program otherwise:</p>
<div class="highlight"><pre><span></span><code>size = 10
if size &gt; 5:
    exit()
</code></pre></div>
<p>Python provides a shortcut for these type of checks, called <em>assertions</em>. Assertions are the simplest type of test.
They are used as a tool for bounding acceptable behavior during runtime.
The <code>assert</code> keyword in Python has the same behavior as the <code>if</code> statement above, but it also provides some information about where the check happened and an optional message:</p>
<p><div class="highlight"><pre><span></span><code>assert True == False
</code></pre></div>
<div class="highlight"><pre><span></span><code>Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  AssertionError
</code></pre></div></p>
<div class="highlight"><pre><span></span><code>assert True == True
</code></pre></div>
<p>Assertions halt code execution instantly if the comparison is false and do nothing if the comparison is true.
These are therefore a good tool for guarding the function against inappropriate input:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">mean</span><span class="p">(</span><span class="n">num_list</span><span class="p">):</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">num_list</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">num_list</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">num_list</span><span class="p">)</span>
</code></pre></div>
<p>The advantage of assertions is their ease of use.
They are rarely more than one line of code.
The disadvantage is that assertions halt execution indiscriminately and the helpfulness of the resulting error message is usually quite limited.
In practice, assertions are almost never directly used in Python programs.</p>
<h2 id="exceptions">Exceptions</h2>
<p>Exceptions are more sophisticated than assertions.
When an error is encountered, an informative exception is 'thrown' or 'raised'.</p>
<p>For example, instead of the assertion in the case before, an exception can be used.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">mean</span><span class="p">(</span><span class="n">num_list</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">num_list</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>
        <span class="s2">&quot;The algebraic mean of an empty list is undefined. &quot;</span>
        <span class="s2">&quot;Please provide a list of numbers.&quot;</span>
    <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">num_list</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">num_list</span><span class="p">)</span>
</code></pre></div>
<p>Once an exception is raised, it will be passed upward in the program scope.
An exception can be used to trigger additional error messages or an alternative behavior.
Rather than immediately halting code execution, the exception can be 'caught' upstream with a try-except block.
When wrapped in a try-except block, the exception can be intercepted before it reaches global scope and halts execution.</p>
<p>To add information or replace the message before it is passed upstream, the try-catch block can be used to catch-and-reraise the exception:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">mean</span><span class="p">(</span><span class="n">num_list</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">num_list</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">num_list</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">ZeroDivisionError</span> <span class="k">as</span> <span class="n">original_error</span><span class="p">:</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="s2">&quot;The algebraic mean of an empty list is undefined. Please provide a list of numbers.&quot;</span>
        <span class="k">raise</span> <span class="ne">ZeroDivisionError</span><span class="p">(</span><span class="n">original_error</span><span class="o">.</span><span class="fm">__str__</span><span class="p">()</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span>  <span class="n">msg</span><span class="p">)</span>
</code></pre></div>
<p>Alternatively, the exception can be handled appropriately for the use case.
If an alternative behavior is preferred, the exception can be disregarded and a responsive behavior can be implemented like so:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">mean</span><span class="p">(</span><span class="n">num_list</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">num_list</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">num_list</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">ZeroDivisionError</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>
</code></pre></div>
<p>If a single function might raise more than one type of exception, each can be caught and handled separately.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">mean</span><span class="p">(</span><span class="n">num_list</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">num_list</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">num_list</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">ZeroDivisionError</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>
    <span class="k">except</span> <span class="ne">TypeError</span> <span class="k">as</span> <span class="n">original_error</span><span class="p">:</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="s2">&quot;The algebraic mean of an non-numerical list is undefined.</span><span class="se">\</span>
<span class="s2">               Please provide a list of numbers.&quot;</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">original_error</span><span class="o">.</span><span class="fm">__str__</span><span class="p">()</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span>  <span class="n">msg</span><span class="p">)</span>
</code></pre></div>
<p>Exceptions have the advantage of being simple to include and when accompanied by useful help message, can be helpful to the user.
However, not all behaviors can or should be found with runtime exceptions.
Most behaviors should be validated with unit tests.</p>
<h2 id="unit-tests">Unit tests</h2>
<p>Unit tests are so called because they test the functionality of the code by interrogating individual functions and methods.
Functions and methods can often be considered the atomic units of software but what is considered to be the smallest code <em>unit</em> is subjective.
Implementing unit tests often has the effect of encouraging both the code and the tests to be as small, well-defined, and modular as possible.
In Python, unit tests typically take the form of test functions that call and make assertions about methods and functions in the code base.
For now, we'll write some tests for the mean function and simply run them individually to see whether they fail.
Later in this lesson, we'll use a test framework to collect and run them.
Using a test framework makes running tests streamlined.</p>
<p>Unit tests are typically made of three pieces, some set-up, a number of assertions, and some tear-down.
Set-up can be as simple as initializing the input values or as complex as creating and initializing concrete instances of a class.
Ultimately, the test occurs when an assertion is made, comparing the observed and expected values.
For example, let us test that our mean function successfully calculates the known value for a simple list.</p>
<p>Before running the next code, save your <code>mean</code> function to a file called <code>mean.py</code> in the working directory.</p>
<p>You can use this code to save to file:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">mean</span><span class="p">(</span><span class="n">num_list</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">num_list</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">num_list</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">ZeroDivisionError</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>
    <span class="k">except</span> <span class="ne">TypeError</span> <span class="k">as</span> <span class="n">original_error</span><span class="p">:</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="s2">&quot;The algebraic mean of an non-numerical list is undefined.</span><span class="se">\</span>
<span class="s2">               Please provide a list of numbers.&quot;</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">original_error</span><span class="o">.</span><span class="fm">__str__</span><span class="p">()</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span>  <span class="n">msg</span><span class="p">)</span>
</code></pre></div>
<p>Now, back in your Jupyter Notebook run the following code:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">mean</span> <span class="kn">import</span> <span class="o">*</span>

<span class="k">def</span> <span class="nf">test_mean_with_ints</span><span class="p">():</span>
    <span class="n">num_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
    <span class="n">observed_value</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">num_list</span><span class="p">)</span>
    <span class="n">expected_value</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="k">assert</span> <span class="n">observed_value</span> <span class="o">==</span> <span class="n">expected_value</span>
</code></pre></div>
<p>The test above:</p>
<ul>
<li>sets up the input parameters (the list <code>[1, 2, 3, 4, 5]</code>);</li>
<li>collects the observed result;</li>
<li>declares the expected result (calculated with our human brain);</li>
<li>and compares the two with an assertion.</li>
</ul>
<p>A unit test suite is made up of many tests just like this one.
A single implemented function may be tested in numerous ways.</p>
<p>In a file called <code>test_mean.py</code>, implement the following code:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">mean</span> <span class="kn">import</span> <span class="o">*</span>

<span class="k">def</span> <span class="nf">test_mean_with_ints</span><span class="p">():</span>
    <span class="n">num_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
    <span class="n">observed_value</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">num_list</span><span class="p">)</span>
    <span class="n">expected_value</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="k">assert</span> <span class="n">observed_value</span> <span class="o">==</span> <span class="n">expected_value</span>

<span class="k">def</span> <span class="nf">test_mean_with_zero</span><span class="p">():</span>
    <span class="n">num_list</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">]</span>
    <span class="n">observed_value</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">num_list</span><span class="p">)</span>
    <span class="n">expected_value</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="k">assert</span> <span class="n">observed_value</span> <span class="o">==</span> <span class="n">expected_value</span>

<span class="k">def</span> <span class="nf">test_mean_with_double</span><span class="p">():</span>
    <span class="c1"># This one will fail in Python 2</span>
    <span class="n">num_list</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
    <span class="n">observed_value</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">num_list</span><span class="p">)</span>
    <span class="n">expected_value</span> <span class="o">=</span> <span class="mf">2.5</span>
    <span class="k">assert</span> <span class="n">observed_value</span> <span class="o">==</span> <span class="n">expected_value</span>

<span class="k">def</span> <span class="nf">test_mean_with_long</span><span class="p">():</span>
    <span class="n">big</span> <span class="o">=</span> <span class="mi">100000000</span>
    <span class="n">observed_value</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">big</span><span class="p">))</span>
    <span class="n">expected_value</span> <span class="o">=</span> <span class="n">big</span><span class="o">/</span><span class="mf">2.0</span>
    <span class="k">assert</span> <span class="n">observed_value</span> <span class="o">==</span> <span class="n">expected_value</span>
</code></pre></div>
<p>Use Jupyter Notebook to import the <code>test_mean</code> package and run each test like this:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">test_mean</span> <span class="kn">import</span> <span class="o">*</span>

<span class="n">test_mean_with_ints</span><span class="p">()</span>
<span class="n">test_mean_with_zero</span><span class="p">()</span>
<span class="n">test_mean_with_double</span><span class="p">()</span>
<span class="n">test_mean_with_long</span><span class="p">()</span>
</code></pre></div>
<p>We just wrote and ran five tests for our <code>mean()</code> function.
You may have noticed that several of the tests look very similar to each other -- they introduce an input, call <code>mean()</code>, and test its output against an expected value.
We'll come back to this later, offering a way to write a single test that applies to multiple inputs.</p>
<h2 id="using-the-test-framework-pytest">Using the test framework <code>pytest</code></h2>
<p>We created a suite of tests for our mean function, but it was annoying to run them one at a time.
It would be a lot better if there were some way to run them all at once, just reporting which tests fail and which succeed.</p>
<p>Thankfully, that exists.
Recall our tests:</p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span> <span class="nn">mean</span> <span class="kn">import</span> <span class="o">*</span>

<span class="k">def</span> <span class="nf">test_mean_with_ints</span><span class="p">():</span>
    <span class="n">num_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
    <span class="n">observed_value</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">num_list</span><span class="p">)</span>
    <span class="n">expected_value</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="k">assert</span> <span class="n">observed_value</span> <span class="o">==</span> <span class="n">expected_value</span>

<span class="k">def</span> <span class="nf">test_mean_with_zero</span><span class="p">():</span>
    <span class="n">num_list</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">]</span>
    <span class="n">observed_value</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">num_list</span><span class="p">)</span>
    <span class="n">expected_value</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="k">assert</span> <span class="n">observed_value</span> <span class="o">==</span> <span class="n">expected_value</span>

<span class="k">def</span> <span class="nf">test_mean_with_double</span><span class="p">():</span>
    <span class="c1"># This one will fail in Python 2</span>
    <span class="n">num_list</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
    <span class="n">observed_value</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">num_list</span><span class="p">)</span>
    <span class="n">expected_value</span> <span class="o">=</span> <span class="mf">2.5</span>
    <span class="k">assert</span> <span class="n">observed_value</span> <span class="o">==</span> <span class="n">expected_value</span>

<span class="k">def</span> <span class="nf">test_mean_with_long</span><span class="p">():</span>
    <span class="n">big</span> <span class="o">=</span> <span class="mi">100000000</span>
    <span class="n">observed_value</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">big</span><span class="p">))</span>
    <span class="n">expected_value</span> <span class="o">=</span> <span class="n">big</span><span class="o">/</span><span class="mf">2.0</span>
    <span class="k">assert</span> <span class="n">observed_value</span> <span class="o">==</span> <span class="n">expected_value</span>
</code></pre></div>
<p>Once these tests are written in a file called <code>test_mean.py</code>, the command <code>pytest</code> can be run on the terminal or command line from the directory containing the tests (note that you'll have to use <code>py.test</code> for older versions of the <code>pytest</code> package):</p>
<div class="highlight"><pre><span></span><code>pytest
</code></pre></div>
<div class="highlight"><pre><span></span><code>collected 5 items

test_mean.py .....

========================== 4 passed in 2.68 seconds ===========================
</code></pre></div>
<p>In the above case, the pytest package sniffed out the tests in the directory and ran them together to produce a report of the sum of the files and functions matching the regular expression <code>[Tt]est[-_].*</code>.</p>
<p>The major benefit a testing framework provides is exactly that, a utility to find and run the tests automatically.
With pytest, this is the command-line tool called <code>pytest</code>.
When <code>pytest</code> is run, it will search all directories below where it was called, find all of the Python files in these directories whose names start or end with <code>test</code>, import them, and run all of the functions and classes whose names start with <code>test</code> or <code>Test</code>.
This automatic registration of test code saves tons of time and provides a consistent organization framework across Python projects.</p>
<p>When you run <code>pytest</code>, it will print a dot (<code>.</code>) on the screen for every test that passes, an <code>F</code> for every test that fails or where there was an unexpected error. After the dots, pytest will print summary information.</p>
<p>To see what a test failure looks like, modify the <code>expected_value</code> in one of the tests and run pytest again. You should see something like this:</p>
<p><div class="highlight"><pre><span></span><code>collected 5 items

test_mean.py F....                                                                   [100%]

========================================= FAILURES =========================================
___________________________________ test_mean_with_ints ____________________________________

    def test_mean_with_ints():
        num_list = [1, 2, 3, 4, 5]
        observed_value = mean(num_list)
        expected_value = 4
&gt;       assert observed_value == expected_value
E       assert 3.0 == 4

test_mean.py:8: AssertionError
================================= short test summary info ==================================
FAILED test_mean.py::test_mean_with_ints - assert 3.0 == 4
=============================== 1 failed, 4 passed in 0.86s ================================
</code></pre></div>
Notice that pytest detects the failed test and provides detailed information about why the test failed
by displaying the values of the variables used in the <code>assert</code> comparison. 
In this case, this information is not surprising, since we triggered the failure by deliberately modifying the <code>expected_value</code> to an incorrect number. 
But in more realistic scenarios, this information is often very helpful for understanding why a test is failing (and whether the failure indicates a true bug in the code or a bug in the test itself). </p>
<h2 id="testing-for-expected-exceptions">Testing for expected exceptions</h2>
<p>In many cases, it is important to check that our code responds to unexpected inputs appropriately. 
Recall that our <code>mean</code> function checks for a <code>TypeError</code> when attempting to calculate the mean.
How can we test that it performs this check correctly and raises the expected exception?</p>
<p>Pytest provides a mechanism to test for expected exceptions. It looks like this:</p>
<p><div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">test_mean_with_non_numeric_list</span><span class="p">():</span>
    <span class="n">num_list</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;0&#39;</span><span class="p">,</span> <span class="s1">&#39;1&#39;</span><span class="p">,</span> <span class="s1">&#39;2&#39;</span><span class="p">]</span>
    <span class="k">with</span> <span class="n">pytest</span><span class="o">.</span><span class="n">raises</span><span class="p">(</span><span class="ne">TypeError</span><span class="p">):</span>
        <span class="n">mean</span><span class="p">(</span><span class="n">num_list</span><span class="p">)</span>
</code></pre></div>
Here, we use a <code>with</code> block to tell pytest that we expect the code within the <code>with</code> block
to raise an exception. Pytest checks that this code does indeed raise an exception of the correct type,
and if not, flags the test as a failure. </p>
<p>Try adding this function to <code>test_mean.py</code> and verify that all the tests pass. 
(hint: don't forget to add <code>import pytest</code> at the top of <code>test_mean.py</code>, since our new test uses <code>pytest.raises</code>). </p>
<h3 id="challenge-1-altering-functions-to-pass-all-tests">Challenge 1: Altering functions to pass all tests</h3>
<p>A scenario we did not consider when writing our <code>mean</code> function is when the list of numbers passed to <code>mean</code> contains complex numbers. 
Because the arithmetic mean of complex numbers may be difficult to interpret,
suppose we decide that we don't want our function to handle this case.</p>
<p>Let's write a test to reflect this. Add the following test to <code>test_mean.py</code>:
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">test_mean_with_complex</span><span class="p">():</span>
    <span class="n">num_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span> <span class="o">+</span> <span class="mi">1</span><span class="n">j</span><span class="p">]</span>
    <span class="k">with</span> <span class="n">pytest</span><span class="o">.</span><span class="n">raises</span><span class="p">(</span><span class="ne">TypeError</span><span class="p">):</span>
        <span class="n">mean</span><span class="p">(</span><span class="n">num_list</span><span class="p">)</span>
</code></pre></div>
Try running the tests again. You should see that this test fails.
This is because our <code>mean</code> function does not check for complex numbers, 
and Python's builtin <code>sum</code> function is able to handle complex numbers, 
so <code>mean</code> returns a result rather than raising an exception.</p>
<p>Now, modify the function <code>mean</code> in <code>mean.py</code> from the previous section until our new test passes.
When it passes, <code>pytest</code> will produce results like the following:</p>
<div class="highlight"><pre><span></span><code>pytest
</code></pre></div>
<div class="highlight"><pre><span></span><code>collected 5 items

test_mean.py .....

========================== 5 passed in 2.68 seconds ===========================
</code></pre></div>
<details>
 <summary> Challenge solution </summary>

There are many ways this challenge could be solved.
One way is to check for the presence of complex numbers before calculating the mean.

<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">mean</span><span class="p">(</span><span class="n">num_list</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">num</span><span class="p">,</span> <span class="nb">complex</span><span class="p">)</span> <span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="n">num_list</span><span class="p">):</span>
       <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Calculating the mean of complex numbers is not supported.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">num_list</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">num_list</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">ZeroDivisionError</span><span class="p">:</span>
            <span class="k">return</span> <span class="mi">0</span>
        <span class="k">except</span> <span class="ne">TypeError</span> <span class="k">as</span> <span class="n">original_error</span><span class="p">:</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="s2">&quot;The algebraic mean of an non-numerical list is undefined.</span><span class="se">\</span>
<span class="s2">                   Please provide a list of numbers.&quot;</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="n">original_error</span><span class="o">.</span><span class="fm">__str__</span><span class="p">()</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span>  <span class="n">msg</span><span class="p">)</span>
</code></pre></div>
</details>

<p>Note, using <code>pytest -v</code> (the 'v' stands for 'verbose') will result in <code>pytest</code> listing which tests are executed and whether they pass or not:</p>
<div class="highlight"><pre><span></span><code>pytest
</code></pre></div>
<div class="highlight"><pre><span></span><code>collected 5 items

test_mean.py .....

test_mean.py::test_mean_with_ints PASSED
test_mean.py::test_mean_with_zero PASSED
test_mean.py::test_mean_with_double PASSED
test_mean.py::test_mean_with_long PASSED
test_mean.py::test_mean_with_complex PASSED

========================== 5 passed in 2.57 seconds ===========================
</code></pre></div>
<p>As we write more code, we would write more tests, and pytest would produce more dots.</p>
<h3 id="parametrized-tests">Parametrized tests</h3>
<p>Often, different unit tests of the same "unit" have similar logic, but are applied to different inputs; this is the case with our <code>mean()</code> test suite above.
<code>pytest</code> offers a convenient mechanism to write just a single test and apply it to multiple inputs, called test parameterization.
This way, adding a new test case is as simple as defining one more input and expected output.</p>
<p>To parametrize a test, we "decorate" it with its expected inputs and outputs, and <code>pytest</code> will expand the test to run as many times as needed to check all inputs.
In the <code>pytest</code> output, this will look exactly like running multiple tests that have different functions.</p>
<details>
<summary>About Python decorators </summary>
"Decorators" are a Python way to enhance or wrap a function with additional behavior.
They are added just before a function definition, using a name that starts with `@`, and can take arguments separate from the function arguments, for example:

<pre><code>
@log_to_file("output.txt")
def mean(num_list):
  ...
</code></pre>
This code is equivalent to wrapping or "decorating" the <code>mean</code> function with the function <code>log_to_file("output.txt")</code>:
<pre><code>
def _mean(num_list):
    ...

mean = log_to_file("output.txt")(_mean)
</code></pre>
The ampersand-based "decorator" syntax is just a clearer and more readable way of wrapping a function with another function. In this example, that other function is in fact <code>log_to_file("output.txt")</code>. This is possible because <code>log_to_file</code> is a function that takes a filename as an input and <i>returns another function</i> that itself takes a function as an argument and then returns a new "wrapped" function.
</details>

<p>For example, here's a parametrized version of our test suite for <code>mean()</code>.
In this case, we decorate the test with two parameters that can be used within the test: <code>num_list</code> and <code>expected_value</code>.
The first parameter will be used to call the <code>mean()</code> function, and the second is its expected result, which the test will validate.
We then provide a list of matching pairs of these parameters, each of which will run as a test:</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">pytest</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">parametrize</span><span class="p">(</span><span class="s2">&quot;num_list,expected_value&quot;</span><span class="p">,</span> <span class="p">[</span>
    <span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="mi">3</span><span class="p">),</span>
    <span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="mi">3</span><span class="p">),</span>
    <span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10000</span><span class="p">),</span> <span class="mi">10000</span><span class="o">/</span><span class="mf">2.0</span><span class="p">),</span>
  <span class="p">])</span>
<span class="k">def</span> <span class="nf">test_mean</span><span class="p">(</span><span class="n">num_list</span><span class="p">,</span> <span class="n">expected_value</span><span class="p">):</span>
    <span class="n">observed_value</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">num_list</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">observed_value</span> <span class="o">==</span> <span class="n">expected_value</span>
</code></pre></div>
<h2 id="integration-tests">Integration tests</h2>
<p>Integration tests focus on gluing code together or the results of code when multiple functions are used.
See below for an conceptual example of an integration test.</p>
<p>Consider three functions <code>add_one()</code>, <code>multiply_by_two()</code>, and <code>add_one_and_multiply_by_two()</code> as a simplistic example.
Function <code>add_one()</code> increments a number by one, <code>multiply_by_two()</code> multiplies a number by two, and <code>add_one_and_multiply_by_two()</code> composes them as defined below:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">add_one</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span>

<span class="k">def</span> <span class="nf">multiply_by_two</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">add_one_and_multiply_by_two</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">multiply_by_two</span><span class="p">(</span><span class="n">add_one</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</code></pre></div>
<p>Functions <code>add_one()</code> and <code>multiply_by_two()</code> can be unit tested since they perform singular operations.
However, <code>add_one_and_multiply_by_two()</code> can't be truly unit tested as it delegates the real work to <code>add_one()</code> and <code>multiply_by_two()</code>.
Testing <code>add_one_and_multiply_by_two()</code> will evaluate the integration of <code>add_one()</code> and <code>multiply_by_two()</code>.</p>
<p>Integration tests still adhere to the practice of comparing expected outcomes with observed results.
A sample <code>test_add_one_and_multiply_by_two()</code> is illustrated below:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">test_add_one_and_multiply_by_two</span><span class="p">():</span>
    <span class="n">expected_value</span> <span class="o">=</span> <span class="mi">6</span>
    <span class="n">observed_value</span> <span class="o">=</span> <span class="n">add_one_and_multiply_by_two</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">observed_value</span> <span class="o">==</span> <span class="n">expected_value</span>
</code></pre></div>
<p>The definition of a code unit is somewhat ambiguous, making the distinction between integration tests and unit tests a bit unclear.
Integration tests can range from extremely simple to highly complex, contrasting with unit tests.
If a function or class merely amalgamates two or more unit-tested code pieces, an integration test is necessary.
If a function introduces new untested behavior, a unit test is needed.</p>
<p>The structure of integration tests closely resembles that of unit tests, comparing expected results with observed values.
However, deriving the expected result or preparing the code for execution can be significantly more complex.
Integration tests are generally more time-consuming due to their extensive nature.
This distinction is helpful to differentiate between straightforward (unit) and more nuanced (integration) test-writing requirements.</p>
<h2 id="regression-tests">Regression tests</h2>
<p>Regression tests refer to past outputs for expected behavior.
The anticipated outcome is based on previous computations for the same inputs.</p>
<p>Regression tests hold the past as "correct."
They notify developers about how and when a codebase has evolved such that it produces different results.
However, they don't provide insights into why the changes occurred.
The discrepancy between current and previous code outputs is termed a regression.</p>
<p>Like integration tests, regression tests are high-level and often encompass the entire code base.
A prevalent regression test strategy extends across multiple code versions.
For instance, an input file for version X of a workflow is processed, and the output file is saved, typically online.
While developing version Y, the test suite automatically fetches the output for version X, processes the same input file for version Y, and contrasts the two output files.
Any significant discrepancies trigger a test failure.
Regression tests can identify failures missed by integration and unit tests.
Each project may adopt a slightly varied approach to regression testing, based on its software requirements.
Testing frameworks aid in constructing regression tests but don’t provide additional sophistication beyond the discussed concepts.</p>
<h2 id="continuous-integration">Continuous integration</h2>
<p>Continuous integration makes running tests as easy as possible by integrating the test suite into the development process.
Every time a change is made to the repository, the continuous integration system builds and checks that code.</p>
<p>Based on the instructions you provide, a continuous integration server can:</p>
<ul>
<li>check out new code from a repository</li>
<li>spin up instances of supported operating systems (i.e. various versions of OSX, Linux, Windows, etc.).</li>
<li>spin up those instances with different software versions (i.e. python 2.7 and python 3.0)</li>
<li>run the build and test scripts</li>
<li>check for errors</li>
<li>report the results.</li>
</ul>
<p>Since the first step the server conducts is to check out the code from a repository, we'll need to put our code online to make use of this kind of server.</p>
<h3 id="set-up-a-mean-git-repository-on-github">Set Up a Mean Git Repository on GitHub</h3>
<p>Our <code>mean.py</code> <code>test_mean.py</code> files can be the contents of a repository on GitHub.</p>
<ul>
<li>Go to GitHub and <a href="https://github.com/new">create a repository</a> called aug-mean. Do this in your own user account and don't add any files to the new repo (no README/LICENSE, etc.).</li>
<li>Turn the <code>aug-mean</code> directory that we've been working in on your computer into a git repository following the "…or create a new repository on the command line" instructions on GitHub:
<div class="highlight"><pre><span></span><code><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;# aug-mean&quot;</span><span class="w"> </span>&gt;&gt;<span class="w"> </span>README.md
git<span class="w"> </span>init
git<span class="w"> </span>add<span class="w"> </span>README.md
git<span class="w"> </span>commit<span class="w"> </span>-m<span class="w"> </span><span class="s2">&quot;first commit&quot;</span>
git<span class="w"> </span>branch<span class="w"> </span>-M<span class="w"> </span>main
git<span class="w"> </span>remote<span class="w"> </span>add<span class="w"> </span>origin<span class="w"> </span>git@github.com:yourusername/aug-mean.git
git<span class="w"> </span>push<span class="w"> </span>-u<span class="w"> </span>origin<span class="w"> </span>main
</code></pre></div></li>
<li>Create a new branch (<code>git checkout -b yourinitials/init</code>).</li>
<li>Use git to <code>add</code>, <code>commit</code>, and <code>push</code> the two files <code>mean.py</code> and <code>test_mean.py</code> to GitHub.</li>
</ul>
<h3 id="github-actions">GitHub Actions</h3>
<p><a href="https://github.com/features/actions">GitHub Actions</a> is a continuous integration service provided by GitHub.
It's integrated directly into GitHub repositories and does not require additional accounts or external services.
Note that GitHub Actions usage is free for standard GitHub-hosted runners in public repositories, and for self-hosted runners.
For private repositories, each GitHub account receives a certain amount of free minutes and storage for use with GitHub-hosted runners, depending on the account's plan (see <a href="https://docs.github.com/en/actions/learn-github-actions/usage-limits-billing-and-administration">here</a> for more information).</p>
<p>To use GitHub Actions, create a directory called <code>.github</code> and within it, create another directory called <code>workflows</code>.</p>
<div class="highlight"><pre><span></span><code>mkdir -p .github/workflows
</code></pre></div>
<p>Inside the <code>workflows</code> directory, you can create a YAML file (e.g. <code>ci.yml</code>) to define your continuous integration process:</p>
<div class="highlight"><pre><span></span><code><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pytest CI</span>

<span class="nt">on</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">push</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">pull_request</span><span class="p p-Indicator">]</span>

<span class="nt">jobs</span><span class="p">:</span>
<span class="w">  </span><span class="nt">build</span><span class="p">:</span>
<span class="w">    </span><span class="nt">runs-on</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubuntu-latest</span>
<span class="w">    </span><span class="nt">steps</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/checkout@v2</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Set up Python 3.10</span>
<span class="w">      </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/setup-python@v2</span>
<span class="w">      </span><span class="nt">with</span><span class="p">:</span>
<span class="w">        </span><span class="nt">python-version</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;3.10&#39;</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Install dependencies</span>
<span class="w">      </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">        </span><span class="no">python -m pip install --upgrade pip</span>
<span class="w">        </span><span class="no">pip install -r requirements.txt</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Run tests</span>
<span class="w">      </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pytest</span>
</code></pre></div>
<p>Below, we break down how this workflow works:
* <code>name</code>: Our workflow is named <code>pytest CI</code>.
* <code>on</code>: The workflow will be triggered to run automatically whenever commits are pushed to the repository or whenever a pull request is created or updated with new commits.
* <code>jobs</code>: Specifies what the workflow will actually run. In this case, we specify that we want to run on the latest version of the ubuntu operating system using python version 3.10. These instructions are enough to launch a computer with python running on it. Then, we specify that we want to install dependencies from a <code>requirements.txt</code> file using pip. Lastly, we run our tests using <code>pytest</code>.</p>
<details>
<summary> Running GitHub Actions workflows on multiple operating systems using a matrix</summary>

Often times, developers want to check that their tests will pass not just with an ubuntu operating system and one version of Python, but with many operating systems and many versions of Python.
This can be done using a matrix, which we demonstrate below.
We specify lists of operating systems and python versions that we want to run our CI with, and then use a matrix call to run them all.

<div class="highlight"><pre><span></span><code><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pytest CI</span>

<span class="nt">on</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">push</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">pull_request</span><span class="p p-Indicator">]</span>

<span class="nt">jobs</span><span class="p">:</span>
<span class="w">  </span><span class="nt">build</span><span class="p">:</span>
<span class="w">    </span><span class="nt">runs-on</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${{ matrix.os }}</span>
<span class="w">    </span><span class="nt">strategy</span><span class="p">:</span>
<span class="w">      </span><span class="nt">matrix</span><span class="p">:</span>
<span class="w">        </span><span class="nt">os</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">ubuntu-latest</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">macos-latest</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="nv">windows-latest</span><span class="p p-Indicator">]</span>
<span class="w">        </span><span class="nt">python-version</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&#39;3.7&#39;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&#39;3.8&#39;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&#39;3.9&#39;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&#39;3.10&#39;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&#39;3.11&#39;</span><span class="p p-Indicator">]</span>
<span class="w">    </span><span class="nt">steps</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/checkout@v2</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Set up Python ${{ matrix.python-version }}</span>
<span class="w">      </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/setup-python@v2</span>
<span class="w">      </span><span class="nt">with</span><span class="p">:</span>
<span class="w">        </span><span class="nt">python-version</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${{ matrix.python-version }}</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Install dependencies</span>
<span class="w">      </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
<span class="w">        </span><span class="no">python -m pip install --upgrade pip</span>
<span class="w">        </span><span class="no">pip install -r requirements.txt</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Run tests</span>
<span class="w">      </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pytest</span>
</code></pre></div>

</details>

<h3 id="triggering-ci">Triggering CI</h3>
<ol>
<li>Add <code>.github/workflows/ci.yml</code> to your repository</li>
<li>Commit and push it.</li>
<li>Open a pull request with your changes.</li>
<li>Check how the CI is going in the PR.</li>
</ol>
<p>Right now, the CI is failing.
Why is that?
When we set up for today's lesson, we used conda to install pytest.
In the CI workflow, we specified that our dependencies (in this case pytest) should be installed from a <code>requirements.txt</code> file.
That <code>requirements.txt</code> file is a conventional way to list all of the python packages that we need.
We haven't created that file yet.
Let's go ahead an create it, add it, commit it, and push it.
Since we need pytest, the <code>requirements.txt</code> file looks like this:</p>
<div class="highlight"><pre><span></span><code>pytest==7.4.3
</code></pre></div>
<p>Pushing new changes to our branch automatically re-triggers the CI tests to re-run.
When all of our tests pass, we'll see a big green check mark next stating that "All checks have passed" and a green check next to our commit in our commit history.
If some of your checks don't pass, you can see what went wrong by clicking on the check which will launch the runtime information.</p>
<p>Also note that while in this example we use pip and a <code>requirements.txt</code> file to install dependencies, dependencies can be installed in other ways such as with <code>conda</code>.</p>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2022 <a href="https://www.arcadiascience.com">Arcadia Science</a>
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../../..", "features": [], "search": "../../../assets/javascripts/workers/search.5bf1dace.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.078830c0.min.js"></script>
      
    
  </body>
</html>